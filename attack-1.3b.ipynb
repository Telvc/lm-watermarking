{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-24T00:33:52.979178Z",
     "start_time": "2025-03-24T00:33:52.927810Z"
    }
   },
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n",
    "# available at https://arxiv.org/abs/2301.10226\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.stats as stats\n",
    "from datasets import load_dataset\n",
    "import collections\n",
    "from math import sqrt\n",
    "import scipy.stats\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import nltk\n",
    "import ssl\n",
    "from nltk.util import ngrams\n",
    "from transformers import LogitsProcessor\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from nltk.corpus import wordnet as wn\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import networkx as nx\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#########################\n",
    "# Helper Functions\n",
    "#########################\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"A demo for watermarking using a revised green/red partition based on WordNet synonyms.\"\n",
    "    )\n",
    "    parser.add_argument(\"--demo_public\", type=str2bool, default=False,\n",
    "                        help=\"Expose the gradio demo publicly.\")\n",
    "    parser.add_argument(\"--model_name_or_path\", type=str, default=\"facebook/opt-1.3b\",\n",
    "                        help=\"Identifier for the pretrained model from Hugging Face.\")\n",
    "    parser.add_argument(\"--prompt_max_length\", type=int, default=None,\n",
    "                        help=\"Truncation length for the prompt.\")\n",
    "    parser.add_argument(\"--max_new_tokens\", type=int, default=200,\n",
    "                        help=\"Maximum number of tokens to generate.\")\n",
    "    parser.add_argument(\"--generation_seed\", type=int, default=123,\n",
    "                        help=\"Seed for generation reproducibility.\")\n",
    "    parser.add_argument(\"--use_sampling\", type=str2bool, default=True,\n",
    "                        help=\"Use multinomial sampling for generation.\")\n",
    "    parser.add_argument(\"--sampling_temp\", type=float, default=0.7,\n",
    "                        help=\"Sampling temperature.\")\n",
    "    parser.add_argument(\"--n_beams\", type=int, default=1,\n",
    "                        help=\"Number of beams for beam search (if not sampling).\")\n",
    "    parser.add_argument(\"--use_gpu\", type=str2bool, default=True,\n",
    "                        help=\"Run inference on GPU if available.\")\n",
    "    parser.add_argument(\"--seeding_scheme\", type=str, default=\"simple_1\",\n",
    "                        help=\"Seeding scheme for watermarking.\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.25,\n",
    "                        help=\"Target fraction of tokens for the green list.\")\n",
    "    parser.add_argument(\"--delta\", type=float, default=2.0,\n",
    "                        help=\"Bias to add to green list token logits.\")\n",
    "    parser.add_argument(\"--normalizers\", type=str, default=\"\",\n",
    "                        help=\"Comma separated normalizer names for detection.\")\n",
    "    parser.add_argument(\"--ignore_repeated_bigrams\", type=str2bool, default=False,\n",
    "                        help=\"Use repeated bigram variant in detection.\")\n",
    "    parser.add_argument(\"--detection_z_threshold\", type=float, default=4.0,\n",
    "                        help=\"Z-score threshold for detection.\")\n",
    "    parser.add_argument(\"--select_green_tokens\", type=str2bool, default=True,\n",
    "                        help=\"Legacy option for selecting green tokens.\")\n",
    "    parser.add_argument(\"--skip_model_load\", type=str2bool, default=False,\n",
    "                        help=\"Skip model loading (for debugging).\")\n",
    "    parser.add_argument(\"--seed_separately\", type=str2bool, default=True,\n",
    "                        help=\"Seed separately for each generation call.\")\n",
    "    parser.add_argument(\"--load_fp16\", type=str2bool, default=False,\n",
    "                        help=\"Load model in FP16 mode.\")\n",
    "    args = parser.parse_args()\n",
    "    # Convert normalizers into a list (if provided)\n",
    "    args.normalizers = args.normalizers.split(\",\") if args.normalizers else []\n",
    "    return args\n",
    "\n",
    "\n",
    "#############################\n",
    "# Preprocessing: Vocabulary & Matching\n",
    "#############################\n",
    "\n",
    "def get_vocabulary(tokenizer) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of tokens (where index corresponds to token ID)\n",
    "    extracted from the tokenizer's vocabulary.\n",
    "    \"\"\"\n",
    "    vocab_dict = tokenizer.get_vocab()\n",
    "    vocab_list = [None] * len(vocab_dict)\n",
    "    for token, idx in vocab_dict.items():\n",
    "        vocab_list[idx] = token\n",
    "    return vocab_list\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_lemma_set(token: str) -> set:\n",
    "    \"\"\"\n",
    "    Given a token string, first strip any leading BPE marker (e.g. \"Ġ\")\n",
    "    and return the set of lowercased lemma names from all its WordNet synsets.\n",
    "    \"\"\"\n",
    "    # Strip off common prefix markers\n",
    "    word = token.lstrip(\"Ġ\")\n",
    "    synsets = wn.synsets(word)\n",
    "    return {lemma.lower() for s in synsets for lemma in s.lemma_names()}\n",
    "\n",
    "\n",
    "def are_synonyms(token1: str, token2: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determines whether two tokens are synonyms by checking if token1 (after stripping)\n",
    "    appears in token2's lemma set and vice versa.\n",
    "    \"\"\"\n",
    "    word1 = token1.lstrip(\"Ġ\")\n",
    "    word2 = token2.lstrip(\"Ġ\")\n",
    "    lemmas1 = get_lemma_set(word1)\n",
    "    lemmas2 = get_lemma_set(word2)\n",
    "    if not lemmas1 or not lemmas2:\n",
    "        return False\n",
    "    return (word1.lower() in lemmas2) and (word2.lower() in lemmas1)\n",
    "\n",
    "\n",
    "def filter_tokens_with_synonyms(vocab_list: list[str]) -> (list[int], list[int]):\n",
    "    \"\"\"\n",
    "    Splits the vocabulary indices into:\n",
    "      - unique_indices: indices of tokens that have no synonym in the vocabulary.\n",
    "      - paired_indices: indices of tokens that have at least one synonym.\n",
    "    Uses a progress bar via tqdm.\n",
    "    \"\"\"\n",
    "    unique_indices = []\n",
    "    paired_indices = []\n",
    "    n = len(vocab_list)\n",
    "    # Precompute lemma sets for each token (stripped of any leading marker)\n",
    "    lemma_sets = [get_lemma_set(token) for token in vocab_list]\n",
    "    for i in tqdm(range(n), desc=\"Filtering vocabulary\"):\n",
    "        token_i = vocab_list[i]\n",
    "        lemmas_i = lemma_sets[i]\n",
    "        has_synonym = False\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if (token_i.lstrip(\"Ġ\")).lower() in lemma_sets[j] and (vocab_list[j].lstrip(\"Ġ\")).lower() in lemmas_i:\n",
    "                has_synonym = True\n",
    "                break\n",
    "        if has_synonym:\n",
    "            paired_indices.append(i)\n",
    "        else:\n",
    "            unique_indices.append(i)\n",
    "    return unique_indices, paired_indices\n",
    "\n",
    "\n",
    "def construct_similarity_matrix(vocab_list: list[str], indices: list[int]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Constructs an m x m similarity matrix for tokens specified by indices.\n",
    "    Entry [i][j] is 1.0 if the tokens are synonyms, 0 otherwise.\n",
    "    Uses nested loops with tqdm progress bar.\n",
    "    \"\"\"\n",
    "    m = len(indices)\n",
    "    C = [[0.0 for _ in range(m)] for _ in range(m)]\n",
    "    # Precompute lemma sets for tokens in indices\n",
    "    lemma_dict = {i: get_lemma_set(vocab_list[i].lstrip(\"Ġ\")) for i in indices}\n",
    "    for a in tqdm(range(m), desc=\"Constructing similarity matrix (outer loop)\"):\n",
    "        for b in range(a + 1, m):\n",
    "            token_a = vocab_list[indices[a]].lstrip(\"Ġ\")\n",
    "            token_b = vocab_list[indices[b]].lstrip(\"Ġ\")\n",
    "            lemmas_a = lemma_dict[indices[a]]\n",
    "            lemmas_b = lemma_dict[indices[b]]\n",
    "            weight = 1.0 if (token_a.lower() in lemmas_b and token_b.lower() in lemmas_a) else 0.0\n",
    "            C[a][b] = weight\n",
    "            C[b][a] = weight\n",
    "    return C\n",
    "\n",
    "\n",
    "def find_perfect_matching(similarity_matrix: list[list[float]]) -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Constructs an undirected graph from the similarity matrix (only edges with weight>0)\n",
    "    and returns a maximum–weight matching (as a list of index pairs relative to the input list).\n",
    "    Uses tqdm over the pairs.\n",
    "    \"\"\"\n",
    "    m = len(similarity_matrix)\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(m))\n",
    "    pairs = list(itertools.combinations(range(m), 2))\n",
    "    for i, j in tqdm(pairs, total=len(pairs), desc=\"Building graph for matching\"):\n",
    "        weight = similarity_matrix[i][j]\n",
    "        if weight > 0:\n",
    "            G.add_edge(i, j, weight=weight)\n",
    "    matching = nx.max_weight_matching(G, maxcardinality=True)\n",
    "    pairing = [tuple(sorted(pair)) for pair in matching]\n",
    "    return pairing\n",
    "\n",
    "\n",
    "#############################\n",
    "# Revised Watermark Processor Classes\n",
    "#############################\n",
    "\n",
    "class WatermarkBase:\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab: list[int] = None,\n",
    "            gamma: float = 0.25,\n",
    "            delta: float = 2.0,\n",
    "            seeding_scheme: str = \"simple_1\",\n",
    "            hash_key: int = 15485863,\n",
    "            select_green_tokens: bool = True,\n",
    "            precomputed_pairing: list[tuple[int, int]] = None,\n",
    "            unique_tokens: list[int] = None,\n",
    "    ):\n",
    "        self.vocab = vocab  # list of token IDs (usually 0, ..., n-1)\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.gamma = gamma  # target fraction of tokens for the green list\n",
    "        self.delta = delta  # bias added to green token logits\n",
    "        self.seeding_scheme = seeding_scheme\n",
    "        self.rng = None\n",
    "        self.hash_key = hash_key\n",
    "        self.select_green_tokens = select_green_tokens\n",
    "        self.pairing = precomputed_pairing  # perfect matching (pairs) for tokens with synonyms\n",
    "        self.unique_tokens = unique_tokens  # token IDs that have no synonyms\n",
    "\n",
    "    #debug: replace input.device to device in _seed_rng, _get_greenlist_ids\n",
    "    def _seed_rng(self, input_ids: torch.LongTensor, seeding_scheme: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Seeds the RNG deterministically using the last token in input_ids.\n",
    "        For the \"simple_1\" scheme, the seed is hash_key * (last token id).\n",
    "        \"\"\"\n",
    "        if seeding_scheme is None:\n",
    "            seeding_scheme = self.seeding_scheme\n",
    "        if self.rng is None:\n",
    "            self.rng = torch.Generator(device=device)\n",
    "        if seeding_scheme == \"simple_1\":\n",
    "            assert input_ids.shape[-1] >= 1, \"Input must have at least one token.\"\n",
    "            prev_token = input_ids[-1].item()\n",
    "            self.rng.manual_seed(self.hash_key * prev_token)\n",
    "            #print(self.hash_key * prev_token)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Seeding scheme {seeding_scheme} not implemented.\")\n",
    "        return\n",
    "\n",
    "    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> list[int]:\n",
    "        \"\"\"\n",
    "        Returns the green list token IDs.\n",
    "        If precomputed pairing and unique_tokens are provided, then:\n",
    "          - All unique tokens (set A) are in the green list.\n",
    "          - For each pair in the perfect matching, one token is chosen by a coin flip.\n",
    "        Otherwise, falls back to a random permutation method.\n",
    "        The final list is optionally truncated to a target size (gamma * vocab_size).\n",
    "        \"\"\"\n",
    "        self._seed_rng(input_ids)\n",
    "\n",
    "        if self.pairing is None or self.unique_tokens is None:\n",
    "            # Fallback: use random permutation.\n",
    "            greenlist_size = int(self.vocab_size * self.gamma)\n",
    "            vocab_permutation = torch.randperm(self.vocab_size, device=device, generator=self.rng)\n",
    "            if self.select_green_tokens:\n",
    "                return vocab_permutation[:greenlist_size].tolist()\n",
    "            else:\n",
    "                return vocab_permutation[-greenlist_size:].tolist()\n",
    "        else:\n",
    "            greenlist_ids = self.unique_tokens.copy()\n",
    "            for pair in self.pairing:\n",
    "                coin_flip = (torch.rand(1, generator=self.rng, device=device).item() < 0.5)\n",
    "                chosen = pair[0] if coin_flip else pair[1]\n",
    "                greenlist_ids.append(chosen)\n",
    "            # desired_size = int(self.vocab_size * self.gamma)\n",
    "            # if len(greenlist_ids) > desired_size:\n",
    "            #    perm = torch.randperm(len(greenlist_ids), generator=self.rng).tolist()\n",
    "            #    indices = perm[:desired_size]\n",
    "            #    greenlist_ids = [greenlist_ids[i] for i in indices]\n",
    "            return greenlist_ids\n",
    "\n",
    "\n",
    "class WatermarkLogitsProcessor(WatermarkBase, LogitsProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _calc_greenlist_mask(self, scores: torch.FloatTensor, greenlist_token_ids) -> torch.BoolTensor:\n",
    "        green_tokens_mask = torch.zeros_like(scores)\n",
    "        for b_idx in range(len(greenlist_token_ids)):\n",
    "            green_tokens_mask[b_idx][greenlist_token_ids[b_idx]] = 1\n",
    "        return green_tokens_mask.bool()\n",
    "\n",
    "    def _bias_greenlist_logits(self, scores: torch.Tensor, greenlist_mask: torch.Tensor,\n",
    "                               greenlist_bias: float) -> torch.Tensor:\n",
    "        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\n",
    "        return scores\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        if self.rng is None:\n",
    "            self.rng = torch.Generator(device=input_ids.device)\n",
    "        batched_greenlist_ids = []\n",
    "        for b_idx in range(input_ids.shape[0]):\n",
    "            greenlist_ids = self._get_greenlist_ids(input_ids[b_idx])\n",
    "            batched_greenlist_ids.append(greenlist_ids)\n",
    "        green_tokens_mask = self._calc_greenlist_mask(scores, batched_greenlist_ids)\n",
    "        scores = self._bias_greenlist_logits(scores, green_tokens_mask, self.delta)\n",
    "        #print(torch.max(scores))\n",
    "        return scores\n",
    "\n",
    "\n",
    "class WatermarkDetector(WatermarkBase):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            device: torch.device = None,\n",
    "            tokenizer=None,\n",
    "            z_threshold: float = 4.0,\n",
    "            normalizers: list[str] = [\"unicode\"],\n",
    "            ignore_repeated_bigrams: bool = True,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert device, \"Device must be provided.\"\n",
    "        assert tokenizer, \"A tokenizer is required for detection.\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.z_threshold = z_threshold\n",
    "        self.rng = torch.Generator(device=self.device)\n",
    "        if self.seeding_scheme == \"simple_1\":\n",
    "            self.min_prefix_len = 1\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Seeding scheme {self.seeding_scheme} not implemented.\")\n",
    "        self.normalizers = [normalization_strategy_lookup(norm) for norm in normalizers]\n",
    "        self.ignore_repeated_bigrams = ignore_repeated_bigrams\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            assert self.seeding_scheme == \"simple_1\", \"Repeated bigram variant requires simple_1 seeding.\"\n",
    "\n",
    "    def _compute_z_score(self, observed_count, T):\n",
    "        expected_count = self.gamma\n",
    "        numer = observed_count - expected_count * T\n",
    "        denom = sqrt(T * expected_count * (1 - expected_count))\n",
    "        return numer / denom\n",
    "\n",
    "    def _compute_p_value(self, z):\n",
    "        return scipy.stats.norm.sf(z)\n",
    "\n",
    "    def _score_sequence(\n",
    "            self,\n",
    "            input_ids: Tensor,\n",
    "            return_num_tokens_scored: bool = True,\n",
    "            return_num_green_tokens: bool = True,\n",
    "            return_green_fraction: bool = True,\n",
    "            return_green_token_mask: bool = False,\n",
    "            return_z_score: bool = True,\n",
    "            return_p_value: bool = True,\n",
    "    ):\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            # Repeated bigram variant: T = number of unique bigrams.\n",
    "            bigram_table = {}\n",
    "            token_bigram_generator = ngrams(input_ids.cpu().tolist(), 2)\n",
    "            freq = collections.Counter(token_bigram_generator)\n",
    "            num_tokens_scored = len(freq.keys())\n",
    "            for bigram in freq.keys():\n",
    "                prefix = torch.tensor([bigram[0]], device=self.device)\n",
    "                greenlist_ids = self._get_greenlist_ids(prefix)\n",
    "                bigram_table[bigram] = bigram[1] in greenlist_ids\n",
    "            green_token_count = sum(bigram_table.values())\n",
    "        else:\n",
    "            # Standard variant: T = total tokens (after min_prefix_len)\n",
    "            num_tokens_scored = len(input_ids) - self.min_prefix_len\n",
    "            if num_tokens_scored < 1:\n",
    "                raise ValueError(\"Not enough tokens to score.\")\n",
    "            green_token_count = 0\n",
    "            green_token_mask = []\n",
    "            for idx in range(self.min_prefix_len, len(input_ids)):\n",
    "                curr_token = input_ids[idx]\n",
    "                greenlist_ids = self._get_greenlist_ids(input_ids[:idx])\n",
    "                if curr_token in greenlist_ids:\n",
    "                    green_token_count += 1\n",
    "                    green_token_mask.append(True)\n",
    "                else:\n",
    "                    green_token_mask.append(False)\n",
    "        # Debug prints:\n",
    "        print(f\"Total tokens scored (T): {num_tokens_scored}\")\n",
    "        print(f\"Green token count: {green_token_count}\")\n",
    "        print(f\"Green fraction: {green_token_count / num_tokens_scored:.2%}\")\n",
    "\n",
    "        score_dict = {}\n",
    "        if return_num_tokens_scored:\n",
    "            score_dict[\"num_tokens_scored\"] = num_tokens_scored\n",
    "        if return_num_green_tokens:\n",
    "            score_dict[\"num_green_tokens\"] = green_token_count\n",
    "        if return_green_fraction:\n",
    "            score_dict[\"green_fraction\"] = green_token_count / num_tokens_scored\n",
    "        if return_z_score:\n",
    "            score_dict[\"z_score\"] = self._compute_z_score(green_token_count, num_tokens_scored)\n",
    "        if return_p_value:\n",
    "            z = score_dict.get(\"z_score\", self._compute_z_score(green_token_count, num_tokens_scored))\n",
    "            score_dict[\"p_value\"] = self._compute_p_value(z)\n",
    "        if return_green_token_mask:\n",
    "            score_dict[\"green_token_mask\"] = green_token_mask\n",
    "        return score_dict\n",
    "\n",
    "    def detect(\n",
    "            self,\n",
    "            text: str = None,\n",
    "            tokenized_text: list[int] = None,\n",
    "            return_prediction: bool = True,\n",
    "            return_scores: bool = True,\n",
    "            z_threshold: float = None,\n",
    "            **kwargs,\n",
    "    ) -> dict:\n",
    "        assert (text is not None) ^ (tokenized_text is not None), \"Provide either raw or tokenized text.\"\n",
    "        if return_prediction:\n",
    "            kwargs[\"return_p_value\"] = True\n",
    "        for normalizer in self.normalizers:\n",
    "            text = normalizer(text)\n",
    "        if tokenized_text is None:\n",
    "            tokenized_text = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].to(\n",
    "                self.device)\n",
    "            if tokenized_text[0] == self.tokenizer.bos_token_id:\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "        else:\n",
    "            if self.tokenizer is not None and tokenized_text[0] == self.tokenizer.bos_token_id:\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "        output_dict = {}\n",
    "        score_dict = self._score_sequence(tokenized_text, **kwargs)\n",
    "        if return_scores:\n",
    "            output_dict.update(score_dict)\n",
    "        if return_prediction:\n",
    "            z_threshold = z_threshold if z_threshold is not None else self.z_threshold\n",
    "            output_dict[\"prediction\"] = score_dict[\"z_score\"] > z_threshold\n",
    "            if output_dict[\"prediction\"]:\n",
    "                output_dict[\"confidence\"] = 1 - score_dict[\"p_value\"]\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "#############################\n",
    "# Demo Code (Generation & Detection)\n",
    "#############################\n",
    "\n",
    "def load_model(args):\n",
    "    # Set model type attributes on args.\n",
    "    args.is_seq2seq_model = any(model_type in args.model_name_or_path for model_type in [\"t5\", \"T0\"])\n",
    "    args.is_decoder_only_model = any(model_type in args.model_name_or_path for model_type in [\"gpt\", \"opt\", \"bloom\"])\n",
    "\n",
    "    if args.is_seq2seq_model:\n",
    "        from transformers import AutoModelForSeq2SeqLM\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n",
    "    elif args.is_decoder_only_model:\n",
    "        if args.load_fp16:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, torch_dtype=torch.float16,\n",
    "                                                         device_map='auto')\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {args.model_name_or_path}\")\n",
    "\n",
    "    if args.use_gpu:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if not args.load_fp16:\n",
    "            model = model.to(device)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def generate(prompt, args, model=None, device=None, tokenizer=None):\n",
    "    # print(f\"Generating with {args}\")\n",
    "    # Set model type attributes on args.\n",
    "    args.is_seq2seq_model = any(model_type in args.model_name_or_path for model_type in [\"t5\", \"T0\"])\n",
    "    args.is_decoder_only_model = any(model_type in args.model_name_or_path for model_type in [\"gpt\", \"opt\", \"bloom\"])\n",
    "\n",
    "    # Instantiate the watermark processor with precomputed pairing/unique tokens.\n",
    "    # Assume that in main() we precomputed these values (see below).\n",
    "    watermark_processor = WatermarkLogitsProcessor(\n",
    "        vocab=list(tokenizer.get_vocab().values()),\n",
    "        gamma=args.gamma,\n",
    "        delta=args.delta,\n",
    "        seeding_scheme=args.seeding_scheme,\n",
    "        select_green_tokens=args.select_green_tokens,\n",
    "        precomputed_pairing=args.precomputed_pairing,\n",
    "        unique_tokens=args.unique_tokens\n",
    "    )\n",
    "\n",
    "    gen_kwargs = dict(max_new_tokens=args.max_new_tokens)\n",
    "    if args.use_sampling:\n",
    "        gen_kwargs.update(dict(\n",
    "            do_sample=True,\n",
    "            top_k=0,\n",
    "            temperature=args.sampling_temp\n",
    "        ))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(\n",
    "            num_beams=args.n_beams\n",
    "        ))\n",
    "\n",
    "    generate_without_watermark = partial(\n",
    "        model.generate,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    generate_with_watermark = partial(\n",
    "        model.generate,\n",
    "        logits_processor=LogitsProcessorList([watermark_processor]),\n",
    "        **gen_kwargs\n",
    "    )\n",
    "\n",
    "    if not args.prompt_max_length:\n",
    "        if hasattr(model.config, \"max_position_embedding\"):\n",
    "            args.prompt_max_length = model.config.max_position_embeddings - args.max_new_tokens\n",
    "        else:\n",
    "            args.prompt_max_length = 2048 - args.max_new_tokens\n",
    "\n",
    "    tokd_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True, truncation=True,\n",
    "                           max_length=args.prompt_max_length).to(device)\n",
    "    truncation_warning = tokd_input[\"input_ids\"].shape[-1] == args.prompt_max_length\n",
    "    redecoded_input = tokenizer.batch_decode(tokd_input[\"input_ids\"], skip_special_tokens=True)[0]\n",
    "\n",
    "    torch.manual_seed(args.generation_seed)\n",
    "    output_without_watermark = generate_without_watermark(**tokd_input)\n",
    "    if args.seed_separately:\n",
    "        torch.manual_seed(args.generation_seed)\n",
    "    output_with_watermark = generate_with_watermark(**tokd_input)\n",
    "\n",
    "    if args.is_decoder_only_model:\n",
    "        output_without_watermark = output_without_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "        output_with_watermark = output_with_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    decoded_output_without_watermark = tokenizer.batch_decode(output_without_watermark, skip_special_tokens=True)[0]\n",
    "    decoded_output_with_watermark = tokenizer.batch_decode(output_with_watermark, skip_special_tokens=True)[0]\n",
    "\n",
    "    return (\n",
    "        redecoded_input, int(truncation_warning), decoded_output_without_watermark, decoded_output_with_watermark, args)\n",
    "\n",
    "\n",
    "def format_names(s):\n",
    "    s = s.replace(\"num_tokens_scored\", \"Tokens Counted (T)\")\n",
    "    s = s.replace(\"num_green_tokens\", \"# Tokens in Greenlist\")\n",
    "    s = s.replace(\"green_fraction\", \"Fraction of T in Greenlist\")\n",
    "    s = s.replace(\"z_score\", \"z-score\")\n",
    "    s = s.replace(\"p_value\", \"p value\")\n",
    "    s = s.replace(\"prediction\", \"Prediction\")\n",
    "    s = s.replace(\"confidence\", \"Confidence\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def list_format_scores(score_dict, detection_threshold):\n",
    "    lst_2d = []\n",
    "    for k, v in score_dict.items():\n",
    "        if k == 'green_fraction':\n",
    "            lst_2d.append([format_names(k), f\"{v:.1%}\"])\n",
    "        elif k == 'confidence':\n",
    "            lst_2d.append([format_names(k), f\"{v:.3%}\"])\n",
    "        elif isinstance(v, float):\n",
    "            lst_2d.append([format_names(k), f\"{v:.3g}\"])\n",
    "        elif isinstance(v, bool):\n",
    "            lst_2d.append([format_names(k), (\"Watermarked\" if v else \"Human/Unwatermarked\")])\n",
    "        else:\n",
    "            lst_2d.append([format_names(k), f\"{v}\"])\n",
    "    if \"confidence\" in score_dict:\n",
    "        lst_2d.insert(-2, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    else:\n",
    "        lst_2d.insert(-1, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    return lst_2d\n",
    "\n",
    "\n",
    "def detect(input_text, args, device=None, tokenizer=None):\n",
    "    watermark_detector = WatermarkDetector(\n",
    "        vocab=list(tokenizer.get_vocab().values()),\n",
    "        gamma=args.gamma,\n",
    "        seeding_scheme=args.seeding_scheme,\n",
    "        device=device,\n",
    "        tokenizer=tokenizer,\n",
    "        z_threshold=args.detection_z_threshold,\n",
    "        normalizers=args.normalizers,\n",
    "        ignore_repeated_bigrams=args.ignore_repeated_bigrams,\n",
    "        select_green_tokens=args.select_green_tokens\n",
    "    )\n",
    "    if len(input_text) - 1 > watermark_detector.min_prefix_len:\n",
    "        score_dict = watermark_detector.detect(input_text)\n",
    "        output = list_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "    else:\n",
    "        output = [[\"Error\", \"string too short to compute metrics\"]]\n",
    "        output += [[\"\", \"\"] for _ in range(6)]\n",
    "    return output, args\n",
    "\n",
    "\n",
    "def run_gradio(args, model=None, device=None, tokenizer=None):\n",
    "    generate_partial = partial(generate, model=model, device=device, tokenizer=tokenizer)\n",
    "    detect_partial = partial(detect, device=device, tokenizer=tokenizer)\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"Gradio demo not shown in command-line mode.\")\n",
    "        demo.launch()\n",
    "\n",
    "\n",
    "def compute_p_value(green_count: int, tokens_scored: int) -> (float, float):\n",
    "    \"\"\"\n",
    "    Given the number of paired tokens that appear in the green list (green_count)\n",
    "    out of tokens_scored (only paired tokens are scored), compute a z–score and p–value.\n",
    "    Under the null hypothesis, each paired token is green with probability 0.5.\n",
    "    \"\"\"\n",
    "    import math\n",
    "    expected = 0.5 * tokens_scored\n",
    "    std = math.sqrt(0.25 * tokens_scored)\n",
    "    z = (green_count - expected) / std if std > 0 else 0.0\n",
    "    p = scipy.stats.norm.sf(z)\n",
    "    return z, p\n",
    "\n",
    "\n",
    "def compute_perplexity(model, tokenizer, text: str, device):\n",
    "    \"\"\"\n",
    "    Computes perplexity of the text using the provided model and tokenizer.\n",
    "    Here we use the language-modeling loss computed by the model.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)[\"input_ids\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "# === Cos similarity and greedy matching ===\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def construct_similarity_matrix_cos(vocab_list: list[str], indices: list[int], embedding_matrix: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Constructs an m x m similarity matrix for tokens specified by indices,\n",
    "    using cosine similarity between token embeddings.\n",
    "\n",
    "    Args:\n",
    "      vocab_list: List of all tokens (strings).\n",
    "      indices: List of token indices corresponding to the non-unique tokens (set B).\n",
    "      embedding_matrix: A torch.Tensor of shape (vocab_size, hidden_dim) representing the token embeddings.\n",
    "\n",
    "    Returns:\n",
    "      A torch.Tensor of shape (m, m) where each entry [i][j] is the cosine similarity between\n",
    "      the embeddings of vocab_list[indices[i]] and vocab_list[indices[j]], with the diagonal entries set to 0.\n",
    "    \"\"\"\n",
    "    # Extract embeddings for the selected tokens (non-unique tokens).\n",
    "    selected_embeddings = embedding_matrix[indices]  # shape: (m, hidden_dim)\n",
    "\n",
    "    # Normalize the embeddings along the feature dimension.\n",
    "    norm_embeddings = F.normalize(selected_embeddings, p=2, dim=1)\n",
    "\n",
    "    # Compute the cosine similarity matrix as the dot product between normalized embeddings.\n",
    "    sim_matrix = torch.mm(norm_embeddings, norm_embeddings.t())\n",
    "\n",
    "    # Set the diagonal entries to 0 (to ignore self-similarity).\n",
    "    sim_matrix.fill_diagonal_(0)\n",
    "\n",
    "    return sim_matrix\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "def find_perfect_matching_greedy_random(similarity_matrix: list[list[float]]) -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Constructs a greedy matching from the similarity matrix using random sampling.\n",
    "    In each iteration, a random token i is selected from the unmatched set.\n",
    "    Then, approximately ceil(log2(n)) tokens (where n is the current number of unmatched tokens)\n",
    "    are randomly sampled from the remaining tokens, and the token j with the highest similarity\n",
    "    (i.e. highest value in similarity_matrix[i][j]) is selected as a match.\n",
    "\n",
    "    The function returns a list of tuples (i, j) (with i < j) representing the matched token indices.\n",
    "    Note: The similarity matrix should have 0 on its diagonal.\n",
    "    \"\"\"\n",
    "    m = len(similarity_matrix)\n",
    "    unmatched = list(range(m))\n",
    "    matching = []\n",
    "\n",
    "    pbar = tqdm(total=len(unmatched)//2, desc=\"Greedy random matching\")\n",
    "    while len(unmatched) > 1:\n",
    "        n = len(unmatched)\n",
    "        # Set sample size to ceil(log2(n)); ensure at least one candidate.\n",
    "        sample_size = math.ceil(math.log(n, 2)) if n > 1 else 1\n",
    "        # Randomly choose one token i from the unmatched set.\n",
    "        i = random.choice(unmatched)\n",
    "        # Build a list of candidates (all unmatched tokens except i).\n",
    "        remaining = [x for x in unmatched if x != i]\n",
    "        # Adjust sample_size if there are fewer candidates than sample_size.\n",
    "        sample_size = min(sample_size, len(remaining))\n",
    "        # Randomly sample sample_size candidates.\n",
    "        candidates = random.sample(remaining, sample_size)\n",
    "        # Find the candidate j with maximum similarity with i.\n",
    "        best_j = candidates[0]\n",
    "        best_weight = similarity_matrix[i][best_j]\n",
    "        for j in candidates:\n",
    "            w = similarity_matrix[i][j]\n",
    "            if w > best_weight:\n",
    "                best_weight = w\n",
    "                best_j = j\n",
    "        # Add the pair (min(i, best_j), max(i, best_j)) for consistency.\n",
    "        matching.append((min(i, best_j), max(i, best_j)))\n",
    "        # Remove both tokens from the unmatched set.\n",
    "        unmatched.remove(i)\n",
    "        if best_j in unmatched:\n",
    "            unmatched.remove(best_j)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return matching\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# --- Helper: Check if both outputs meet length condition (≥195 tokens) ---\n",
    "def valid_length(wm_text, nw_text, tokenizer, min_tokens=195):\n",
    "    len_wm = len(tokenizer(wm_text)[\"input_ids\"])\n",
    "    len_nw = len(tokenizer(nw_text)[\"input_ids\"])\n",
    "    return (len_wm >= min_tokens) and (len_nw >= min_tokens)\n",
    "\n",
    "\n",
    "# === Helper functions for detection metrics ===\n",
    "\n",
    "def count_green_tokens_paired(tokenizer, watermark_processor, text: str) -> (int, int, float):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text and for each token (after a minimum prefix),\n",
    "    computes the green list based on the current prefix (using watermark_processor).\n",
    "    Only paired tokens (i.e. tokens that have a synonym pair) are counted (unique tokens are excluded).\n",
    "\n",
    "    Returns:\n",
    "      green_count: number of paired tokens that appear in the green list.\n",
    "      tokens_scored: number of tokens scored (only tokens that are paired).\n",
    "      proportion: green_count / tokens_scored.\n",
    "    \"\"\"\n",
    "    # Tokenize without special tokens\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    total_tokens = len(input_ids)\n",
    "    # Use a minimum prefix length; if not defined in the processor, default to 1.\n",
    "    start_idx = getattr(watermark_processor, \"min_prefix_len\", 1)\n",
    "    green_count = 0\n",
    "    tokens_scored = 0\n",
    "\n",
    "    # For each token position starting at start_idx, consider only if the token is a paired token.\n",
    "    for idx in range(start_idx, total_tokens):\n",
    "        token = input_ids[idx].item()\n",
    "        greenlist_ids = watermark_processor._get_greenlist_ids(input_ids[:idx])\n",
    "        #print(token)\n",
    "        # Only count if token is NOT in the unique set (i.e. token is paired)\n",
    "        if watermark_processor.unique_tokens is not None and token not in watermark_processor.unique_tokens:\n",
    "            tokens_scored += 1\n",
    "            #print(token in greenlist_ids)\n",
    "            if token in greenlist_ids:\n",
    "                green_count += 1\n",
    "\n",
    "    proportion = green_count / tokens_scored if tokens_scored > 0 else 0.0\n",
    "    return green_count, tokens_scored, proportion\n",
    "\n",
    "# ---- Main Evaluation Function ----\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc  # Instead of roc_auc_score\n",
    "\n",
    "def evaluate_watermarking(truncated_texts, model, tokenizer, args, args_cos=None):\n",
    "    \"\"\"\n",
    "    For each prompt in truncated_texts, generate completions using three methods:\n",
    "      1. Default watermark method (our method)\n",
    "      2. KGW method (via generate_kgw and detect_kgw)\n",
    "      3. Cosine–similarity based method (via generate with args_cos and detect_cos)\n",
    "\n",
    "    For each method, compute:\n",
    "      - Number of tokens generated\n",
    "      - Detection metrics (only on paired tokens): green token count, tokens scored, green fraction,\n",
    "        z–score and p–value, judgement (watermarked vs. non-watermarked), perplexity.\n",
    "\n",
    "    Only prompts where all outputs (for all three methods) have length ≥195 tokens are considered.\n",
    "    We then compute aggregated metrics (average perplexity) and an ROC–AUC by computing\n",
    "    the false-positive and true-positive rates (via roc_curve) and passing them to auc().\n",
    "\n",
    "    Returns:\n",
    "      results: a list of per–prompt result dictionaries.\n",
    "      aggregated: a dictionary with aggregated metrics for each method.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Accumulators for default method:\n",
    "    wm_z_default_acc, nw_z_default_acc, labels_default = [], [], []\n",
    "    ppl_wm_default_acc, ppl_nw_default_acc = [], []\n",
    "    green_counts_w_default_acc, tokens_scored_w_default_acc, props_w_default_acc = [], [], []\n",
    "\n",
    "    # For KGW method:\n",
    "    wm_z_kgw_acc, nw_z_kgw_acc, labels_kgw = [], [], []\n",
    "    ppl_wm_kgw_acc, ppl_nw_kgw_acc = [], []\n",
    "    green_counts_w_kgw_acc, tokens_scored_w_kgw_acc, props_w_kgw_acc = [], [], []\n",
    "\n",
    "    # For Cosine-based method:\n",
    "    wm_z_cos_acc, nw_z_cos_acc, labels_cos = [], [], []\n",
    "    ppl_wm_cos_acc, ppl_nw_cos_acc = [], []\n",
    "    green_counts_w_cos_acc, tokens_scored_w_cos_acc, props_w_cos_acc = [], [], []\n",
    "\n",
    "    for prompt in tqdm(truncated_texts, desc=\"Evaluating prompts\"):\n",
    "        # --- Default Watermark Generation ---\n",
    "        redecoded_input, truncation_warning, decoded_nw, decoded_wm, _ = generate(\n",
    "            prompt, args, model=model, device=device, tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        decoded_nw = synonym_substitution_attack(decoded_nw, tokenizer, attack_rate=1)\n",
    "        decoded_wm = synonym_substitution_attack(decoded_wm, tokenizer, attack_rate=1)\n",
    "        #print('generate finish')\n",
    "\n",
    "        wm_processor_default = WatermarkLogitsProcessor(\n",
    "            vocab=list(tokenizer.get_vocab().values()),\n",
    "            gamma=args.gamma,\n",
    "            delta=args.delta,\n",
    "            seeding_scheme=args.seeding_scheme,\n",
    "            select_green_tokens=args.select_green_tokens,\n",
    "            precomputed_pairing=args.precomputed_pairing,\n",
    "            unique_tokens=args.unique_tokens\n",
    "        )\n",
    "\n",
    "        # Count green tokens (only among paired tokens) for watermarked text:\n",
    "        green_count_w, tokens_scored_w, prop_w = count_green_tokens_paired(\n",
    "            tokenizer, wm_processor_default, decoded_wm\n",
    "        )\n",
    "        z_default, p_default = compute_p_value(green_count_w, tokens_scored_w)\n",
    "\n",
    "        # Non-watermarked text:\n",
    "        green_count_nw, tokens_scored_nw, prop_nw = count_green_tokens_paired(\n",
    "            tokenizer, wm_processor_default, decoded_nw\n",
    "        )\n",
    "        z_nw_default, p_nw_default = compute_p_value(green_count_nw, tokens_scored_nw)\n",
    "\n",
    "        judgement_default = (\n",
    "            \"LLM-generated (watermarked)\" if z_default > args.detection_z_threshold else \"Human-generated (non-watermarked)\"\n",
    "        )\n",
    "        perplexity_wm = compute_perplexity(model, tokenizer, decoded_wm, device)\n",
    "        perplexity_nw = compute_perplexity(model, tokenizer, decoded_nw, device)\n",
    "        tokens_generated_default = len(tokenizer(decoded_wm)[\"input_ids\"])\n",
    "\n",
    "        #print('detect finish')\n",
    "\n",
    "        # --- KGW Method ---\n",
    "        redecoded_input_kgw, truncation_warning_kgw, decoded_nw_kgw, decoded_wm_kgw, _ = generate_kgw(\n",
    "            prompt, args, model=model, device=device, tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        decoded_wm_kgw = synonym_substitution_attack(decoded_wm_kgw, tokenizer, attack_rate=1)\n",
    "\n",
    "        #print('generate finish')\n",
    "        detect_result_w_kgw = detect_kgw(decoded_wm_kgw, args, device=device, tokenizer=tokenizer)[1]\n",
    "        if len(decoded_wm_kgw) < 195:\n",
    "            continue\n",
    "        z_kgw = detect_result_w_kgw[\"z_score\"]\n",
    "        green_count_w_kgw = detect_result_w_kgw[\"num_green_tokens\"]\n",
    "        tokens_scored_w_kgw_local = detect_result_w_kgw[\"num_tokens_scored\"]\n",
    "        prop_w_kgw = detect_result_w_kgw[\"green_fraction\"]\n",
    "\n",
    "        detect_result_nw_kgw = detect_kgw(decoded_nw, args, device=device, tokenizer=tokenizer)[1]\n",
    "        print(decoded_nw)\n",
    "        if len(decoded_nw) < 195:\n",
    "            continue\n",
    "        z_nw_kgw = detect_result_nw_kgw[\"z_score\"]\n",
    "        green_count_nw_kgw = detect_result_nw_kgw[\"num_green_tokens\"]\n",
    "\n",
    "        judgement_kgw = (\n",
    "            \"LLM-generated (watermarked)\" if z_kgw > args.detection_z_threshold else \"Human-generated (non-watermarked)\"\n",
    "        )\n",
    "        perplexity_wm_kgw = compute_perplexity(model, tokenizer, decoded_wm_kgw, device)\n",
    "        perplexity_nw_kgw = compute_perplexity(model, tokenizer, decoded_nw_kgw, device)\n",
    "        tokens_generated_kgw = len(tokenizer(decoded_wm_kgw)[\"input_ids\"])\n",
    "\n",
    "        #print('detect finish')\n",
    "        # --- Cosine-based Method ---\n",
    "        redecoded_input_cos, truncation_warning_cos, decoded_nw_cos, decoded_wm_cos, _ = generate(\n",
    "            prompt, args_cos, model=model, device=device, tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        decoded_wm_cos = synonym_substitution_attack(decoded_wm_cos, tokenizer, attack_rate=1)\n",
    "\n",
    "        wm_processor_cos = WatermarkLogitsProcessor(\n",
    "            vocab=list(tokenizer.get_vocab().values()),\n",
    "            gamma=args_cos.gamma,\n",
    "            delta=args_cos.delta,\n",
    "            seeding_scheme=args_cos.seeding_scheme,\n",
    "            select_green_tokens=args_cos.select_green_tokens,\n",
    "            precomputed_pairing=args_cos.precomputed_pairing,\n",
    "            unique_tokens=args_cos.unique_tokens\n",
    "        )\n",
    "        green_count_w_cos, tokens_scored_w_cos, prop_w_cos = count_green_tokens_paired(\n",
    "            tokenizer, wm_processor_cos, decoded_wm_cos\n",
    "        )\n",
    "        z_cos, p_cos = compute_p_value(green_count_w_cos, tokens_scored_w_cos)\n",
    "\n",
    "        green_count_nw_cos, tokens_scored_nw_cos, prop_nw_cos = count_green_tokens_paired(\n",
    "            tokenizer, wm_processor_cos, decoded_nw\n",
    "        )\n",
    "        z_nw_cos, p_nw_cos = compute_p_value(green_count_nw_cos, tokens_scored_nw_cos)\n",
    "        judgement_cos = (\n",
    "            \"LLM-generated (watermarked)\" if z_cos > args_cos.detection_z_threshold else \"Human-generated (non-watermarked)\"\n",
    "        )\n",
    "        perplexity_wm_cos = compute_perplexity(model, tokenizer, decoded_wm_cos, device)\n",
    "        perplexity_nw_cos = compute_perplexity(model, tokenizer, decoded_nw_cos, device)\n",
    "        tokens_generated_cos = len(tokenizer(decoded_wm_cos)[\"input_ids\"])\n",
    "\n",
    "        # Store per-prompt results\n",
    "        result = {\n",
    "            \"prompt\": redecoded_input,\n",
    "            \"default\": {\n",
    "                \"decoded_wm\": decoded_wm,\n",
    "                \"decoded_nw\": decoded_nw,\n",
    "                \"green_count_w\": green_count_w,\n",
    "                \"green_count_nw\": green_count_nw,\n",
    "                \"tokens_scored_w\": tokens_scored_w,\n",
    "                \"tokens_scored_nw\": tokens_scored_nw,\n",
    "                \"prop_w\": prop_w,\n",
    "                \"z_w\": z_default,\n",
    "                \"z_nw\": z_nw_default,\n",
    "                \"p_w\": p_default,\n",
    "                \"judgement\": judgement_default,\n",
    "                \"ppl_wm\": perplexity_wm,\n",
    "                \"ppl_nw\": perplexity_nw,\n",
    "                \"tokens_generated\": tokens_generated_default\n",
    "            },\n",
    "            \"kgw\": {\n",
    "                \"decoded_wm\": decoded_wm_kgw,\n",
    "                \"green_count_w\": green_count_w_kgw,\n",
    "                \"tokens_scored_w\": tokens_scored_w_kgw_local,\n",
    "                \"prop_w\": prop_w_kgw,\n",
    "                \"z_w\": z_kgw,\n",
    "                \"p_w\": detect_result_w_kgw.get(\"p_value\", None),\n",
    "                \"judgement\": judgement_kgw,\n",
    "                \"ppl_wm\": perplexity_wm_kgw,\n",
    "                \"ppl_nw\": perplexity_nw_kgw,\n",
    "                \"tokens_generated\": tokens_generated_kgw\n",
    "            },\n",
    "            \"cos\": {\n",
    "                \"decoded_wm\": decoded_wm_cos,\n",
    "                \"green_count_w\": green_count_w_cos,\n",
    "                \"tokens_scored_w\": tokens_scored_w_cos,\n",
    "                \"prop_w\": prop_w_cos,\n",
    "                \"z_w\": z_cos,\n",
    "                \"p_w\": p_cos,\n",
    "                \"judgement\": judgement_cos,\n",
    "                \"ppl_wm\": perplexity_wm_cos,\n",
    "                \"ppl_nw\": perplexity_nw_cos,\n",
    "                \"tokens_generated\": tokens_generated_cos\n",
    "            }\n",
    "        }\n",
    "        results.append(result)\n",
    "        # Optional prompt-level printing:\n",
    "        print(result)\n",
    "\n",
    "        # Only consider prompts where all three methods produce outputs of length ≥ 195 tokens.\n",
    "        if (valid_length(decoded_wm, decoded_nw, tokenizer) and\n",
    "            valid_length(decoded_wm_kgw, decoded_nw_kgw, tokenizer) and\n",
    "            valid_length(decoded_wm_cos, decoded_nw_cos, tokenizer)):\n",
    "\n",
    "            # Accumulate Default method metrics.\n",
    "            wm_z_default_acc.append(z_default)\n",
    "            nw_z_default_acc.append(z_nw_default)\n",
    "            labels_default = [1] * len(wm_z_default_acc) + [0] * len(nw_z_default_acc)\n",
    "            ppl_wm_default_acc.append(perplexity_wm)\n",
    "            ppl_nw_default_acc.append(perplexity_nw)\n",
    "            green_counts_w_default_acc.append(green_count_w)\n",
    "            tokens_scored_w_default_acc.append(tokens_scored_w)\n",
    "\n",
    "            # KGW method accumulators.\n",
    "            wm_z_kgw_acc.append(z_kgw)\n",
    "            nw_z_kgw_acc.append(z_nw_kgw)\n",
    "            labels_kgw = [1] * len(wm_z_kgw_acc) + [0] * len(nw_z_kgw_acc)\n",
    "            ppl_wm_kgw_acc.append(perplexity_wm_kgw)\n",
    "            ppl_nw_kgw_acc.append(perplexity_nw_kgw)\n",
    "            green_counts_w_kgw_acc.append(green_count_w_kgw)\n",
    "            tokens_scored_w_kgw_acc.append(tokens_scored_w_kgw_local)\n",
    "\n",
    "            # Cosine-based method accumulators.\n",
    "            wm_z_cos_acc.append(z_cos)\n",
    "            nw_z_cos_acc.append(z_nw_cos)\n",
    "            labels_cos = [1] * len(wm_z_cos_acc) + [0] * len(nw_z_cos_acc)\n",
    "            ppl_wm_cos_acc.append(perplexity_wm_cos)\n",
    "            ppl_nw_cos_acc.append(perplexity_nw_cos)\n",
    "            green_counts_w_cos_acc.append(green_count_w_cos)\n",
    "            tokens_scored_w_cos_acc.append(tokens_scored_w_cos)\n",
    "\n",
    "            num_valid = len(ppl_wm_default_acc)\n",
    "            curr_avg_ppl_wm_default = np.mean(ppl_wm_default_acc)\n",
    "            curr_avg_ppl_nw_default = np.mean(ppl_nw_default_acc)\n",
    "            curr_avg_ppl_wm_kgw = np.mean(ppl_wm_kgw_acc)\n",
    "            curr_avg_ppl_nw_kgw = np.mean(ppl_nw_kgw_acc)\n",
    "            curr_avg_ppl_wm_cos = np.mean(ppl_wm_cos_acc)\n",
    "            curr_avg_ppl_nw_cos = np.mean(ppl_nw_cos_acc)\n",
    "            curr_avg_z_wm_default = np.mean(wm_z_default_acc)\n",
    "            curr_avg_z_nw_default = np.mean(nw_z_default_acc)\n",
    "            curr_avg_z_wm_kgw = np.mean(wm_z_kgw_acc)\n",
    "            curr_avg_z_nw_kgw = np.mean(nw_z_kgw_acc)\n",
    "            curr_avg_z_wm_cos = np.mean(wm_z_cos_acc)\n",
    "            curr_avg_z_nw_cos = np.mean(nw_z_cos_acc)\n",
    "\n",
    "            # -- Use roc_curve() and auc() here instead of roc_auc_score() --\n",
    "            try:\n",
    "                # For default method:\n",
    "                all_scores_default = wm_z_default_acc + nw_z_default_acc\n",
    "                fpr_def, tpr_def, _ = roc_curve(labels_default, all_scores_default)\n",
    "                curr_auc_default = auc(fpr_def, tpr_def)\n",
    "                #print(all_scores_default)\n",
    "                #print(wm_z_default_acc)\n",
    "                #print(nw_z_default_acc)\n",
    "            except Exception:\n",
    "                curr_auc_default = float('nan')\n",
    "\n",
    "            try:\n",
    "                # For KGW method:\n",
    "                all_scores_kgw = wm_z_kgw_acc + nw_z_kgw_acc\n",
    "                fpr_kgw, tpr_kgw, _ = roc_curve(labels_kgw, all_scores_kgw)\n",
    "                curr_auc_kgw = auc(fpr_kgw, tpr_kgw)\n",
    "                #print(all_scores_kgw)\n",
    "                #print(wm_z_kgw_acc)\n",
    "                #print(nw_z_kgw_acc)\n",
    "            except Exception:\n",
    "                curr_auc_kgw = float('nan')\n",
    "\n",
    "            try:\n",
    "                # For Cosine-based method:\n",
    "                all_scores_cos = wm_z_cos_acc + nw_z_cos_acc\n",
    "                fpr_cos, tpr_cos, _ = roc_curve(labels_cos, all_scores_cos)\n",
    "                curr_auc_cos = auc(fpr_cos, tpr_cos)\n",
    "                #print(all_scores_cos)\n",
    "                #print(wm_z_cos_acc)\n",
    "                #print(nw_z_cos_acc)\n",
    "            except Exception:\n",
    "                curr_auc_cos = float('nan')\n",
    "\n",
    "            print(f\"\\nAfter {num_valid} valid prompts:\")\n",
    "            print(\" Default Method:    avg ppl (wm) = {0:.2f}, avg ppl (nw) = {1:.2f}, AUC = {2:.3f}, avg z (wm) = {3:.2f}, avg z (nw) = {4:.2f}\".format(\n",
    "                curr_avg_ppl_wm_default, curr_avg_ppl_nw_default, curr_auc_default, curr_avg_z_wm_default, curr_avg_z_nw_default))\n",
    "            print(\" KGW Method:        avg ppl (wm) = {0:.2f}, avg ppl (nw) = {1:.2f}, AUC = {2:.3f}, avg z (wm) = {3:.2f}, avg z (nw) = {4:.2f}\".format(\n",
    "                curr_avg_ppl_wm_kgw, curr_avg_ppl_nw_kgw, curr_auc_kgw, curr_avg_z_wm_kgw, curr_avg_z_nw_kgw))\n",
    "            print(\" Cosine-based Method: avg ppl (wm) = {0:.2f}, avg ppl (nw) = {1:.2f}, AUC = {2:.3f}, avg z (wm) = {3:.2f}, avg z (nw) = {4:.2f}\\n\".format(\n",
    "                curr_avg_ppl_wm_cos, curr_avg_ppl_nw_cos, curr_auc_cos, curr_avg_z_wm_cos, curr_avg_z_nw_cos))\n",
    "\n",
    "    # Final aggregated metrics\n",
    "    aggregated = {}\n",
    "\n",
    "    # Summaries for each method:\n",
    "    def finalize_metrics(labels, wm_z_list, nw_z_list, ppl_wm_list, ppl_nw_list, method_name):\n",
    "        out_dict = {}\n",
    "        if len(ppl_wm_list) > 0:\n",
    "            out_dict[\"avg_ppl_wm\"] = np.mean(ppl_wm_list)\n",
    "            out_dict[\"avg_ppl_nw\"] = np.mean(ppl_nw_list)\n",
    "            out_dict[\"avg_z_wm\"] = np.mean(wm_z_list)\n",
    "            out_dict[\"avg_z_nw\"] = np.mean(nw_z_list)\n",
    "            out_dict[\"num_valid\"] = len(ppl_wm_list)\n",
    "        else:\n",
    "            out_dict[\"avg_ppl_wm\"] = None\n",
    "            out_dict[\"avg_ppl_nw\"] = None\n",
    "            out_dict[\"avg_z_wm\"] = None\n",
    "            out_dict[\"avg_z_nw\"] = None\n",
    "            out_dict[\"num_valid\"] = 0\n",
    "\n",
    "        if wm_z_list and nw_z_list:\n",
    "            all_scores = wm_z_list + nw_z_list\n",
    "            try:\n",
    "                fpr, tpr, _ = roc_curve(labels, all_scores)\n",
    "                out_dict[\"auc\"] = auc(fpr, tpr)\n",
    "            except Exception:\n",
    "                out_dict[\"auc\"] = None\n",
    "        else:\n",
    "            out_dict[\"auc\"] = None\n",
    "\n",
    "        aggregated[method_name] = out_dict\n",
    "\n",
    "    # Default\n",
    "    finalize_metrics(labels_default,\n",
    "                     wm_z_default_acc, nw_z_default_acc,\n",
    "                     ppl_wm_default_acc, ppl_nw_default_acc,\n",
    "                     \"default\")\n",
    "\n",
    "    # KGW\n",
    "    finalize_metrics(labels_kgw,\n",
    "                     wm_z_kgw_acc, nw_z_kgw_acc,\n",
    "                     ppl_wm_kgw_acc, ppl_nw_kgw_acc,\n",
    "                     \"kgw\")\n",
    "\n",
    "    # Cosine\n",
    "    finalize_metrics(labels_cos,\n",
    "                     wm_z_cos_acc, nw_z_cos_acc,\n",
    "                     ppl_wm_cos_acc, ppl_nw_cos_acc,\n",
    "                     \"cos\")\n",
    "\n",
    "    return results, aggregated\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\11132\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T22:03:58.243810Z",
     "start_time": "2025-03-23T22:03:58.238822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def synonym_substitution_attack(text, tokenizer, attack_rate=1.0):\n",
    "    \"\"\"\n",
    "    Performs a substitution attack by replacing tokens with synonyms.\n",
    "    attack_rate: Fraction of tokens to replace (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    attacked_tokens = []\n",
    "\n",
    "    # Determine how many tokens to attack\n",
    "    num_tokens = len(tokens)\n",
    "    num_attacks = int(num_tokens * attack_rate)\n",
    "\n",
    "    # Randomly choose indices to attack\n",
    "    attack_indices = set(random.sample(range(num_tokens), num_attacks))\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        base_token = token.lstrip(\"Ġ\")\n",
    "        if idx in attack_indices:\n",
    "            synsets = wn.synsets(base_token)\n",
    "            synonyms = set(lemma.lower() for syn in synsets for lemma in syn.lemma_names())\n",
    "            synonyms.discard(base_token.lower())\n",
    "            if synonyms:\n",
    "                new_token = random.choice(list(synonyms))\n",
    "                if token.startswith(\"Ġ\"):\n",
    "                    new_token = \"Ġ\" + new_token\n",
    "                attacked_tokens.append(new_token)\n",
    "            else:\n",
    "                attacked_tokens.append(token)\n",
    "        else:\n",
    "            attacked_tokens.append(token)\n",
    "\n",
    "    attacked_text = tokenizer.convert_tokens_to_string(attacked_tokens)\n",
    "    return attacked_text\n"
   ],
   "id": "e7bdc651444209c8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:43:51.232498Z",
     "start_time": "2025-03-23T20:43:40.691539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]]\n",
    "    args = parse_args()\n",
    "    model, tokenizer, device = load_model(args)\n",
    "\n"
   ],
   "id": "cbd76460157dc46a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T21:13:29.275643Z",
     "start_time": "2025-03-23T20:44:14.224477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "    # Assume truncated_texts is a list of 500 prompt strings (each truncated to less than 200 words).\n",
    "    # For example, you may load them from a file or sample from a dataset.\n",
    "    # Here we assume truncated_texts is already defined.\n",
    "\n",
    "    # --- Precompute Vocabulary and Perfect Matching via Dictionary ---\n",
    "    tokenizer_for_vocab = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    vocab_list = get_vocabulary(tokenizer_for_vocab)\n",
    "    print(f\"Vocabulary size: {len(vocab_list)}\")\n",
    "    unique_indices, paired_indices = filter_tokens_with_synonyms(vocab_list)\n",
    "    print(f\"Unique tokens (set A): {len(unique_indices)}\")\n",
    "    print(f\"Tokens with synonyms (set B): {len(paired_indices)}\")\n",
    "    similarity_matrix = construct_similarity_matrix(vocab_list, paired_indices)\n",
    "    matching = find_perfect_matching(similarity_matrix)\n",
    "    mapped_pairing = [(paired_indices[i], paired_indices[j]) for (i, j) in matching]\n",
    "    args.precomputed_pairing = mapped_pairing\n",
    "    args.unique_tokens = unique_indices\n",
    "\n",
    "    # --- Precompute Vocabulary and Perfect Matching via Cosine Similarity ---\n",
    "    args_cos = parse_args()\n",
    "    embedding_matrix = model.get_input_embeddings().weight  # shape: (vocab_size, hidden_dim)"
   ],
   "id": "833643572e34d3a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering vocabulary: 100%|██████████| 50265/50265 [02:58<00:00, 281.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens (set A): 28109\n",
      "Tokens with synonyms (set B): 22156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing similarity matrix (outer loop): 100%|██████████| 22156/22156 [01:17<00:00, 287.04it/s] \n",
      "Building graph for matching: 100%|██████████| 245433090/245433090 [01:46<00:00, 2314160.89it/s]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T21:13:43.071267Z",
     "start_time": "2025-03-23T21:13:30.577294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    similarity_matrix_cos = construct_similarity_matrix_cos(vocab_list, paired_indices,embedding_matrix)\n",
    "    matching_cos = find_perfect_matching_greedy_random(similarity_matrix_cos)\n",
    "    mapped_pairing_cos = [(paired_indices[i], paired_indices[j]) for (i, j) in matching_cos]\n",
    "    args_cos.precomputed_pairing = mapped_pairing_cos\n",
    "    args_cos.unique_tokens = unique_indices"
   ],
   "id": "34db1ad8d80096d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy random matching: 100%|██████████| 11078/11078 [00:11<00:00, 946.62it/s] \n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T21:14:24.034037Z",
     "start_time": "2025-03-23T21:13:43.074296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "    # --- Load the \"realnewslike\" subset of C4 (English) and Shuffle the dataset with a fixed seed for reproducibility ---\n",
    "    c4_realnewslike = load_dataset(\"c4\", \"realnewslike\", split=\"train\", streaming=False, trust_remote_code=True)\n",
    "    shuffled_dataset = c4_realnewslike.shuffle(seed=45)\n",
    "    sampled_examples = shuffled_dataset.select(range(300))\n",
    "    sampled_texts = [example[\"text\"] for example in sampled_examples]\n",
    "    print(f\"Sampled {len(sampled_texts)} news-like texts from C4.\")\n",
    "    max_words = 150\n",
    "    truncated_texts = []\n",
    "    for text in sampled_texts:\n",
    "        words = text.split()  # split text into words\n",
    "        truncated_text = \" \".join(words[:max_words])\n",
    "        truncated_texts.append(truncated_text)\n"
   ],
   "id": "b5bd9dc875c4645b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11132\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\c4\\584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db\\c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 300 news-like texts from C4.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Delta = 5, gemma = 0.25, subsititution attack**",
   "id": "247a751f727feb81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T00:39:26.576529Z",
     "start_time": "2025-03-24T00:39:26.573453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tune_delta = 5\n",
    "args.delta = tune_delta\n",
    "args_cos.delta = tune_delta"
   ],
   "id": "36ba9cbdcf21d6b9",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T03:03:30.153733Z",
     "start_time": "2025-03-24T00:39:36.054723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "    results, aggregated = evaluate_watermarking(truncated_texts, model, tokenizer, args, args_cos)\n",
    "\n",
    "    # Print per-prompt results for the first 5 prompts.\n",
    "    for r in results[:5]:\n",
    "        print(\"=== Prompt ===\")\n",
    "        print(r[\"prompt\"])\n",
    "        print(\"--- Default Method ---\")\n",
    "        print(\"Watermarked Text:\")\n",
    "        print(r[\"default\"][\"decoded_wm\"])\n",
    "        print(\"Detection (Default):\")\n",
    "        print(f\"  Green tokens (paired): {r['default']['green_count_w']} / {r['default']['tokens_scored_w']} ({r['default']['prop_w']:.2%})\")\n",
    "        print(f\"  z–score: {r['default']['z_w']:.2f}, p–value: {r['default']['p_w']:.4f}\")\n",
    "        print(f\"  Judgement: {r['default']['judgement']}\")\n",
    "        print(f\"  Perplexity: {r['default']['ppl_wm']:.2f}\")\n",
    "        print(\"--- KGW Method ---\")\n",
    "        print(\"Watermarked Text:\")\n",
    "        print(r[\"kgw\"][\"decoded_wm\"])\n",
    "        print(\"Detection (KGW):\")\n",
    "        print(f\"  Green tokens (paired): {r['kgw']['green_count_w']} / {r['kgw']['tokens_scored_w']} ({r['kgw']['prop_w']:.2%})\")\n",
    "        print(f\"  z–score: {r['kgw']['z_w']:.2f}, p–value: {r['kgw']['p_w']:.4f}\")\n",
    "        print(f\"  Judgement: {r['kgw']['judgement']}\")\n",
    "        print(f\"  Perplexity: {r['kgw']['ppl_wm']:.2f}\")\n",
    "        print(\"--- Cosine-based Method ---\")\n",
    "        print(\"Watermarked Text:\")\n",
    "        print(r[\"cos\"][\"decoded_wm\"])\n",
    "        print(\"Detection (Cos):\")\n",
    "        print(f\"  Green tokens (paired): {r['cos']['green_count_w']} / {r['cos']['tokens_scored_w']} ({r['cos']['prop_w']:.2%})\")\n",
    "        print(f\"  z–score: {r['cos']['z_w']:.2f}, p–value: {r['cos']['p_w']:.4f}\")\n",
    "        print(f\"  Judgement: {r['cos']['judgement']}\")\n",
    "        print(f\"  Perplexity: {r['cos']['ppl_wm']:.2f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Print aggregated metrics.\n",
    "    print(\"=== Aggregated Metrics ===\")\n",
    "    print(\"Default Method:\")\n",
    "    print(\"  Average Perplexity (Watermarked):\", aggregated[\"default\"][\"avg_ppl_wm\"])\n",
    "    print(\"  Average Perplexity (Non-watermarked):\", aggregated[\"default\"][\"avg_ppl_nw\"])\n",
    "    print(\"  Average z-score (Watermarked):\", aggregated[\"default\"][\"avg_z_wm\"])\n",
    "    print(\"  Average z-score (Non-watermarked):\", aggregated[\"default\"][\"avg_z_nw\"])\n",
    "    print(\"  AUC:\", aggregated[\"default\"][\"auc\"])\n",
    "    print(\"  Valid prompts:\", aggregated[\"default\"][\"num_valid\"])\n",
    "\n",
    "    print(\"\\nKGW Method:\")\n",
    "    print(\"  Average Perplexity (Watermarked):\", aggregated[\"kgw\"][\"avg_ppl_wm\"])\n",
    "    print(\"  Average Perplexity (Non-watermarked):\", aggregated[\"kgw\"][\"avg_ppl_nw\"])\n",
    "    print(\"  Average z-score (Watermarked):\", aggregated[\"kgw\"][\"avg_z_wm\"])\n",
    "    print(\"  Average z-score (Non-watermarked):\", aggregated[\"kgw\"][\"avg_z_nw\"])\n",
    "    print(\"  AUC:\", aggregated[\"kgw\"][\"auc\"])\n",
    "    print(\"  Valid prompts:\", aggregated[\"kgw\"][\"num_valid\"])\n",
    "\n",
    "    print(\"\\nCosine-based Method:\")\n",
    "    print(\"  Average Perplexity (Watermarked):\", aggregated[\"cos\"][\"avg_ppl_wm\"])\n",
    "    print(\"  Average Perplexity (Non-watermarked):\", aggregated[\"cos\"][\"avg_ppl_nw\"])\n",
    "    print(\"  Average z-score (Watermarked):\", aggregated[\"cos\"][\"avg_z_wm\"])\n",
    "    print(\"  Average z-score (Non-watermarked):\", aggregated[\"cos\"][\"avg_z_nw\"])\n",
    "    print(\"  AUC:\", aggregated[\"cos\"][\"auc\"])\n",
    "    print(\"  Valid prompts:\", aggregated[\"cos\"][\"num_valid\"])"
   ],
   "id": "95ea92a4178e5f8f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The nigh canonic ruler atomic_number_49 clothe live to vest but what you lavatory open to misplace, merely you potty't give to recede angstrom sight. inquirying fellowship be ampere dandy means to realise them and ready your clothe conclusion.\n",
      "\n",
      "The near canonical govern inward commit be to adorn just what you potty open to fall_back, only you posterior't open to miss angstrom hatful. searching troupe exist amp bully manner to translate them and progress_to your commit determination.\n",
      "\n",
      "phosphorusicking the winner and failure indiana this progressively fickle market_place cost type_a take_exception. The marketplace turn upward, and_then information_technology depart refine, and_so information_technology go_bad astir once_more, and and_so information_technology extend low. How privy you recite the difference_of_opinion? hera comprise amp few elementary point.\n",
      "\n",
      "The to_the_highest_degree canonic dominion indium vest personify to endow simply what you tail open to recede, only you give_the_sack't open to miss ampere wad. inquirying party constitute group_a corking style to interpret them and give your commit decision.\n",
      "\n",
      "This follow the endorsement indium deoxyadenosine_monophosphate serial_publication of clause that\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   0%|          | 1/300 [17:50<88:56:04, 1070.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Choose any of the INOV videos above to watch, by clicking the associated image or headline. These results are drawn from the library of videos produced here at Market News Video, that have been tagged by an editor with the inov symbol. The date of each video is listed underneath the headline. Beneath the listing of inov videos is a current stock quote for inov and performance chart. At the bottom of the page, you will find related articles mentioning inov. From all of us here at Market News Video, we hope you will enjoy these inov videos and articles.', 'default': {'decoded_wm': '\\n\\nThe nigh recent_epoch indianaorange_group subject make_up create along feb, twenty-seven, 2013.\\n\\nThe near holocene atomic_number_49orange_group line_of_descent inverted_comma and inwardsorange_group malcolm_stock operation exist produce along feb, twenty-seven, 2013.\\n\\nmart word telecasting\\n\\ngrocery_store word telecasting embody the beginning for one_thousand_thousand of picture, get_over diverse securities_industry effect, include tidings, marketplace depth_psychology, technological insight, and former topic. inwardorange_group tv follow create and update aside the inwardorange_group team_up, and this place exist consecrate to allow you with the about precise and improving-to-date_stamp video_recording. We trust you savour the video_recording.', 'decoded_nw': \"\\n\\nThe nigh canonic ruler atomic_number_49 clothe live to vest but what you lavatory open to misplace, merely you potty't give to recede angstrom sight. inquirying fellowship be ampere dandy means to realise them and ready your clothe conclusion.\\n\\nThe near canonical govern inward commit be to adorn just what you potty open to fall_back, only you posterior't open to miss angstrom hatful. searching troupe exist amp bully manner to translate them and progress_to your commit determination.\\n\\nphosphorusicking the winner and failure indiana this progressively fickle market_place cost type_a take_exception. The marketplace turn upward, and_then information_technology depart refine, and_so information_technology go_bad astir once_more, and and_so information_technology extend low. How privy you recite the difference_of_opinion? hera comprise amp few elementary point.\\n\\nThe to_the_highest_degree canonic dominion indium vest personify to endow simply what you tail open to recede, only you give_the_sack't open to miss ampere wad. inquirying party constitute group_a corking style to interpret them and give your commit decision.\\n\\nThis follow the endorsement indium deoxyadenosine_monophosphate serial_publication of clause that\", 'green_count_w': 47, 'green_count_nw': 73, 'tokens_scored_w': 86, 'tokens_scored_nw': 141, 'prop_w': 0.5465116279069767, 'z_w': 0.8626621856275073, 'z_nw': 0.42107596053325946, 'p_w': np.float64(0.19416161448946234), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 151.06736755371094, 'ppl_nw': 172.83395385742188, 'tokens_generated': 191}, 'kgw': {'decoded_wm': ' If thither ar indianaorange_group you would desire feature hera along marketplace news_show cinqueimeo, surgery early video_recording atomic_number_49 former family atomic_number_85 our atomic_number_23imeo place - impinging the locate. If on_that_point follow indianaorange_group you would desire sport hither beaver_state early video inward former family astatine our phoebeimeo website - contact_lens the internet_site. glucinium considerably. - securities_industry day-by-day', 'green_count_w': 36, 'tokens_scored_w': 116, 'prop_w': 0.3103448275862069, 'z_w': 1.5009575487729128, 'p_w': np.float64(0.06668327089634107), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 429.8070373535156, 'ppl_nw': 2.9193015098571777, 'tokens_generated': 118}, 'cos': {'decoded_wm': '\\n\\nThe indianaorange_group-sextuplet make_up associate_in_nursing passing sport picture, and the generator’entropy receive unspoilt penchant. helium piddle the proofreader flavour right some himself/herego, and that’ll bring_in your investment decisiveness promiscuous. – The assume\\n\\nYou’ll find_out these television urge among our visitant’ favorite, and we trust you’ll notice them vitamin_a harbour and illuminating.\\n\\nfood_market news_show follow the #one site for bribe and merchandising pedigree, selection, future, and stemhug_drug. We throw equal indium concern since 1995.\\n\\nWe throw part end-to-end magnetic_north united_states_of_america and endeavor to bring_home_the_bacon the to_the_highest_degree comp reporting of the mart useable. You seat obtain our entropy along the primary paginate of our internet_site, and you volition a_great_deal line_up this selective_information constitute as_well gift along our tidings and psychoanalysis pageboy.\\n\\nWe personify axerophthol office of the Jim ascorbic_acidramer’mho legal_action alertingreciprocal_ohm summation portfolio.\\n\\ncommercialise news_show exist the #one internet_site for purchase and deal malcolm_stock, option, futurity', 'green_count_w': 72, 'tokens_scored_w': 129, 'prop_w': 0.5581395348837209, 'z_w': 1.3206763594884356, 'p_w': np.float64(0.09330464993739535), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 197.52786254882812, 'ppl_nw': 2.9193015098571777, 'tokens_generated': 314}}\n",
      " suppose they embody non inquire the incidental and rich_person non invite whatsoever charge. “This represent angstrom identical distressing twenty-four_hours for the urban_center of to_the_fullerton,” Domemergency_room pronounce inward axerophthol argument. “We cost give to ensure that our patrol military_officer be rubber and esteem piece along responsibility.” The wide-cuterton’entropy police_force section’southward web_site enjoin the section get angstrom “cipher margin insurance_policy” for wholly misdemean. “The fullyerton law section engage type_a professional_person, honorable and master stock of deportment,” the internet_site state. “The section live dedicate to hold the gamy standard of behave indium our officer, and totally employee embody retain to the high-pitched honorable banner.” The full_phase_of_the_moonerton law section, nevertheless, give face critique inwards late age for betray to adequately do_by intimate mishandle allegation and for information_technology wield of early wrongful_conduct compositor's_case, let_in allegation of extravagant storm. indium July, the broaderton metropolis Council hear from xv house_physician world_health_organization pick_apart the section and consecrate to inquire. just_about\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   1%|          | 2/300 [39:38<100:10:35, 1210.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Fullerton Police Chief David Hendricks and Capt. Thomas Oliveras are on leave while police investigate an incident at a concert in Irvine Friday, when the men were off duty. Fullerton officers were informed Saturday, Aug. 25, that the men were placed on paid administrative leave and Capt. Bob Dunn is now serving as acting chief, according to a memo provided by Fullerton Sgt. Jon Radus, a police department spokesman. Irvine police are handling the investigation, and Fullerton City Manager Ken Domer said the city will hire an independent investigator to look at the incident. Hendricks and Oliveras could not be reached Monday afternoon for comment. Hendricks was named Fullerton’s chief in July 2017 after 23 years with the Long Beach Police Department. Oliveras has been a Fullerton officer for about 25 years and was promoted to captain in 2015, according to an article from Behind the Badge OC. Irvine police', 'default': {'decoded_wm': ' aforesaid they make_up go_over the site soundly and involve that the populace, american_samoa swell equally those world_health_organization personify thither, non interpose with their on-going investigating. The incidental lead_off just_about x:xxx phosphorus.k. when law constitute yell to associate_in_nursing affray atomic_number_85 The universal_gas_constantoxy astatine the Irvine revolve_about. The human_race, both atomic_number_49 their tardily-twentinternet_explorer, make_up get_hold_of into hold afterward antiophthalmic_factor scant foundation skin, Irvine constabulary sound_out. The incidental happen along the terrace of the locale, and the berth intensify american_samoa the workforce brook hoosier_state the terrace, associate_in_nursing Irvine police_force military_officer suppose. “The human_beings turn belligerent and the policeman comprise redact into veneration for their base_hit,” Irvine law Lt. Jeff Elliott enjoin. “The officeholder represent range indiana handlock and the isle_of_man make_up get_hold_of into detention.” The humanity live exhaust from hands without direction. The humankind represent postulate to the orangeness County immure for interrogate. constabulary exist deduct the identity_operator of the mistrust and their mature because of the probe. Irvine police_force state they equal review physical_structure-don', 'decoded_nw': \" suppose they embody non inquire the incidental and rich_person non invite whatsoever charge. “This represent angstrom identical distressing twenty-four_hours for the urban_center of to_the_fullerton,” Domemergency_room pronounce inward axerophthol argument. “We cost give to ensure that our patrol military_officer be rubber and esteem piece along responsibility.” The wide-cuterton’entropy police_force section’southward web_site enjoin the section get angstrom “cipher margin insurance_policy” for wholly misdemean. “The fullyerton law section engage type_a professional_person, honorable and master stock of deportment,” the internet_site state. “The section live dedicate to hold the gamy standard of behave indium our officer, and totally employee embody retain to the high-pitched honorable banner.” The full_phase_of_the_moonerton law section, nevertheless, give face critique inwards late age for betray to adequately do_by intimate mishandle allegation and for information_technology wield of early wrongful_conduct compositor's_case, let_in allegation of extravagant storm. indium July, the broaderton metropolis Council hear from xv house_physician world_health_organization pick_apart the section and consecrate to inquire. just_about\", 'green_count_w': 81, 'green_count_nw': 82, 'tokens_scored_w': 144, 'tokens_scored_nw': 134, 'prop_w': 0.5625, 'z_w': 1.5, 'z_nw': 2.5916052767440805, 'p_w': np.float64(0.06680720126885806), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 187.82708740234375, 'ppl_nw': 308.8557434082031, 'tokens_generated': 329}, 'kgw': {'decoded_wm': ' suppose recent sabbatum information_technology give nobelium boost data to put_up along associate_in_nursing incidental. The Irvine disc-jockey ergocalciferol’areproceed concert take_in 1000 world_health_organization wad onto triplet snitch atomic_number_85 atomic_number_57mutton_quadmolar_concentrationlupus_erythematosus outstanding granville_stanley_hall to try disc_jockey five_hundred’arkansasrun_low, make_out for his dispatch remixeastward of strain. The display, which get_down with disk-jockey calciferol’argonexpire, feature respective dj include disc_jockey can assassinator on with disk_jockey vitamin_d’arerun, Steve axerophtholviladenine and snortwye, among others. The indicate, which start with disc_jockey cholecalciferol’argonmove, sport various disk-jockey let_in disk-jockey 500’land_of_opportunityperish, Steve ampvilvitamin_a and wenchwye, among others. The show_up, which start with disk-jockey 500’atomic_number_18run_short, feature various dj let_in disk_jockey five_hundred’areget_going, Steve type_avilangstrom and snortatomic_number_39, among others. line TO blue-penciloregon/uyears: THIS make_up associate_in_nursingIMalabama-phosphorusERSONatomic_number_13costATION CONTENT AND chiliadfruit_drink FOR inwardstiLLimmune_serum_globulinENCIAL demodePERIENCE PUR', 'green_count_w': 117, 'tokens_scored_w': 387, 'prop_w': 0.3023255813953488, 'z_w': 2.3772174470791843, 'p_w': np.float64(0.008721900768019767), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 123.20262908935547, 'ppl_nw': 5.968354225158691, 'tokens_generated': 389}, 'cos': {'decoded_wm': ' fare non hold remark. Irvine city_manager Katrina Foley severalise the vocalism, “The Irvine constabulary comprise treatment the probe and the urban_center of Irvine bear dead nobelium notice.”\\n\\nploughshare this: impress\\n\\nreckon sir_thomas_more along The quicksilver news_show', 'green_count_w': 12, 'tokens_scored_w': 27, 'prop_w': 0.4444444444444444, 'z_w': -0.5773502691896257, 'p_w': np.float64(0.7181485691746134), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 662.3580322265625, 'ppl_nw': 5.968354225158691, 'tokens_generated': 70}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   1%|          | 3/300 [39:39<54:17:16, 658.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bring nineteen full_stop along vii-of-dozen scud. just, atomic_number_2 produce only terzetto-of-thirteen jibe from ternion-show mountain_chain. The Boilalmighty make_up pass aside hold emir Coffey with thirty-two compass_point and ball_club take_a_hop. mn center_field emir Coffey (thirty-three) seduce against Purdue centre lustrelessness hour_anglefortify (xxv) and defend carnut jonathan_edwards (xi) during the sec one-half of associate_in_nursing NCAA college hoops spunky atomic_number_49 Minneapolis, tues, butt_on xvi, 2021. (AP pic/robert_i Kluckenthalpyohn)\n",
      "inchjury stay_on to harry the Boforty-ninemanufacturer. And today, the boastful decade rubric be atomic_number_49 peril.\n",
      "\"We oasis't equal able-bodied to romp with the squad we accept,\" channelize bus lustrelessness mountain_lion aforementioned. \"We've live truly free-enterprise. We've make_up truly grueling-faradought. We've represent very conclusion. just we harbor't comprise capable to swordplay in_concert. Until we induce that, atomic_number_53 get_into't make_love where we're astatine. We're become to dungeon exploit.\"\n",
      "scorn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   1%|▏         | 4/300 [1:04:41<81:29:42, 991.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"The Boilermakers' 73-69 loss means they are now tied for first place with Michigan State and Michigan. Purdue trailed by 12 late in the first half and came back to tie it with under nine minutes to play. The Boilermakers are tied with Michigan and Michigan State for first place in the Big Ten. Amir Coffey collected 32 points, eight rebounds and three assists for Minnesota. MINNEAPOLIS – Purdue basketball's coronation as Big Ten champion hit a snag in Minnesota on Tuesday night. After winning ugly so many times down the stretch, the No. 11 Boilermakers could not overcome their shortcomings in a 73-69 loss at Williams Arena. Purdue missed a chance to clinch at least a share of its second Big Ten championship in three years. It must now win Saturday at Northwestern to assure a piece of the crown. Carsen Edwards' 7-of-31 shooting yielded 22 points. Ryan Cline\", 'default': {'decoded_wm': ' append fourteen point and septet backlash. The Boildivine cost without their take scorekeeper, railroad_carnut edward_iii (good genu), for the tierce heterosexual_person spunky. atomic_number_2 principal Purdue (xxi-terzetto, fourteen-triad) hoosier_state item (xxi.iii), rally (octet.zero) and wait_on (quint.tercet).“We’ve obtain to encounter some_other, unlike affair to ut with our jest_at,” Purdue autobus flatness catamount say reporter. “We’ve bewilder to get agency to playing_period ampere team_up that’sec drop_dead to ut something. And we didn’thymine this_night. We didn’tetraiodothyronine go information_technology. We didn’thyroxine sire information_technology inwards whatsoever early spirited.” gopher_state (dozen-xiii, half-dozen-dozen) utilize amp xii-zero spurt hoosier_state the center of the one-third canton and keep Purdue to eleven gunpoint all_over the death phoebe ane/deuce min. The prosperous thouophers injection fifty-two.fivesome% from the sphere and hold_up eleven-for-', 'decoded_nw': ' bring nineteen full_stop along vii-of-dozen scud. just, atomic_number_2 produce only terzetto-of-thirteen jibe from ternion-show mountain_chain. The Boilalmighty make_up pass aside hold emir Coffey with thirty-two compass_point and ball_club take_a_hop. mn center_field emir Coffey (thirty-three) seduce against Purdue centre lustrelessness hour_anglefortify (xxv) and defend carnut jonathan_edwards (xi) during the sec one-half of associate_in_nursing NCAA college hoops spunky atomic_number_49 Minneapolis, tues, butt_on xvi, 2021. (AP pic/robert_i Kluckenthalpyohn)\\ninchjury stay_on to harry the Boforty-ninemanufacturer. And today, the boastful decade rubric be atomic_number_49 peril.\\n\"We oasis\\'t equal able-bodied to romp with the squad we accept,\" channelize bus lustrelessness mountain_lion aforementioned. \"We\\'ve live truly free-enterprise. We\\'ve make_up truly grueling-faradought. We\\'ve represent very conclusion. just we harbor\\'t comprise capable to swordplay in_concert. Until we induce that, atomic_number_53 get_into\\'t make_love where we\\'re astatine. We\\'re become to dungeon exploit.\"\\nscorn', 'green_count_w': 79, 'green_count_nw': 74, 'tokens_scored_w': 138, 'tokens_scored_nw': 148, 'prop_w': 0.572463768115942, 'z_w': 1.7025130615174973, 'z_nw': 0.0, 'p_w': np.float64(0.044329616041373636), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 256.5453796386719, 'ppl_nw': 255.3892822265625, 'tokens_generated': 298}, 'kgw': {'decoded_wm': \" impart nineteen signal and vii slip for mn. The profits pass_on the kophers their tenatomic_number_90 profits against Purdue inwards thirteen mollify, go_out to when Lou top_executive and Dennis john_smith represent indium college. The Purdue exit sink the constant_of_gravitationophers ternion daub to xx-septet inward the veritable flavor. The acquire keep mn rate atomic_number_102. xi. The Purdue red drop_down Purdue to xv-baker's_dozen. The pull_ahead hold_back north_star_state rank no_more. xi. The unsuccessful_person pretermit ampere spotlight and could tied glucinium supplante eastern_samoa angstrom_unit nobelium. eleven if great_lakes_state and newmarket submit remnant upward bond surgery to_a_lower_place them with game inch pass inward sat’second back against ia and astatine buckeye_state express. The nonstarter unload adenine place and could eve cost supplante group_a ampere nobelium. eleven if mi and lake_michigan united_states_department_of_state destruction upwards wed operating_theater downstairs them with back indiana handwriting inward sat’sulfur game against ioway and astatine ohio_river country. The unsuccessful_person flatten angstrom place and could even_out make_up supplante a type_a atomic_number_102. eleven if lake_michigan and wolverine_state say ending upwards tie operating_theater downstairs them with bet_on\", 'green_count_w': 109, 'tokens_scored_w': 351, 'prop_w': 0.31054131054131057, 'z_w': 2.619417593285804, 'p_w': np.float64(0.004404002540017894), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 227.72369384765625, 'ppl_nw': 5.370729446411133, 'tokens_generated': 353}, 'cos': {'decoded_wm': ' tally nineteen tip and viii resile. The Boilalmighty make_up without their go scorekeeper, gondolanut edward_viii (proper genu), for the 6th true bet_on. atomic_number_2 star Purdue (xiv-x, octet-sestet) with nineteen.vii manoeuvre per halting. The Bo49almighty likewise exist without revolve_about chock_up arnold_palmer (decently knee_joint) and safety_device flat hour_anglemunition (right_wing mortise_joint) for the low clock_time this yr. gopher_state (nineteen-quintet, decade-deuce) embody without information_technology upside ii instrumentalist. fourth-year forrad Omar Payne leave_alone the team_up for \"personal cause,\" lead private_instructor Richard hellino allege. sophomoreomore freemason Saunders (veracious human_knee) overleap his xiithursday straightforward gamey referable to angstrom_unit sprained genu. \"We make_up the number_1 team_up to pulse them, and we make_up the start squad to bewilder them without autonut,\" phosphorusaternobelium read. \"The squad personify answer moderately intimately without him. We\\'ve puzzle to hold_back victorious.\" \"We didn\\'t turn identical sickly,\" phosphorusateratomic_number_102 say. \"We\\'re non rattling', 'green_count_w': 72, 'tokens_scored_w': 140, 'prop_w': 0.5142857142857142, 'z_w': 0.3380617018914066, 'p_w': np.float64(0.36765834531867025), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 338.8492736816406, 'ppl_nw': 5.370729446411133, 'tokens_generated': 313}}\n",
      "\n",
      "After 1 valid prompts:\n",
      " Default Method:    avg ppl (wm) = 256.55, avg ppl (nw) = 255.39, AUC = 1.000, avg z (wm) = 1.70, avg z (nw) = 0.00\n",
      " KGW Method:        avg ppl (wm) = 227.72, avg ppl (nw) = 5.37, AUC = 1.000, avg z (wm) = 2.62, avg z (nw) = 1.01\n",
      " Cosine-based Method: avg ppl (wm) = 338.85, avg ppl (nw) = 5.37, AUC = 1.000, avg z (wm) = 0.34, avg z (nw) = -0.49\n",
      "\n",
      " For thomas_more info along how to donate operating_room how to attend_to the march, electronic_mail stdabrickequalityfruit_drink@gmail.com.\n",
      "\n",
      "vii:xxx atomic_number_15.mb.\n",
      "\n",
      "The magnanimous emerald_isle detail embody backwards hoosier_state ithiel_town for the irish_people fete astatine the bblemish Lakeface. For sir_thomas_more info, confab world_wide_web.bblemishlakecomatomic_number_8.org.\n",
      "\n",
      "eighter atomic_number_15.thou.\n",
      "\n",
      "The bspoil Lake Comatomic_number_8 St. saint_patrick's clarence_shepard_day_jr. troop live come upward.\n",
      "\n",
      "eighter_from_decatur phosphorus.mebibyte.\n",
      "\n",
      "The bdefect Lake Comoxygen St. saint_patrick's twenty-four_hours march represent total upwards!\n",
      "\n",
      "eighter_from_decatur atomic_number_15.yard.\n",
      "\n",
      "Celebrate the 24-hour_interval with the bblemish Lake Comoxygen St. saint_patrick's daytime troop, let_in angstrom promenade path of bimpair and Lake Comatomic_number_8 community_of_interests, irish_gaelic euphony, and angstrom_unit terpsichore for everyone.\n",
      "\n",
      "The Lake Comtype_o country sleeping_accommodation of department_of_commerce induce unionised associate_in_nursing irish_whisky fete atomic_number_85 the bmutilate Lakefigurehead. The fete feature_of_speech dwell euphony, irish_whisky ill-treat dance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   2%|▏         | 5/300 [1:28:24<93:59:41, 1147.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"The region's largest St. Patrick's Day Parade didn't let the forecast deter the celebration. BELMAR — The region's largest St. Patrick's Day Parade continued an annual tradition that stretches back more than 45 years, despite the threat of an impending snowstorm. Each year, the Belmar Lake Como St. Patrick's Day Parade draws thousands of marchers, dozens of pipe and drum bands, and crowds of spectators from across New Jersey. The parade has become of the best attended in New Jersey. The event also raises money — two scholarships each worth $500 — for two students in a school or program that teaches an Irish activity, such as Irish step dancing or language.\", 'default': {'decoded_wm': ' For the 3rd straight St. st._patrick\\'s promenade, the interior oldtimer Council patronise the \"irish_gaelic laurels ward\", which honour those world_health_organization function and buy_the_farm indiana the uracil.south. u._s._army, vitamin_a wellspring a those world_health_organization oppose hoosier_state the brits, french_people, and Italian regular_army. The march feature poof Liz, the about holocene world-beater to get_into the consequence with the St. st._patrick\\'s troop. Liz wa top antiophthalmic_factor the fairy of the St. saint_patrick\\'s promenade atomic_number_49 1993. She washington put_up inch Ulster, unexampled nj, and elevate atomic_number_49 Bingham_it_upnet_ton, newfangled house_of_york. Liz constitute the young of quaternary small_fry, and later_on calibrate from Bingjambonlong_ton high_school train indium 1972, she make_a_motion to city_of_brotherly_love where she analyse give_suck. Liz follow the immature phallus of the american_language horde Auxiliary postal_service atomic_number_102. sevenerseventy-three and function group_a the position\\'s Auxiliary schoolmistress and commanding_officer from 1974 to 1983. From 1984 to 1987, she process atomic_number_33 the Auxiliary\\'s Ambulatory avail Coordinator and do type_a the berth\\'s Auxiliary chair from 1987 to 1989.', 'decoded_nw': \" For thomas_more info along how to donate operating_room how to attend_to the march, electronic_mail stdabrickequalityfruit_drink@gmail.com.\\n\\nvii:xxx atomic_number_15.mb.\\n\\nThe magnanimous emerald_isle detail embody backwards hoosier_state ithiel_town for the irish_people fete astatine the bblemish Lakeface. For sir_thomas_more info, confab world_wide_web.bblemishlakecomatomic_number_8.org.\\n\\neighter atomic_number_15.thou.\\n\\nThe bspoil Lake Comatomic_number_8 St. saint_patrick's clarence_shepard_day_jr. troop live come upward.\\n\\neighter_from_decatur phosphorus.mebibyte.\\n\\nThe bdefect Lake Comoxygen St. saint_patrick's twenty-four_hours march represent total upwards!\\n\\neighter_from_decatur atomic_number_15.yard.\\n\\nCelebrate the 24-hour_interval with the bblemish Lake Comoxygen St. saint_patrick's daytime troop, let_in angstrom promenade path of bimpair and Lake Comatomic_number_8 community_of_interests, irish_gaelic euphony, and angstrom_unit terpsichore for everyone.\\n\\nThe Lake Comtype_o country sleeping_accommodation of department_of_commerce induce unionised associate_in_nursing irish_whisky fete atomic_number_85 the bmutilate Lakefigurehead. The fete feature_of_speech dwell euphony, irish_whisky ill-treat dance\", 'green_count_w': 67, 'green_count_nw': 98, 'tokens_scored_w': 155, 'tokens_scored_nw': 177, 'prop_w': 0.432258064516129, 'z_w': -1.6867605906952476, 'z_nw': 1.428127453205375, 'p_w': np.float64(0.9541753019455169), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 134.83609008789062, 'ppl_nw': 81.46792602539062, 'tokens_generated': 353}, 'kgw': {'decoded_wm': \" The assign learning success comprise sophomoreomore atomic_number_85 St. st._john_the_apostle pentadianmichel_ney civilise of engineering (dame_rebecca_west house_of_windsor, freshly t-shirt, jersey). With rainfall inch the calculate and the blow predict deoxyadenosine_monophosphate betimes axerophthol quartet atomic_number_15.mebibyte., troop contriver determine to scrub the result. The march citizens_committee and_then distinct to impress information_technology to mon good_afternoon when shape look friendly to take_hold the outcome. The promenade commission enounce about captain_hicks,d looker and vitamin_a thou sir_thomas_more abut done Belgsubdivisionount foursquare. The march citizens_committee as_well order troop attendant donate $triad.pentad trillion, which admit thomas_more money self-contained during the weekend than whatsoever retiring promenade, personal_organiser of the St. glibkink' march foretell yesterday astatine associate_in_nursing good_afternoon tidings league. The march's pda embody forthwith coordinate for information_technology succeeding variant marching_music nina_from_carolina indiana rebecca_westtheatre_of_operations, novel island_of_jersey, labor_organizer of the St. raphaystack' march foretell yesterday astatine associate_in_nursing good_afternoon news_program league. The exhibit's pda exist immediately organize for information_technology following variant adjoin nina_from_carolina indium dame_rebecca_westsubject_area, fresh new_jersey, personal_organiser of the St. chuckrick\", 'green_count_w': 107, 'tokens_scored_w': 384, 'prop_w': 0.2786458333333333, 'z_w': 1.2963624321753373, 'p_w': np.float64(0.09742532430443474), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 168.90773010253906, 'ppl_nw': 5.523035049438477, 'tokens_generated': 386}, 'cos': {'decoded_wm': \" For the 7th yr, the troop constitute schedule to commence atomic_number_85 x:fifteen ampere.one_thousand. astatine the bmarch Municipal commons. simply ampere rain_downrage set_about to break and turn axerophthol double-uintry blz. The troop follow scratch. The troop exist peerless of the high_spot of the county's St. saint_patrick's twenty-four_hour_period festivity and drag m of deflowerchers, gobs of pipework and thrum lot, and crew of spectator_pump from crosswise the part. The outcome throw suit of the sound advert, and climb the well-nigh money, inwards the department_of_state. This comprise the exhibit's 7th class. The issue, which follow carry for the foremost clock this twelvemonth, constitute unitary of the highlight of the county's St. st._patrick's daytime festivity. For the 7th yr, the exhibit make_up schedule to start astatine decade:xv angstrom.mb. atomic_number_85 the bdeflower Municipal parking_lot. just adenine peltingviolent_storm commence to prepare and get amp wolframintry blzee. The promenade follow invalidate. The troop be ace of the foreground of the county\", 'green_count_w': 60, 'tokens_scored_w': 146, 'prop_w': 0.410958904109589, 'z_w': -2.1517753103661565, 'p_w': np.float64(0.9842924723415246), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 178.40618896484375, 'ppl_nw': 5.523035049438477, 'tokens_generated': 283}}\n",
      "\n",
      "After 2 valid prompts:\n",
      " Default Method:    avg ppl (wm) = 195.69, avg ppl (nw) = 168.43, AUC = 0.500, avg z (wm) = 0.01, avg z (nw) = 0.71\n",
      " KGW Method:        avg ppl (wm) = 198.32, avg ppl (nw) = 5.45, AUC = 1.000, avg z (wm) = 1.96, avg z (nw) = -0.62\n",
      " Cosine-based Method: avg ppl (wm) = 258.63, avg ppl (nw) = 5.45, AUC = 0.250, avg z (wm) = -0.91, avg z (nw) = 0.77\n",
      "\n",
      "\n",
      "flummoxTY watch_crystal castle hirer Ian hollow_outay make_up angstrom_unit utmost-calciferolrub endeavour to subscribe Wildeep-fried izzardaha from Manchester joined\n",
      "“unity possess involve most angstrom lend for Wilfarad simply jacques_louis_david bequeath take him atomic_number_85 the the_right_way clip Ian holleray along Wildeep-fried izzardaha concentrate\n",
      "“We personify rattling close-fitting to contract him when atomic_number_2 lead connect, thus information_technology’siemens adenine lilliputian minute baffle. “unity give postulate virtually deoxyadenosine_monophosphate loanword for Wilatomic_number_9 merely jacques_louis_david bequeath choice him astatine the ripe clock. “on_that_point’sulfur equal ampere batch of meditation nearly information_technology simply helium’randomness felicitous astatine join and atomic_number_2 volition act type_a liberal parting for them this harden.” zeeaha make exist tie with angstrom proceed backward to castle, on with fixed_storageelevated_railwayuracil Lukyoruba and henry_m._robert Lewandowski, only holeay take_a_firm_stand atomic_number_2 would but take_a_crap amp act for zedaha if atomic_number_2 could bring deoxyadenosine_monophosphate business_deal practise for his possess player. “Wildegree_fahrenheit possess exist astatine combine for triad old_age and this personify the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   2%|▏         | 6/300 [1:50:53<99:16:24, 1215.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Crystal Palace boss Ian Holloway has revealed he made a deadline day approach to sign Wilfried Zaha from Manchester United ‘just in case’ there was a chance of a loan deal. Zaha signed for United from the Eagles last January, but was loaned back to Palace for the remainder of the season and played a crucial part in Holloway’s men gaining promotion back to the Premier League. But Holloway, who signed no fewer than 15 players during the summer transfer window, admits he made a last-ditch attempt to seal another loan deal for the highly-rated 20-year-old. ‘I have asked about a loan for Wilf but David [Moyes] will select him at the right time,’ said the Palace boss. Zaha has made just one appearance for David Moyes’ side so far this season, playing the first 61 minutes of their 2-0 victory over Wigan in the Community Shield in August.', 'default': {'decoded_wm': '\\ncontractTY Wilfry omegaaha experience cost tumble for associate_in_nursing die from isle_of_man link this calendar_month\\n‘We throw angstrom few discussion and iodin rung to saint_david nigh that and helium exist truly lament. ‘Wilfluorine hasn’metric_ton establish the right_hand decisiveness for the decently intellect. helium’ll give_birth to seminal_fluid backward and constitute with the squad, which helium’ll lack to ut, and atomic_number_2’ll sustain to make_out back_up and comprise antiophthalmic_factor portion of the squad. ‘only we totally crap mistake, everybody relieve_oneself misunderstanding. cypher pop_off tabu along lend for the properly understanding.’ zedaha embody along lend atomic_number_85 the double_birdie from watch_glass castle for the 2010-2011 and 2011-2012 mollify. merge honcho Moyeast, speechmaking in_the_beginning nowadays, sound_out atomic_number_2 wa silence hop to contract the bone sea-coast outside.\\nperplexTY Wildeep-fried ezedaha equal along lend astatine the bird_of_jove for the 2010-2011 and 2011-2012 season\\n‘We livelihood work_out along information_technology [the loanword] because we', 'decoded_nw': '\\nflummoxTY watch_crystal castle hirer Ian hollow_outay make_up angstrom_unit utmost-calciferolrub endeavour to subscribe Wildeep-fried izzardaha from Manchester joined\\n“unity possess involve most angstrom lend for Wilfarad simply jacques_louis_david bequeath take him atomic_number_85 the the_right_way clip Ian holleray along Wildeep-fried izzardaha concentrate\\n“We personify rattling close-fitting to contract him when atomic_number_2 lead connect, thus information_technology’siemens adenine lilliputian minute baffle. “unity give postulate virtually deoxyadenosine_monophosphate loanword for Wilatomic_number_9 merely jacques_louis_david bequeath choice him astatine the ripe clock. “on_that_point’sulfur equal ampere batch of meditation nearly information_technology simply helium’randomness felicitous astatine join and atomic_number_2 volition act type_a liberal parting for them this harden.” zeeaha make exist tie with angstrom proceed backward to castle, on with fixed_storageelevated_railwayuracil Lukyoruba and henry_m._robert Lewandowski, only holeay take_a_firm_stand atomic_number_2 would but take_a_crap amp act for zedaha if atomic_number_2 could bring deoxyadenosine_monophosphate business_deal practise for his possess player. “Wildegree_fahrenheit possess exist astatine combine for triad old_age and this personify the', 'green_count_w': 71, 'green_count_nw': 93, 'tokens_scored_w': 131, 'tokens_scored_nw': 191, 'prop_w': 0.5419847328244275, 'z_w': 0.9610744623271417, 'z_nw': -0.36178730264621084, 'p_w': np.float64(0.16825736455405932), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 265.5908203125, 'ppl_nw': 214.91786193847656, 'tokens_generated': 301}, 'kgw': {'decoded_wm': ' castle chairwoman Steve Parish washington beguiled for his musician every_bit atomic_number_2 north_korean_won promotional_material atomic_number_85 his third_base try and assert helium deserve the congratulations. speechmaking afterward, Parish hail the identify instrumentalist that micturate upwards his position during the crusade. atomic_number_2 exist single actor 1 would non read be ane of the prominent instrumentalist that make_up non with pine_tree_state. single would non pronounce information_technology consume lay_down pine_tree_state predetermine – information_technology birth hold pine_tree_state reliable group_a angstrom_unit castle chair. helium equal single instrumentalist atomic_number_53 would non order embody unmatchable of the great instrumentalist that washington non with maine. atomic_number_53 would non enounce information_technology ha throw pine_tree_state one-sided – information_technology give create pine_tree_state dependable arsenic adenine castle chair. interim the double_birdie stamp likewise take that the add-on of the £thirtyjillion onetime Manchester link_up virtuoso evergreen_state ‘antiophthalmic_factor giving putsch’ and angstrom_unit estimable relocation for his side_of_meat inwards what equal control ampere amp yobbo prime conference. castle grimace Sunderland inwards tues’siemens low_gear premiere conference face-off astatine St saint_andrew_the_apostle’sec bowl – their flavour bitch polish_off. castle confront Sunderland inwards tues’reciprocal_ohm initiative premiere conference', 'green_count_w': 114, 'tokens_scored_w': 353, 'prop_w': 0.32294617563739375, 'z_w': 3.1651131818944607, 'p_w': np.float64(0.0007751131798114199), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 205.75035095214844, 'ppl_nw': 5.3759307861328125, 'tokens_generated': 355}, 'cos': {'decoded_wm': '\\n\\npal_uptops follow essay strengthener this calendar_month\\n\\n‘1 bring_in ampere loanword act for him (later the vitamin_eflorida prize) to collapse him around biz fourth_dimension and, of form, information_technology turn. atomic_number_2 cost inwards type_a dandy physical_body. simply the flavour follow pass to get slurred and libertine and we be calculate to bugger_off around role_player atomic_number_49. ‘We deliver obtain to nonplus musician inward adenine shortly eastern_samoa potential.’ castle’second trouble atomic_number_85 the backrest base they rich_person profess sir_thomas_more than they hold tally inward the chancellor conference and they cause neglect knocked_out along the cover-one-half of the conference for the yesteryear ii flavour. They constitute the death nightclub to ratify for the title and they constitute right_away make to effort and check undirected inwards the upside-one-half of the prime_minister conference. ‘We constitute bet to add instrumentalist indium and we bequeath insure where we belong from at_that_place,’ core_outay bring.\\n\\nRoy Hodgword hold constitute inward commove of castle for to_a_lesser_extent than dozen calendar_month', 'green_count_w': 71, 'tokens_scored_w': 141, 'prop_w': 0.5035460992907801, 'z_w': 0.0842151921066519, 'p_w': np.float64(0.46644266977143223), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 335.5089111328125, 'ppl_nw': 5.3759307861328125, 'tokens_generated': 292}}\n",
      "\n",
      "After 3 valid prompts:\n",
      " Default Method:    avg ppl (wm) = 218.99, avg ppl (nw) = 183.93, AUC = 0.556, avg z (wm) = 0.33, avg z (nw) = 0.36\n",
      " KGW Method:        avg ppl (wm) = 200.79, avg ppl (nw) = 5.42, AUC = 1.000, avg z (wm) = 2.36, avg z (nw) = -0.40\n",
      " Cosine-based Method: avg ppl (wm) = 284.25, avg ppl (nw) = 5.42, AUC = 0.444, avg z (wm) = -0.58, avg z (nw) = 0.05\n",
      "\n",
      " comprise similar, 'Yes, information_technology exist.' \" esstedegree_fahrenheit state helium require to pretend certainly the house personify doctor ahead mass bulge ram onto iodin-880. \"ane recount them, 'exactly campaign tardily.' \"\n",
      "\n",
      "Caltrans be stress to score the raw 1-880 unionlimit state_highway sign_up indiana milehella net. information_technology's regretful, soh information_technology's cost gear_up.\n",
      "\n",
      "NBC true_laurel country's Michelle rube_goldberg describe\n",
      "\n",
      "father axerophthol day-by-day summation of the meridian account along NBC bay_laurel surface_area and beyond with our morning_time newssheet. house astir hera.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   2%|▏         | 7/300 [2:07:53<93:44:28, 1151.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'There are typos. And then there are typos. Caltrans could have sure used spellcheck when putting up a new green highway sign Wednesday night on the northbound side of Interstate Highway 880. Instead of directing people to Milpitas off state route 237, the sign directs them to \"Milipitas,\" with an extra \"i.\" An alert motorist with excellent spelling skills notified NBC Bay Area about the mistake on Thursday. NBC Bay Area then alerted Caltrans, who said the news agency was the first to report it, according to spokesman Bernard Walik. Before noon, the misspelling was covered in white plastic. The sign should be fixed in a month. Walik said there would be no cost to taxpayers, because the contractor who made the mistake will have to remedy it on his own. \"People were calling me and saying, \\'Is it really Mili-pitas?\\' \" Mayor Jose Esteves said, emphasizing the mispronounciation. \"I', 'default': {'decoded_wm': ' be the_likes_of, \\'Yes, information_technology make_up.\\' \"\\n\\natomic_number_85 to_the_lowest_degree ane Caltrans employee, associate_in_nursing technologist, neglect to right enounce the metropolis, the Los Angeles meter account.\\n\\nThe really literal constitute this:\\n\\nace of ii polarity the declarer multicolour along the motorway report \"swedish_mile-pitfalla,\" with associate_in_nursing supererogatory \"iodin,\" along the thruway due_north of interstate_highway main_road octonaryeighty inward milecollieryadenine.', 'decoded_nw': ' comprise similar, \\'Yes, information_technology exist.\\' \" esstedegree_fahrenheit state helium require to pretend certainly the house personify doctor ahead mass bulge ram onto iodin-880. \"ane recount them, \\'exactly campaign tardily.\\' \"\\n\\nCaltrans be stress to score the raw 1-880 unionlimit state_highway sign_up indiana milehella net. information_technology\\'s regretful, soh information_technology\\'s cost gear_up.\\n\\nNBC true_laurel country\\'s Michelle rube_goldberg describe\\n\\nfather axerophthol day-by-day summation of the meridian account along NBC bay_laurel surface_area and beyond with our morning_time newssheet. house astir hera.', 'green_count_w': 29, 'green_count_nw': 47, 'tokens_scored_w': 61, 'tokens_scored_nw': 82, 'prop_w': 0.47540983606557374, 'z_w': -0.3841106397986879, 'z_nw': 1.3251783128981585, 'p_w': np.float64(0.6495517770717674), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 240.6680145263672, 'ppl_nw': 546.8843383789062, 'tokens_generated': 147}, 'kgw': {'decoded_wm': ' personify middling concern when 1 conk to labor away and learn type_a \\'cubic_decimeter.\\' only when single ascertain come_out_of_the_closet Google function my gondola constitute dead finely and on_that_point equal non associate_in_nursing mistake anyplace. unity\\'m gladiola that soul refer information_technology and information_technology trip_up the_great_unwashed past storm.\" The signed leave delay piece building along itinerary 237 carry_on. deoxyadenosine_monophosphate calendar_week’southward placard crataegus_laevigata non cost adequate for Caltrans to pay_back some_other rubyunity building sign-language on swedish_milefossaa force approach the tack with ane-680. mental_synthesis have of_late set_about in_that_location likewise, on with modern lane and newly storm from the 680 loss. The star_sign bunch deepen, which sustain be put_in during the older organisation of the Caltrans united_states_department_of_state accountant’reciprocal_ohm executive_director billet, represent directly live hit to take_a_shit board for to_a_greater_extent blood-redoing on the 680 go_out. Walik sound_out if expression along path 237 persist_in thomas_more bolshevikmatchless mental_synthesis ratify and contract with fresh art leave make_it inward deuce to_a_greater_extent week from the Caltrans vender they’ray sign_up to. The declarer volition slay those and exchange them with young', 'green_count_w': 99, 'tokens_scored_w': 359, 'prop_w': 0.2757660167130919, 'z_w': 1.1274411253716097, 'p_w': np.float64(0.1297780097524615), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 301.1987609863281, 'ppl_nw': 12.712804794311523, 'tokens_generated': 361}, 'cos': {'decoded_wm': ' exist jolly scandalise.\" eastwardstef aver the escapepelling represent, helium articulate, \"scarcely associate_in_nursing stroke.\" \"We\\'re entirely man and we totally great_lakes_statepronoz. thing,\" atomic_number_2 pronounce.\\n\\narsenic of this sunrise, the fillepelling personify comprise embrace astir with white-hot pliant.\\n\\nThe pike mansion take_in live type_a scudguide for number_one_wood since the starting_time of the yr. along mon, Caltrans and cubic_centimetrecollieryangstrom_unit official declare the start of respective design route externalize along the union slope of the thruway this yr, include associate_in_nursing melioration to the confederate_states_of_americaborder 680 dealings visible_light. And gang constitute presently put_back the thruway storm from the southwardbandaged 680 to 580. Caltrans enunciate the plan melioration cost signify to supporter driver and pedestrian inwards milliliternether_regionarsenic, include the increase of prosaic and bike lane and wide-eyed pavement. \"We deficiency to promote multitude to practice the safe alternative and the secure road,\" Walik secernate NBC quest field. \"We deprivation to promote the_great_unwashed to expend the safe alternative and the good path.\" Caltrans', 'green_count_w': 66, 'tokens_scored_w': 132, 'prop_w': 0.5, 'z_w': 0.0, 'p_w': np.float64(0.5), 'judgement': 'Human-generated (non-watermarked)', 'ppl_wm': 323.95709228515625, 'ppl_nw': 12.712804794311523, 'tokens_generated': 296}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   2%|▏         | 7/300 [2:23:52<100:22:04, 1233.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m results, aggregated = \u001B[43mevaluate_watermarking\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtruncated_texts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs_cos\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Print per-prompt results for the first 5 prompts.\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m results[:\u001B[32m5\u001B[39m]:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[26]\u001B[39m\u001B[32m, line 882\u001B[39m, in \u001B[36mevaluate_watermarking\u001B[39m\u001B[34m(truncated_texts, model, tokenizer, args, args_cos)\u001B[39m\n\u001B[32m    879\u001B[39m tokens_scored_w_kgw_local = detect_result_w_kgw[\u001B[33m\"\u001B[39m\u001B[33mnum_tokens_scored\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    880\u001B[39m prop_w_kgw = detect_result_w_kgw[\u001B[33m\"\u001B[39m\u001B[33mgreen_fraction\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m882\u001B[39m detect_result_nw_kgw = \u001B[43mdetect_kgw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoded_nw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m1\u001B[39m]\n\u001B[32m    883\u001B[39m \u001B[38;5;28mprint\u001B[39m(decoded_nw)\n\u001B[32m    884\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(decoded_nw) < \u001B[32m195\u001B[39m:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 325\u001B[39m, in \u001B[36mdetect_kgw\u001B[39m\u001B[34m(input_text, args, device, tokenizer)\u001B[39m\n\u001B[32m    315\u001B[39m watermark_detector = WatermarkDetector_kgw(vocab=\u001B[38;5;28mlist\u001B[39m(tokenizer.get_vocab().values()),\n\u001B[32m    316\u001B[39m                                        gamma=args.gamma,\n\u001B[32m    317\u001B[39m                                        seeding_scheme=args.seeding_scheme,\n\u001B[32m   (...)\u001B[39m\u001B[32m    322\u001B[39m                                        ignore_repeated_bigrams=args.ignore_repeated_bigrams,\n\u001B[32m    323\u001B[39m                                        select_green_tokens=args.select_green_tokens)\n\u001B[32m    324\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(input_text) - \u001B[32m1\u001B[39m > watermark_detector.min_prefix_len:\n\u001B[32m--> \u001B[39m\u001B[32m325\u001B[39m     score_dict = \u001B[43mwatermark_detector\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdetect\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    326\u001B[39m     \u001B[38;5;66;03m# output = str_format_scores(score_dict, watermark_detector.z_threshold)\u001B[39;00m\n\u001B[32m    327\u001B[39m     output = list_format_scores(score_dict, watermark_detector.z_threshold)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 258\u001B[39m, in \u001B[36mWatermarkDetector_kgw.detect\u001B[39m\u001B[34m(self, text, tokenized_text, return_prediction, return_scores, z_threshold, **kwargs)\u001B[39m\n\u001B[32m    256\u001B[39m         tokenized_text = tokenized_text[\u001B[32m1\u001B[39m:]\n\u001B[32m    257\u001B[39m output_dict = {}\n\u001B[32m--> \u001B[39m\u001B[32m258\u001B[39m score_dict = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_score_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_scores:\n\u001B[32m    260\u001B[39m     output_dict.update(score_dict)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 217\u001B[39m, in \u001B[36mWatermarkDetector_kgw._score_sequence\u001B[39m\u001B[34m(self, input_ids, return_num_tokens_scored, return_num_green_tokens, return_green_fraction, return_green_token_mask, return_z_score, return_p_value)\u001B[39m\n\u001B[32m    214\u001B[39m             green_token_mask.append(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    215\u001B[39m             \u001B[38;5;66;03m#print(True)\u001B[39;00m\n\u001B[32m    216\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m             green_token_mask.append(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    218\u001B[39m             \u001B[38;5;66;03m#print(False)\u001B[39;00m\n\u001B[32m    219\u001B[39m score_dict = {}\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Rephrase attack**",
   "id": "5e17abd65263719e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "670b6f6c7738da51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6f82edda2a0094a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "de16d19c47fc0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "57fcb1038881ff04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a9b6828703be8809"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9ff8e352e26b9a88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:42:36.439581Z",
     "start_time": "2025-03-23T20:42:36.401020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n",
    "# available at https://arxiv.org/abs/2301.10226\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSeq2SeqLM,\n",
    "                          AutoModelForCausalLM,\n",
    "                          LogitsProcessorList)\n",
    "\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"Util function for user friendly boolean flag args\"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Command line argument specification\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"A minimum working example of applying the watermark to any LLM that supports the huggingface 🤗 `generate` API\")\n",
    "\n",
    "    #parser.add_argument(\n",
    "    #    \"--run_gradio\",\n",
    "    #    type=str2bool,\n",
    "    #    default=False,\n",
    "    #    help=\"Whether to launch as a gradio demo. Set to False if not installed and want to just run the stdout version.\",\n",
    "    #)\n",
    "    parser.add_argument(\n",
    "        \"--demo_public\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to expose the gradio demo to the internet.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        default=\"facebook-opt1.3b\",#\"gpt2-medium\",#\n",
    "        help=\"Main model, path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prompt_max_length\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Truncation length for prompt, overrides model config's max length field.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_new_tokens\",\n",
    "        type=int,\n",
    "        default=200,\n",
    "        help=\"Maximmum number of new tokens to generate.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generation_seed\",\n",
    "        type=int,\n",
    "        default=123,\n",
    "        help=\"Seed for setting the torch global rng prior to generation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_sampling\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to generate using multinomial sampling.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sampling_temp\",\n",
    "        type=float,\n",
    "        default=0.7,\n",
    "        help=\"Sampling temperature to use when generating using multinomial sampling.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_beams\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of beams to use for beam search. 1 is normal greedy decoding\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_gpu\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to run inference and watermark hashing/seeding/permutation on gpu.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seeding_scheme\",\n",
    "        type=str,\n",
    "        default=\"simple_1\",\n",
    "        help=\"Seeding scheme to use to generate the greenlists at each generation and verification step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gamma\",\n",
    "        type=float,\n",
    "        default=0.25,\n",
    "        help=\"The fraction of the vocabulary to partition into the greenlist at each generation and verification step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--delta\",\n",
    "        type=float,\n",
    "        default=2.0,\n",
    "        help=\"The amount/bias to add to each of the greenlist token logits before each token sampling step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--normalizers\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Single or comma separated list of the preprocessors/normalizer names to use when performing watermark detection.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ignore_repeated_bigrams\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to use the detection method that only counts each unqiue bigram once as either a green or red hit.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--detection_z_threshold\",\n",
    "        type=float,\n",
    "        default=4.0,\n",
    "        help=\"The test statistic threshold for the detection hypothesis test.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--select_green_tokens\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"How to treat the permuation when selecting the greenlist tokens at each step. Legacy is (False) to pick the complement/reds first.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip_model_load\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Skip the model loading to debug the interface.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed_separately\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to call the torch seed function before both the unwatermarked and watermarked generate calls.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_fp16\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to run model in float16 precsion.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_model(args):\n",
    "    \"\"\"Load and return the model and tokenizer\"\"\"\n",
    "\n",
    "    args.is_seq2seq_model = any([(model_type in args.model_name_or_path) for model_type in [\"t5\", \"T0\"]])\n",
    "    args.is_decoder_only_model = any(\n",
    "        [(model_type in args.model_name_or_path) for model_type in [\"gpt\", \"opt\", \"bloom\"]])\n",
    "    if args.is_seq2seq_model:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n",
    "    elif args.is_decoder_only_model:\n",
    "        if args.load_fp16:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, torch_dtype=torch.float16,\n",
    "                                                         device_map='auto')\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {args.model_name_or_path}\")\n",
    "\n",
    "    if args.use_gpu:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if args.load_fp16:\n",
    "            pass\n",
    "        else:\n",
    "            model = model.to(device)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def generate_kgw(prompt, args, model=None, device=None, tokenizer=None):\n",
    "    \"\"\"Instatiate the WatermarkLogitsProcessor according to the watermark parameters\n",
    "       and generate watermarked text by passing it to the generate method of the model\n",
    "       as a logits processor. \"\"\"\n",
    "\n",
    "    #print(f\"Generating with {args}\")\n",
    "\n",
    "    watermark_processor = WatermarkLogitsProcessor_kgw(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                                   gamma=args.gamma,\n",
    "                                                   delta=args.delta,\n",
    "                                                   seeding_scheme=args.seeding_scheme,\n",
    "                                                   select_green_tokens=args.select_green_tokens)\n",
    "\n",
    "    gen_kwargs = dict(max_new_tokens=args.max_new_tokens)\n",
    "\n",
    "    if args.use_sampling:\n",
    "        gen_kwargs.update(dict(\n",
    "            do_sample=True,\n",
    "            top_k=0,\n",
    "            temperature=args.sampling_temp\n",
    "        ))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(\n",
    "            num_beams=args.n_beams\n",
    "        ))\n",
    "\n",
    "    generate_without_watermark = partial(\n",
    "        model.generate,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    generate_with_watermark = partial(\n",
    "        model.generate,\n",
    "        logits_processor=LogitsProcessorList([watermark_processor]),\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    if args.prompt_max_length:\n",
    "        pass\n",
    "    elif hasattr(model.config, \"max_position_embedding\"):\n",
    "        args.prompt_max_length = model.config.max_position_embeddings - args.max_new_tokens\n",
    "    else:\n",
    "        args.prompt_max_length = 2048 - args.max_new_tokens\n",
    "\n",
    "    tokd_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True, truncation=True,\n",
    "                           max_length=args.prompt_max_length).to(device)\n",
    "    truncation_warning = True if tokd_input[\"input_ids\"].shape[-1] == args.prompt_max_length else False\n",
    "    redecoded_input = tokenizer.batch_decode(tokd_input[\"input_ids\"], skip_special_tokens=True)[0]\n",
    "\n",
    "    torch.manual_seed(args.generation_seed)\n",
    "    output_without_watermark = generate_without_watermark(**tokd_input)\n",
    "\n",
    "    # optional to seed before second generation, but will not be the same again generally, unless delta==0.0, no-op watermark\n",
    "    if args.seed_separately:\n",
    "        torch.manual_seed(args.generation_seed)\n",
    "    output_with_watermark = generate_with_watermark(**tokd_input)\n",
    "\n",
    "    if args.is_decoder_only_model:\n",
    "        # need to isolate the newly generated tokens\n",
    "        output_without_watermark = output_without_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "        output_with_watermark = output_with_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    decoded_output_without_watermark = tokenizer.batch_decode(output_without_watermark, skip_special_tokens=True)[0]\n",
    "    decoded_output_with_watermark = tokenizer.batch_decode(output_with_watermark, skip_special_tokens=True)[0]\n",
    "\n",
    "    return (redecoded_input,\n",
    "            int(truncation_warning),\n",
    "            decoded_output_without_watermark,\n",
    "            decoded_output_with_watermark,\n",
    "            args)\n",
    "    # decoded_output_with_watermark)\n",
    "\n",
    "\n",
    "def format_names(s):\n",
    "    \"\"\"Format names for the gradio demo interface\"\"\"\n",
    "    s = s.replace(\"num_tokens_scored\", \"Tokens Counted (T)\")\n",
    "    s = s.replace(\"num_green_tokens\", \"# Tokens in Greenlist\")\n",
    "    s = s.replace(\"green_fraction\", \"Fraction of T in Greenlist\")\n",
    "    s = s.replace(\"z_score\", \"z-score\")\n",
    "    s = s.replace(\"p_value\", \"p value\")\n",
    "    s = s.replace(\"prediction\", \"Prediction\")\n",
    "    s = s.replace(\"confidence\", \"Confidence\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def list_format_scores(score_dict, detection_threshold):\n",
    "    \"\"\"Format the detection metrics into a gradio dataframe input format\"\"\"\n",
    "    lst_2d = []\n",
    "    # lst_2d.append([\"z-score threshold\", f\"{detection_threshold}\"])\n",
    "    for k, v in score_dict.items():\n",
    "        if k == 'green_fraction':\n",
    "            lst_2d.append([format_names(k), f\"{v:.1%}\"])\n",
    "        elif k == 'confidence':\n",
    "            lst_2d.append([format_names(k), f\"{v:.3%}\"])\n",
    "        elif isinstance(v, float):\n",
    "            lst_2d.append([format_names(k), f\"{v:.3g}\"])\n",
    "        elif isinstance(v, bool):\n",
    "            lst_2d.append([format_names(k), (\"Watermarked\" if v else \"Human/Unwatermarked\")])\n",
    "        else:\n",
    "            lst_2d.append([format_names(k), f\"{v}\"])\n",
    "    if \"confidence\" in score_dict:\n",
    "        lst_2d.insert(-2, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    else:\n",
    "        lst_2d.insert(-1, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    return lst_2d\n",
    "\n",
    "\n",
    "def detect_kgw(input_text, args, device=None, tokenizer=None):\n",
    "    \"\"\"Instantiate the WatermarkDetection object and call detect on\n",
    "        the input text returning the scores and outcome of the test\"\"\"\n",
    "    watermark_detector = WatermarkDetector_kgw(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                           gamma=args.gamma,\n",
    "                                           seeding_scheme=args.seeding_scheme,\n",
    "                                           device=device,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           z_threshold=args.detection_z_threshold,\n",
    "                                           normalizers=args.normalizers,\n",
    "                                           ignore_repeated_bigrams=args.ignore_repeated_bigrams,\n",
    "                                           select_green_tokens=args.select_green_tokens)\n",
    "    if len(input_text) - 1 > watermark_detector.min_prefix_len:\n",
    "        score_dict = watermark_detector.detect(input_text)\n",
    "        # output = str_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "        output = list_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "    else:\n",
    "        score_dict = None\n",
    "        # output = (f\"Error: string not long enough to compute watermark presence.\")\n",
    "        output = [[\"Error\", \"string too short to compute metrics\"]]\n",
    "        output += [[\"\", \"\"] for _ in range(6)]\n",
    "    return output, score_dict, args\n",
    "\n",
    "\n",
    "def run_gradio(args, model=None, device=None, tokenizer=None):\n",
    "    \"\"\"Define and launch the gradio demo interface\"\"\"\n",
    "    generate_partial = partial(generate_kgw, model=model, device=device, tokenizer=tokenizer)\n",
    "    detect_partial = partial(detect_kgw, device=device, tokenizer=tokenizer)\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        # Top section, greeting and instructions\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=9):\n",
    "                gr.Markdown(\n",
    "                    \"\"\"\n",
    "                    ## 💧 [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226) 🔍\n",
    "                    \"\"\"\n",
    "                )\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\n",
    "                    \"\"\"\n",
    "                    [![](https://badgen.net/badge/icon/GitHub?icon=github&label)](https://github.com/jwkirchenbauer/lm-watermarking)\n",
    "                    \"\"\"\n",
    "                )\n",
    "            # with gr.Column(scale=2):\n",
    "            #     pass\n",
    "            # ![visitor badge](https://visitor-badge.glitch.me/badge?page_id=tomg-group-umd_lm-watermarking) # buggy\n",
    "\n",
    "        with gr.Accordion(\"Understanding the output metrics\", open=False):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                - `z-score threshold` : The cuttoff for the hypothesis test\n",
    "                - `Tokens Counted (T)` : The number of tokens in the output that were counted by the detection algorithm.\n",
    "                    The first token is ommitted in the simple, single token seeding scheme since there is no way to generate\n",
    "                    a greenlist for it as it has no prefix token(s). Under the \"Ignore Bigram Repeats\" detection algorithm,\n",
    "                    described in the bottom panel, this can be much less than the total number of tokens generated if there is a lot of repetition.\n",
    "                - `# Tokens in Greenlist` : The number of tokens that were observed to fall in their respective greenlist\n",
    "                - `Fraction of T in Greenlist` : The `# Tokens in Greenlist` / `T`. This is expected to be approximately `gamma` for human/unwatermarked text.\n",
    "                - `z-score` : The test statistic for the detection hypothesis test. If larger than the `z-score threshold`\n",
    "                    we \"reject the null hypothesis\" that the text is human/unwatermarked, and conclude it is watermarked\n",
    "                - `p value` : The likelihood of observing the computed `z-score` under the null hypothesis. This is the likelihood of\n",
    "                    observing the `Fraction of T in Greenlist` given that the text was generated without knowledge of the watermark procedure/greenlists.\n",
    "                    If this is extremely _small_ we are confident that this many green tokens was not chosen by random chance.\n",
    "                -  `prediction` : The outcome of the hypothesis test - whether the observed `z-score` was higher than the `z-score threshold`\n",
    "                - `confidence` : If we reject the null hypothesis, and the `prediction` is \"Watermarked\", then we report 1-`p value` to represent\n",
    "                    the confidence of the detection based on the unlikeliness of this `z-score` observation.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        with gr.Accordion(\"A note on model capability\", open=True):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                This demo uses open-source language models that fit on a single GPU. These models are less powerful than proprietary commercial tools like ChatGPT, Claude, or Bard.\n",
    "\n",
    "                Importantly, we use a language model that is designed to \"complete\" your prompt, and not a model this is fine-tuned to follow instructions.\n",
    "                For best results, prompt the model with a few sentences that form the beginning of a paragraph, and then allow it to \"continue\" your paragraph.\n",
    "                Some examples include the opening paragraph of a wikipedia article, or the first few sentences of a story.\n",
    "                Longer prompts that end mid-sentence will result in more fluent generations.\n",
    "                \"\"\"\n",
    "            )\n",
    "        gr.Markdown(f\"Language model: {args.model_name_or_path} {'(float16 mode)' if args.load_fp16 else ''}\")\n",
    "\n",
    "        # Construct state for parameters, define updates and toggles\n",
    "        default_prompt = args.__dict__.pop(\"default_prompt\")\n",
    "        session_args = gr.State(value=args)\n",
    "\n",
    "        with gr.Tab(\"Generate and Detect\"):\n",
    "\n",
    "            with gr.Row():\n",
    "                prompt = gr.Textbox(label=f\"Prompt\", interactive=True, lines=10, max_lines=10, value=default_prompt)\n",
    "            with gr.Row():\n",
    "                generate_btn = gr.Button(\"Generate\")\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    output_without_watermark = gr.Textbox(label=\"Output Without Watermark\", interactive=False, lines=14,\n",
    "                                                          max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # without_watermark_detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    without_watermark_detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False,\n",
    "                                                                      row_count=7, col_count=2)\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    output_with_watermark = gr.Textbox(label=\"Output With Watermark\", interactive=False, lines=14,\n",
    "                                                       max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # with_watermark_detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    with_watermark_detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False,\n",
    "                                                                   row_count=7, col_count=2)\n",
    "\n",
    "            redecoded_input = gr.Textbox(visible=False)\n",
    "            truncation_warning = gr.Number(visible=False)\n",
    "\n",
    "            def truncate_prompt(redecoded_input, truncation_warning, orig_prompt, args):\n",
    "                if truncation_warning:\n",
    "                    return redecoded_input + f\"\\n\\n[Prompt was truncated before generation due to length...]\", args\n",
    "                else:\n",
    "                    return orig_prompt, args\n",
    "\n",
    "        with gr.Tab(\"Detector Only\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    detection_input = gr.Textbox(label=\"Text to Analyze\", interactive=True, lines=14, max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False, row_count=7,\n",
    "                                                    col_count=2)\n",
    "            with gr.Row():\n",
    "                detect_btn = gr.Button(\"Detect\")\n",
    "\n",
    "        # Parameter selection group\n",
    "        with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(f\"#### Generation Parameters\")\n",
    "                    with gr.Row():\n",
    "                        decoding = gr.Radio(label=\"Decoding Method\", choices=[\"multinomial\", \"greedy\"],\n",
    "                                            value=(\"multinomial\" if args.use_sampling else \"greedy\"))\n",
    "                    with gr.Row():\n",
    "                        sampling_temp = gr.Slider(label=\"Sampling Temperature\", minimum=0.1, maximum=1.0, step=0.1,\n",
    "                                                  value=args.sampling_temp, visible=True)\n",
    "                    with gr.Row():\n",
    "                        generation_seed = gr.Number(label=\"Generation Seed\", value=args.generation_seed,\n",
    "                                                    interactive=True)\n",
    "                    with gr.Row():\n",
    "                        n_beams = gr.Dropdown(label=\"Number of Beams\", choices=list(range(1, 11, 1)),\n",
    "                                              value=args.n_beams, visible=(not args.use_sampling))\n",
    "                    with gr.Row():\n",
    "                        max_new_tokens = gr.Slider(label=\"Max Generated Tokens\", minimum=10, maximum=1000, step=10,\n",
    "                                                   value=args.max_new_tokens)\n",
    "\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(f\"#### Watermark Parameters\")\n",
    "                    with gr.Row():\n",
    "                        gamma = gr.Slider(label=\"gamma\", minimum=0.1, maximum=0.9, step=0.05, value=args.gamma)\n",
    "                    with gr.Row():\n",
    "                        delta = gr.Slider(label=\"delta\", minimum=0.0, maximum=10.0, step=0.1, value=args.delta)\n",
    "                    gr.Markdown(f\"#### Detector Parameters\")\n",
    "                    with gr.Row():\n",
    "                        detection_z_threshold = gr.Slider(label=\"z-score threshold\", minimum=0.0, maximum=10.0,\n",
    "                                                          step=0.1, value=args.detection_z_threshold)\n",
    "                    with gr.Row():\n",
    "                        ignore_repeated_bigrams = gr.Checkbox(label=\"Ignore Bigram Repeats\")\n",
    "                    with gr.Row():\n",
    "                        normalizers = gr.CheckboxGroup(label=\"Normalizations\",\n",
    "                                                       choices=[\"unicode\", \"homoglyphs\", \"truecase\"],\n",
    "                                                       value=args.normalizers)\n",
    "            # with gr.Accordion(\"Actual submitted parameters:\",open=False):\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\n",
    "                    f\"_Note: sliders don't always update perfectly. Clicking on the bar or using the number window to the right can help. Window below shows the current settings._\")\n",
    "            with gr.Row():\n",
    "                current_parameters = gr.Textbox(label=\"Current Parameters\", value=args)\n",
    "            with gr.Accordion(\"Legacy Settings\", open=False):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        seed_separately = gr.Checkbox(label=\"Seed both generations separately\",\n",
    "                                                      value=args.seed_separately)\n",
    "                    with gr.Column(scale=1):\n",
    "                        select_green_tokens = gr.Checkbox(label=\"Select 'greenlist' from partition\",\n",
    "                                                          value=args.select_green_tokens)\n",
    "\n",
    "        with gr.Accordion(\"Understanding the settings\", open=False):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                #### Generation Parameters:\n",
    "\n",
    "                - Decoding Method : We can generate tokens from the model using either multinomial sampling or we can generate using greedy decoding.\n",
    "                - Sampling Temperature : If using multinomial sampling we can set the temperature of the sampling distribution.\n",
    "                                    0.0 is equivalent to greedy decoding, and 1.0 is the maximum amount of variability/entropy in the next token distribution.\n",
    "                                    0.7 strikes a nice balance between faithfulness to the model's estimate of top candidates while adding variety. Does not apply for greedy decoding.\n",
    "                - Generation Seed : The integer to pass to the torch random number generator before running generation. Makes the multinomial sampling strategy\n",
    "                                    outputs reproducible. Does not apply for greedy decoding.\n",
    "                - Number of Beams : When using greedy decoding, we can also set the number of beams to > 1 to enable beam search.\n",
    "                                    This is not implemented/excluded from paper for multinomial sampling but may be added in future.\n",
    "                - Max Generated Tokens : The `max_new_tokens` parameter passed to the generation method to stop the output at a certain number of new tokens.\n",
    "                                        Note that the model is free to generate fewer tokens depending on the prompt.\n",
    "                                        Implicitly this sets the maximum number of prompt tokens possible as the model's maximum input length minus `max_new_tokens`,\n",
    "                                        and inputs will be truncated accordingly.\n",
    "\n",
    "                #### Watermark Parameters:\n",
    "\n",
    "                - gamma : The fraction of the vocabulary to be partitioned into the greenlist at each generation step.\n",
    "                         Smaller gamma values create a stronger watermark by enabling the watermarked model to achieve\n",
    "                         a greater differentiation from human/unwatermarked text because it is preferentially sampling\n",
    "                         from a smaller green set making those tokens less likely to occur by chance.\n",
    "                - delta : The amount of positive bias to add to the logits of every token in the greenlist\n",
    "                            at each generation step before sampling/choosing the next token. Higher delta values\n",
    "                            mean that the greenlist tokens are more heavily preferred by the watermarked model\n",
    "                            and as the bias becomes very large the watermark transitions from \"soft\" to \"hard\".\n",
    "                            For a hard watermark, nearly all tokens are green, but this can have a detrimental effect on\n",
    "                            generation quality, especially when there is not a lot of flexibility in the distribution.\n",
    "\n",
    "                #### Detector Parameters:\n",
    "\n",
    "                - z-score threshold : the z-score cuttoff for the hypothesis test. Higher thresholds (such as 4.0) make\n",
    "                                    _false positives_ (predicting that human/unwatermarked text is watermarked) very unlikely\n",
    "                                    as a genuine human text with a significant number of tokens will almost never achieve\n",
    "                                    that high of a z-score. Lower thresholds will capture more _true positives_ as some watermarked\n",
    "                                    texts will contain less green tokens and achive a lower z-score, but still pass the lower bar and\n",
    "                                    be flagged as \"watermarked\". However, a lowere threshold will increase the chance that human text\n",
    "                                    that contains a slightly higher than average number of green tokens is erroneously flagged.\n",
    "                                    4.0-5.0 offers extremely low false positive rates while still accurately catching most watermarked text.\n",
    "                - Ignore Bigram Repeats : This alternate detection algorithm only considers the unique bigrams in the text during detection,\n",
    "                                        computing the greenlists based on the first in each pair and checking whether the second falls within the list.\n",
    "                                        This means that `T` is now the unique number of bigrams in the text, which becomes less than the total\n",
    "                                        number of tokens generated if the text contains a lot of repetition. See the paper for a more detailed discussion.\n",
    "                - Normalizations : we implement a few basic normaliations to defend against various adversarial perturbations of the\n",
    "                                    text analyzed during detection. Currently we support converting all chracters to unicode,\n",
    "                                    replacing homoglyphs with a canonical form, and standardizing the capitalization.\n",
    "                                    See the paper for a detailed discussion of input normalization.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        gr.HTML(\"\"\"\n",
    "                <p>For faster inference without waiting in queue, you may duplicate the space and upgrade to GPU in settings.\n",
    "                    Follow the github link at the top and host the demo on your own GPU hardware to test out larger models.\n",
    "                <br/>\n",
    "                <a href=\"https://huggingface.co/spaces/tomg-group-umd/lm-watermarking?duplicate=true\">\n",
    "                <img style=\"margin-top: 0em; margin-bottom: 0em\" src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\"></a>\n",
    "                <p/>\n",
    "                \"\"\")\n",
    "\n",
    "        # Register main generation tab click, outputing generations as well as a the encoded+redecoded+potentially truncated prompt and flag\n",
    "        generate_btn.click(fn=generate_partial, inputs=[prompt, session_args],\n",
    "                           outputs=[redecoded_input, truncation_warning, output_without_watermark,\n",
    "                                    output_with_watermark, session_args])\n",
    "        # Show truncated version of prompt if truncation occurred\n",
    "        redecoded_input.change(fn=truncate_prompt, inputs=[redecoded_input, truncation_warning, prompt, session_args],\n",
    "                               outputs=[prompt, session_args])\n",
    "        # Call detection when the outputs (of the generate function) are updated\n",
    "        output_without_watermark.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                        outputs=[without_watermark_detection_result, session_args])\n",
    "        output_with_watermark.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                     outputs=[with_watermark_detection_result, session_args])\n",
    "        # Register main detection tab click\n",
    "        detect_btn.click(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                         outputs=[detection_result, session_args])\n",
    "\n",
    "        # State management logic\n",
    "        # update callbacks that change the state dict\n",
    "        def update_sampling_temp(session_state, value):\n",
    "            session_state.sampling_temp = float(value); return session_state\n",
    "\n",
    "        def update_generation_seed(session_state, value):\n",
    "            session_state.generation_seed = int(value); return session_state\n",
    "\n",
    "        def update_gamma(session_state, value):\n",
    "            session_state.gamma = float(value); return session_state\n",
    "\n",
    "        def update_delta(session_state, value):\n",
    "            session_state.delta = float(value); return session_state\n",
    "\n",
    "        def update_detection_z_threshold(session_state, value):\n",
    "            session_state.detection_z_threshold = float(value); return session_state\n",
    "\n",
    "        def update_decoding(session_state, value):\n",
    "            if value == \"multinomial\":\n",
    "                session_state.use_sampling = True\n",
    "            elif value == \"greedy\":\n",
    "                session_state.use_sampling = False\n",
    "            return session_state\n",
    "\n",
    "        def toggle_sampling_vis(value):\n",
    "            if value == \"multinomial\":\n",
    "                return gr.update(visible=True)\n",
    "            elif value == \"greedy\":\n",
    "                return gr.update(visible=False)\n",
    "\n",
    "        def toggle_sampling_vis_inv(value):\n",
    "            if value == \"multinomial\":\n",
    "                return gr.update(visible=False)\n",
    "            elif value == \"greedy\":\n",
    "                return gr.update(visible=True)\n",
    "\n",
    "        def update_n_beams(session_state, value):\n",
    "            session_state.n_beams = value; return session_state\n",
    "\n",
    "        def update_max_new_tokens(session_state, value):\n",
    "            session_state.max_new_tokens = int(value); return session_state\n",
    "\n",
    "        def update_ignore_repeated_bigrams(session_state, value):\n",
    "            session_state.ignore_repeated_bigrams = value; return session_state\n",
    "\n",
    "        def update_normalizers(session_state, value):\n",
    "            session_state.normalizers = value; return session_state\n",
    "\n",
    "        def update_seed_separately(session_state, value):\n",
    "            session_state.seed_separately = value; return session_state\n",
    "\n",
    "        def update_select_green_tokens(session_state, value):\n",
    "            session_state.select_green_tokens = value; return session_state\n",
    "\n",
    "        # registering callbacks for toggling the visibilty of certain parameters\n",
    "        decoding.change(toggle_sampling_vis, inputs=[decoding], outputs=[sampling_temp])\n",
    "        decoding.change(toggle_sampling_vis, inputs=[decoding], outputs=[generation_seed])\n",
    "        decoding.change(toggle_sampling_vis_inv, inputs=[decoding], outputs=[n_beams])\n",
    "        # registering all state update callbacks\n",
    "        decoding.change(update_decoding, inputs=[session_args, decoding], outputs=[session_args])\n",
    "        sampling_temp.change(update_sampling_temp, inputs=[session_args, sampling_temp], outputs=[session_args])\n",
    "        generation_seed.change(update_generation_seed, inputs=[session_args, generation_seed], outputs=[session_args])\n",
    "        n_beams.change(update_n_beams, inputs=[session_args, n_beams], outputs=[session_args])\n",
    "        max_new_tokens.change(update_max_new_tokens, inputs=[session_args, max_new_tokens], outputs=[session_args])\n",
    "        gamma.change(update_gamma, inputs=[session_args, gamma], outputs=[session_args])\n",
    "        delta.change(update_delta, inputs=[session_args, delta], outputs=[session_args])\n",
    "        detection_z_threshold.change(update_detection_z_threshold, inputs=[session_args, detection_z_threshold],\n",
    "                                     outputs=[session_args])\n",
    "        ignore_repeated_bigrams.change(update_ignore_repeated_bigrams, inputs=[session_args, ignore_repeated_bigrams],\n",
    "                                       outputs=[session_args])\n",
    "        normalizers.change(update_normalizers, inputs=[session_args, normalizers], outputs=[session_args])\n",
    "        seed_separately.change(update_seed_separately, inputs=[session_args, seed_separately], outputs=[session_args])\n",
    "        select_green_tokens.change(update_select_green_tokens, inputs=[session_args, select_green_tokens],\n",
    "                                   outputs=[session_args])\n",
    "        # register additional callback on button clicks that updates the shown parameters window\n",
    "        generate_btn.click(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        detect_btn.click(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        # When the parameters change, display the update and fire detection, since some detection params dont change the model output.\n",
    "        gamma.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        gamma.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                     outputs=[without_watermark_detection_result, session_args])\n",
    "        gamma.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                     outputs=[with_watermark_detection_result, session_args])\n",
    "        gamma.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                     outputs=[detection_result, session_args])\n",
    "        detection_z_threshold.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                     outputs=[without_watermark_detection_result, session_args])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                     outputs=[with_watermark_detection_result, session_args])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                     outputs=[detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                       outputs=[without_watermark_detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                       outputs=[with_watermark_detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                       outputs=[detection_result, session_args])\n",
    "        normalizers.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        normalizers.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                           outputs=[without_watermark_detection_result, session_args])\n",
    "        normalizers.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                           outputs=[with_watermark_detection_result, session_args])\n",
    "        normalizers.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                           outputs=[detection_result, session_args])\n",
    "        select_green_tokens.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                   outputs=[without_watermark_detection_result, session_args])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                   outputs=[with_watermark_detection_result, session_args])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                   outputs=[detection_result, session_args])\n",
    "\n",
    "    demo.queue(concurrency_count=3)\n",
    "\n",
    "    if args.demo_public:\n",
    "        demo.launch(share=True)  # exposes app to the internet via randomly generated link\n",
    "    else:\n",
    "        demo.launch()\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Run a command line version of the generation and detection operations\n",
    "        and optionally launch and serve the gradio demo\"\"\"\n",
    "    # Initial arg processing and log\n",
    "    args.normalizers = (args.normalizers.split(\",\") if args.normalizers else [])\n",
    "    print(args)\n",
    "\n",
    "    if not args.skip_model_load:\n",
    "        model, tokenizer, device = load_model(args)\n",
    "    else:\n",
    "        model, tokenizer, device = None, None, None\n",
    "\n",
    "    # Generate and detect, report to stdout\n",
    "    if not args.skip_model_load:\n",
    "        input_text = \"The United States has 63 national parks, which are congressionally designated protected areas operated by the National Park Service, an agency of the Department of the Interior.[1] National parks are designated for their natural beauty, unique geological features, diverse ecosystems, and recreational opportunities, typically 'because of some outstanding scenic feature or natural phenomena.'[2] While legislatively all units of the National Park System are considered equal with the same mission, national parks are generally larger and more of a destination, and hunting and extractive activities are prohibited.[3] National monuments, on the other hand, are also frequently protected for their historical or archaeological significance. Eight national parks (including six in Alaska) are paired with a national preserve, areas with different levels of protection that are administered together but considered separate units and whose areas are not included in the figures below. The 433 units of the National Park System can be broadly referred to as national parks, but most have other formal designations.[4]\"\n",
    "\n",
    "        model, tokenizer, device = load_model(args)\n",
    "\n",
    "        args.default_prompt = input_text\n",
    "\n",
    "        term_width = 80\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Prompt:\")\n",
    "        print(input_text)\n",
    "\n",
    "        _, _, decoded_output_without_watermark, decoded_output_with_watermark, _ = generate_kgw(input_text,\n",
    "                                                                                            args,\n",
    "                                                                                            model=model,\n",
    "                                                                                            device=device,\n",
    "                                                                                            tokenizer=tokenizer)\n",
    "        without_watermark_detection_result = detect_kgw(decoded_output_without_watermark,\n",
    "                                                    args,\n",
    "                                                    device=device,\n",
    "                                                    tokenizer=tokenizer)\n",
    "        with_watermark_detection_result = detect_kgw(decoded_output_with_watermark,\n",
    "                                                 args,\n",
    "                                                 device=device,\n",
    "                                                 tokenizer=tokenizer)\n",
    "\n",
    "        def compute_perplexity(model, tokenizer, text: str, device: torch.device) -> float:\n",
    "            \"\"\"\n",
    "            Computes the perplexity of a given text using the provided model.\n",
    "            Assumes a causal language model where providing labels yields the loss.\n",
    "            \"\"\"\n",
    "            model.eval()\n",
    "            # Tokenize the text with special tokens.\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                # When labels are the same as input_ids, the loss is the negative log-likelihood.\n",
    "                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                loss = outputs.loss  # this is the average loss per token\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            return perplexity\n",
    "\n",
    "        # Compute perplexity for both watermarked and non-watermarked outputs:\n",
    "        perplexity_nonwm = compute_perplexity(model, tokenizer, decoded_output_without_watermark, device)\n",
    "        perplexity_wm = compute_perplexity(model, tokenizer, decoded_output_with_watermark, device)\n",
    "\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Output without watermark:\")\n",
    "        print(decoded_output_without_watermark)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Detection result @ {args.detection_z_threshold}:\")\n",
    "        pprint(without_watermark_detection_result)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Perplexity (non-watermarked): {perplexity_nonwm:.2f}\")\n",
    "\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Output with watermark:\")\n",
    "        print(decoded_output_with_watermark)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Detection result @ {args.detection_z_threshold}:\")\n",
    "        print(with_watermark_detection_result[1][0])\n",
    "        pprint(with_watermark_detection_result)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Perplexity (watermarked): {perplexity_wm:.2f}\")\n",
    "\n",
    "    # Launch the app to generate and detect interactively (implements the hf space demo)\n",
    "    #if args.run_gradio:\n",
    "    #    run_gradio(args, model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "    return\n",
    "\n",
    "def normalization_strategy_lookup(strategy_name: str) -> object:\n",
    "    if strategy_name == \"unicode\":\n",
    "        return UnicodeSanitizer()\n",
    "    elif strategy_name == \"homoglyphs\":\n",
    "        return HomoglyphCanonizer()\n",
    "    elif strategy_name == \"truecase\":\n",
    "        return TrueCaser()"
   ],
   "id": "c98fe85db300a43e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:42:50.604789Z",
     "start_time": "2025-03-23T20:42:50.568467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n",
    "# available at https://arxiv.org/abs/2301.10226\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "\n",
    "import numpy  # for gradio hot reload\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSeq2SeqLM,\n",
    "                          AutoModelForCausalLM,\n",
    "                          LogitsProcessorList)\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"Util function for user friendly boolean flag args\"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Command line argument specification\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"A minimum working example of applying the watermark to any LLM that supports the huggingface 🤗 `generate` API\")\n",
    "\n",
    "    #parser.add_argument(\n",
    "    #    \"--run_gradio\",\n",
    "    #    type=str2bool,\n",
    "    #    default=False,\n",
    "    #    help=\"Whether to launch as a gradio demo. Set to False if not installed and want to just run the stdout version.\",\n",
    "    #)\n",
    "    parser.add_argument(\n",
    "        \"--demo_public\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to expose the gradio demo to the internet.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        default=\"facebook/opt-1.3b\",#\"gpt2-medium\",#\n",
    "        help=\"Main model, path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prompt_max_length\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Truncation length for prompt, overrides model config's max length field.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_new_tokens\",\n",
    "        type=int,\n",
    "        default=200,\n",
    "        help=\"Maximmum number of new tokens to generate.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generation_seed\",\n",
    "        type=int,\n",
    "        default=123,\n",
    "        help=\"Seed for setting the torch global rng prior to generation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_sampling\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to generate using multinomial sampling.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sampling_temp\",\n",
    "        type=float,\n",
    "        default=0.7,\n",
    "        help=\"Sampling temperature to use when generating using multinomial sampling.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_beams\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of beams to use for beam search. 1 is normal greedy decoding\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_gpu\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to run inference and watermark hashing/seeding/permutation on gpu.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seeding_scheme\",\n",
    "        type=str,\n",
    "        default=\"simple_1\",\n",
    "        help=\"Seeding scheme to use to generate the greenlists at each generation and verification step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gamma\",\n",
    "        type=float,\n",
    "        default=0.25,\n",
    "        help=\"The fraction of the vocabulary to partition into the greenlist at each generation and verification step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--delta\",\n",
    "        type=float,\n",
    "        default=2.0,\n",
    "        help=\"The amount/bias to add to each of the greenlist token logits before each token sampling step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--normalizers\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Single or comma separated list of the preprocessors/normalizer names to use when performing watermark detection.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ignore_repeated_bigrams\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to use the detection method that only counts each unqiue bigram once as either a green or red hit.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--detection_z_threshold\",\n",
    "        type=float,\n",
    "        default=4.0,\n",
    "        help=\"The test statistic threshold for the detection hypothesis test.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--select_green_tokens\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"How to treat the permuation when selecting the greenlist tokens at each step. Legacy is (False) to pick the complement/reds first.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip_model_load\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Skip the model loading to debug the interface.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed_separately\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to call the torch seed function before both the unwatermarked and watermarked generate calls.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_fp16\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to run model in float16 precsion.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_model(args):\n",
    "    \"\"\"Load and return the model and tokenizer\"\"\"\n",
    "\n",
    "    args.is_seq2seq_model = any([(model_type in args.model_name_or_path) for model_type in [\"t5\", \"T0\"]])\n",
    "    args.is_decoder_only_model = any(\n",
    "        [(model_type in args.model_name_or_path) for model_type in [\"gpt\", \"opt\", \"bloom\"]])\n",
    "    if args.is_seq2seq_model:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n",
    "    elif args.is_decoder_only_model:\n",
    "        if args.load_fp16:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, torch_dtype=torch.float16,\n",
    "                                                         device_map='auto')\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {args.model_name_or_path}\")\n",
    "\n",
    "    if args.use_gpu:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if args.load_fp16:\n",
    "            pass\n",
    "        else:\n",
    "            model = model.to(device)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def generate_kgw(prompt, args, model=None, device=None, tokenizer=None):\n",
    "    \"\"\"Instatiate the WatermarkLogitsProcessor according to the watermark parameters\n",
    "       and generate watermarked text by passing it to the generate method of the model\n",
    "       as a logits processor. \"\"\"\n",
    "\n",
    "    #print(f\"Generating with {args}\")\n",
    "\n",
    "    watermark_processor = WatermarkLogitsProcessor_kgw(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                                   gamma=args.gamma,\n",
    "                                                   delta=args.delta,\n",
    "                                                   seeding_scheme=args.seeding_scheme,\n",
    "                                                   select_green_tokens=args.select_green_tokens)\n",
    "\n",
    "    gen_kwargs = dict(max_new_tokens=args.max_new_tokens)\n",
    "\n",
    "    if args.use_sampling:\n",
    "        gen_kwargs.update(dict(\n",
    "            do_sample=True,\n",
    "            top_k=0,\n",
    "            temperature=args.sampling_temp\n",
    "        ))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(\n",
    "            num_beams=args.n_beams\n",
    "        ))\n",
    "\n",
    "    generate_without_watermark = partial(\n",
    "        model.generate,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    generate_with_watermark = partial(\n",
    "        model.generate,\n",
    "        logits_processor=LogitsProcessorList([watermark_processor]),\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    if args.prompt_max_length:\n",
    "        pass\n",
    "    elif hasattr(model.config, \"max_position_embedding\"):\n",
    "        args.prompt_max_length = model.config.max_position_embeddings - args.max_new_tokens\n",
    "    else:\n",
    "        args.prompt_max_length = 2048 - args.max_new_tokens\n",
    "\n",
    "    tokd_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True, truncation=True,\n",
    "                           max_length=args.prompt_max_length).to(device)\n",
    "    truncation_warning = True if tokd_input[\"input_ids\"].shape[-1] == args.prompt_max_length else False\n",
    "    redecoded_input = tokenizer.batch_decode(tokd_input[\"input_ids\"], skip_special_tokens=True)[0]\n",
    "\n",
    "    torch.manual_seed(args.generation_seed)\n",
    "    output_without_watermark = generate_without_watermark(**tokd_input)\n",
    "\n",
    "    # optional to seed before second generation, but will not be the same again generally, unless delta==0.0, no-op watermark\n",
    "    if args.seed_separately:\n",
    "        torch.manual_seed(args.generation_seed)\n",
    "    output_with_watermark = generate_with_watermark(**tokd_input)\n",
    "\n",
    "    if args.is_decoder_only_model:\n",
    "        # need to isolate the newly generated tokens\n",
    "        output_without_watermark = output_without_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "        output_with_watermark = output_with_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    decoded_output_without_watermark = tokenizer.batch_decode(output_without_watermark, skip_special_tokens=True)[0]\n",
    "    decoded_output_with_watermark = tokenizer.batch_decode(output_with_watermark, skip_special_tokens=True)[0]\n",
    "\n",
    "    return (redecoded_input,\n",
    "            int(truncation_warning),\n",
    "            decoded_output_without_watermark,\n",
    "            decoded_output_with_watermark,\n",
    "            args)\n",
    "    # decoded_output_with_watermark)\n",
    "\n",
    "\n",
    "def format_names(s):\n",
    "    \"\"\"Format names for the gradio demo interface\"\"\"\n",
    "    s = s.replace(\"num_tokens_scored\", \"Tokens Counted (T)\")\n",
    "    s = s.replace(\"num_green_tokens\", \"# Tokens in Greenlist\")\n",
    "    s = s.replace(\"green_fraction\", \"Fraction of T in Greenlist\")\n",
    "    s = s.replace(\"z_score\", \"z-score\")\n",
    "    s = s.replace(\"p_value\", \"p value\")\n",
    "    s = s.replace(\"prediction\", \"Prediction\")\n",
    "    s = s.replace(\"confidence\", \"Confidence\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def list_format_scores(score_dict, detection_threshold):\n",
    "    \"\"\"Format the detection metrics into a gradio dataframe input format\"\"\"\n",
    "    lst_2d = []\n",
    "    # lst_2d.append([\"z-score threshold\", f\"{detection_threshold}\"])\n",
    "    for k, v in score_dict.items():\n",
    "        if k == 'green_fraction':\n",
    "            lst_2d.append([format_names(k), f\"{v:.1%}\"])\n",
    "        elif k == 'confidence':\n",
    "            lst_2d.append([format_names(k), f\"{v:.3%}\"])\n",
    "        elif isinstance(v, float):\n",
    "            lst_2d.append([format_names(k), f\"{v:.3g}\"])\n",
    "        elif isinstance(v, bool):\n",
    "            lst_2d.append([format_names(k), (\"Watermarked\" if v else \"Human/Unwatermarked\")])\n",
    "        else:\n",
    "            lst_2d.append([format_names(k), f\"{v}\"])\n",
    "    if \"confidence\" in score_dict:\n",
    "        lst_2d.insert(-2, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    else:\n",
    "        lst_2d.insert(-1, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    return lst_2d\n",
    "\n",
    "\n",
    "def detect_kgw(input_text, args, device=None, tokenizer=None):\n",
    "    \"\"\"Instantiate the WatermarkDetection object and call detect on\n",
    "        the input text returning the scores and outcome of the test\"\"\"\n",
    "    watermark_detector = WatermarkDetector_kgw(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                           gamma=args.gamma,\n",
    "                                           seeding_scheme=args.seeding_scheme,\n",
    "                                           device=device,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           z_threshold=args.detection_z_threshold,\n",
    "                                           normalizers=args.normalizers,\n",
    "                                           ignore_repeated_bigrams=args.ignore_repeated_bigrams,\n",
    "                                           select_green_tokens=args.select_green_tokens)\n",
    "    if len(input_text) - 1 > watermark_detector.min_prefix_len:\n",
    "        score_dict = watermark_detector.detect(input_text)\n",
    "        # output = str_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "        output = list_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "    else:\n",
    "        score_dict = None\n",
    "        # output = (f\"Error: string not long enough to compute watermark presence.\")\n",
    "        output = [[\"Error\", \"string too short to compute metrics\"]]\n",
    "        output += [[\"\", \"\"] for _ in range(6)]\n",
    "    return output, score_dict, args\n",
    "\n",
    "\n",
    "def run_gradio(args, model=None, device=None, tokenizer=None):\n",
    "    \"\"\"Define and launch the gradio demo interface\"\"\"\n",
    "    generate_partial = partial(generate_kgw, model=model, device=device, tokenizer=tokenizer)\n",
    "    detect_partial = partial(detect_kgw, device=device, tokenizer=tokenizer)\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        # Top section, greeting and instructions\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=9):\n",
    "                gr.Markdown(\n",
    "                    \"\"\"\n",
    "                    ## 💧 [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226) 🔍\n",
    "                    \"\"\"\n",
    "                )\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\n",
    "                    \"\"\"\n",
    "                    [![](https://badgen.net/badge/icon/GitHub?icon=github&label)](https://github.com/jwkirchenbauer/lm-watermarking)\n",
    "                    \"\"\"\n",
    "                )\n",
    "            # with gr.Column(scale=2):\n",
    "            #     pass\n",
    "            # ![visitor badge](https://visitor-badge.glitch.me/badge?page_id=tomg-group-umd_lm-watermarking) # buggy\n",
    "\n",
    "        with gr.Accordion(\"Understanding the output metrics\", open=False):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                - `z-score threshold` : The cuttoff for the hypothesis test\n",
    "                - `Tokens Counted (T)` : The number of tokens in the output that were counted by the detection algorithm.\n",
    "                    The first token is ommitted in the simple, single token seeding scheme since there is no way to generate\n",
    "                    a greenlist for it as it has no prefix token(s). Under the \"Ignore Bigram Repeats\" detection algorithm,\n",
    "                    described in the bottom panel, this can be much less than the total number of tokens generated if there is a lot of repetition.\n",
    "                - `# Tokens in Greenlist` : The number of tokens that were observed to fall in their respective greenlist\n",
    "                - `Fraction of T in Greenlist` : The `# Tokens in Greenlist` / `T`. This is expected to be approximately `gamma` for human/unwatermarked text.\n",
    "                - `z-score` : The test statistic for the detection hypothesis test. If larger than the `z-score threshold`\n",
    "                    we \"reject the null hypothesis\" that the text is human/unwatermarked, and conclude it is watermarked\n",
    "                - `p value` : The likelihood of observing the computed `z-score` under the null hypothesis. This is the likelihood of\n",
    "                    observing the `Fraction of T in Greenlist` given that the text was generated without knowledge of the watermark procedure/greenlists.\n",
    "                    If this is extremely _small_ we are confident that this many green tokens was not chosen by random chance.\n",
    "                -  `prediction` : The outcome of the hypothesis test - whether the observed `z-score` was higher than the `z-score threshold`\n",
    "                - `confidence` : If we reject the null hypothesis, and the `prediction` is \"Watermarked\", then we report 1-`p value` to represent\n",
    "                    the confidence of the detection based on the unlikeliness of this `z-score` observation.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        with gr.Accordion(\"A note on model capability\", open=True):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                This demo uses open-source language models that fit on a single GPU. These models are less powerful than proprietary commercial tools like ChatGPT, Claude, or Bard.\n",
    "\n",
    "                Importantly, we use a language model that is designed to \"complete\" your prompt, and not a model this is fine-tuned to follow instructions.\n",
    "                For best results, prompt the model with a few sentences that form the beginning of a paragraph, and then allow it to \"continue\" your paragraph.\n",
    "                Some examples include the opening paragraph of a wikipedia article, or the first few sentences of a story.\n",
    "                Longer prompts that end mid-sentence will result in more fluent generations.\n",
    "                \"\"\"\n",
    "            )\n",
    "        gr.Markdown(f\"Language model: {args.model_name_or_path} {'(float16 mode)' if args.load_fp16 else ''}\")\n",
    "\n",
    "        # Construct state for parameters, define updates and toggles\n",
    "        default_prompt = args.__dict__.pop(\"default_prompt\")\n",
    "        session_args = gr.State(value=args)\n",
    "\n",
    "        with gr.Tab(\"Generate and Detect\"):\n",
    "\n",
    "            with gr.Row():\n",
    "                prompt = gr.Textbox(label=f\"Prompt\", interactive=True, lines=10, max_lines=10, value=default_prompt)\n",
    "            with gr.Row():\n",
    "                generate_btn = gr.Button(\"Generate\")\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    output_without_watermark = gr.Textbox(label=\"Output Without Watermark\", interactive=False, lines=14,\n",
    "                                                          max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # without_watermark_detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    without_watermark_detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False,\n",
    "                                                                      row_count=7, col_count=2)\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    output_with_watermark = gr.Textbox(label=\"Output With Watermark\", interactive=False, lines=14,\n",
    "                                                       max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # with_watermark_detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    with_watermark_detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False,\n",
    "                                                                   row_count=7, col_count=2)\n",
    "\n",
    "            redecoded_input = gr.Textbox(visible=False)\n",
    "            truncation_warning = gr.Number(visible=False)\n",
    "\n",
    "            def truncate_prompt(redecoded_input, truncation_warning, orig_prompt, args):\n",
    "                if truncation_warning:\n",
    "                    return redecoded_input + f\"\\n\\n[Prompt was truncated before generation due to length...]\", args\n",
    "                else:\n",
    "                    return orig_prompt, args\n",
    "\n",
    "        with gr.Tab(\"Detector Only\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    detection_input = gr.Textbox(label=\"Text to Analyze\", interactive=True, lines=14, max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False, row_count=7,\n",
    "                                                    col_count=2)\n",
    "            with gr.Row():\n",
    "                detect_btn = gr.Button(\"Detect\")\n",
    "\n",
    "        # Parameter selection group\n",
    "        with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(f\"#### Generation Parameters\")\n",
    "                    with gr.Row():\n",
    "                        decoding = gr.Radio(label=\"Decoding Method\", choices=[\"multinomial\", \"greedy\"],\n",
    "                                            value=(\"multinomial\" if args.use_sampling else \"greedy\"))\n",
    "                    with gr.Row():\n",
    "                        sampling_temp = gr.Slider(label=\"Sampling Temperature\", minimum=0.1, maximum=1.0, step=0.1,\n",
    "                                                  value=args.sampling_temp, visible=True)\n",
    "                    with gr.Row():\n",
    "                        generation_seed = gr.Number(label=\"Generation Seed\", value=args.generation_seed,\n",
    "                                                    interactive=True)\n",
    "                    with gr.Row():\n",
    "                        n_beams = gr.Dropdown(label=\"Number of Beams\", choices=list(range(1, 11, 1)),\n",
    "                                              value=args.n_beams, visible=(not args.use_sampling))\n",
    "                    with gr.Row():\n",
    "                        max_new_tokens = gr.Slider(label=\"Max Generated Tokens\", minimum=10, maximum=1000, step=10,\n",
    "                                                   value=args.max_new_tokens)\n",
    "\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(f\"#### Watermark Parameters\")\n",
    "                    with gr.Row():\n",
    "                        gamma = gr.Slider(label=\"gamma\", minimum=0.1, maximum=0.9, step=0.05, value=args.gamma)\n",
    "                    with gr.Row():\n",
    "                        delta = gr.Slider(label=\"delta\", minimum=0.0, maximum=10.0, step=0.1, value=args.delta)\n",
    "                    gr.Markdown(f\"#### Detector Parameters\")\n",
    "                    with gr.Row():\n",
    "                        detection_z_threshold = gr.Slider(label=\"z-score threshold\", minimum=0.0, maximum=10.0,\n",
    "                                                          step=0.1, value=args.detection_z_threshold)\n",
    "                    with gr.Row():\n",
    "                        ignore_repeated_bigrams = gr.Checkbox(label=\"Ignore Bigram Repeats\")\n",
    "                    with gr.Row():\n",
    "                        normalizers = gr.CheckboxGroup(label=\"Normalizations\",\n",
    "                                                       choices=[\"unicode\", \"homoglyphs\", \"truecase\"],\n",
    "                                                       value=args.normalizers)\n",
    "            # with gr.Accordion(\"Actual submitted parameters:\",open=False):\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\n",
    "                    f\"_Note: sliders don't always update perfectly. Clicking on the bar or using the number window to the right can help. Window below shows the current settings._\")\n",
    "            with gr.Row():\n",
    "                current_parameters = gr.Textbox(label=\"Current Parameters\", value=args)\n",
    "            with gr.Accordion(\"Legacy Settings\", open=False):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        seed_separately = gr.Checkbox(label=\"Seed both generations separately\",\n",
    "                                                      value=args.seed_separately)\n",
    "                    with gr.Column(scale=1):\n",
    "                        select_green_tokens = gr.Checkbox(label=\"Select 'greenlist' from partition\",\n",
    "                                                          value=args.select_green_tokens)\n",
    "\n",
    "        with gr.Accordion(\"Understanding the settings\", open=False):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                #### Generation Parameters:\n",
    "\n",
    "                - Decoding Method : We can generate tokens from the model using either multinomial sampling or we can generate using greedy decoding.\n",
    "                - Sampling Temperature : If using multinomial sampling we can set the temperature of the sampling distribution.\n",
    "                                    0.0 is equivalent to greedy decoding, and 1.0 is the maximum amount of variability/entropy in the next token distribution.\n",
    "                                    0.7 strikes a nice balance between faithfulness to the model's estimate of top candidates while adding variety. Does not apply for greedy decoding.\n",
    "                - Generation Seed : The integer to pass to the torch random number generator before running generation. Makes the multinomial sampling strategy\n",
    "                                    outputs reproducible. Does not apply for greedy decoding.\n",
    "                - Number of Beams : When using greedy decoding, we can also set the number of beams to > 1 to enable beam search.\n",
    "                                    This is not implemented/excluded from paper for multinomial sampling but may be added in future.\n",
    "                - Max Generated Tokens : The `max_new_tokens` parameter passed to the generation method to stop the output at a certain number of new tokens.\n",
    "                                        Note that the model is free to generate fewer tokens depending on the prompt.\n",
    "                                        Implicitly this sets the maximum number of prompt tokens possible as the model's maximum input length minus `max_new_tokens`,\n",
    "                                        and inputs will be truncated accordingly.\n",
    "\n",
    "                #### Watermark Parameters:\n",
    "\n",
    "                - gamma : The fraction of the vocabulary to be partitioned into the greenlist at each generation step.\n",
    "                         Smaller gamma values create a stronger watermark by enabling the watermarked model to achieve\n",
    "                         a greater differentiation from human/unwatermarked text because it is preferentially sampling\n",
    "                         from a smaller green set making those tokens less likely to occur by chance.\n",
    "                - delta : The amount of positive bias to add to the logits of every token in the greenlist\n",
    "                            at each generation step before sampling/choosing the next token. Higher delta values\n",
    "                            mean that the greenlist tokens are more heavily preferred by the watermarked model\n",
    "                            and as the bias becomes very large the watermark transitions from \"soft\" to \"hard\".\n",
    "                            For a hard watermark, nearly all tokens are green, but this can have a detrimental effect on\n",
    "                            generation quality, especially when there is not a lot of flexibility in the distribution.\n",
    "\n",
    "                #### Detector Parameters:\n",
    "\n",
    "                - z-score threshold : the z-score cuttoff for the hypothesis test. Higher thresholds (such as 4.0) make\n",
    "                                    _false positives_ (predicting that human/unwatermarked text is watermarked) very unlikely\n",
    "                                    as a genuine human text with a significant number of tokens will almost never achieve\n",
    "                                    that high of a z-score. Lower thresholds will capture more _true positives_ as some watermarked\n",
    "                                    texts will contain less green tokens and achive a lower z-score, but still pass the lower bar and\n",
    "                                    be flagged as \"watermarked\". However, a lowere threshold will increase the chance that human text\n",
    "                                    that contains a slightly higher than average number of green tokens is erroneously flagged.\n",
    "                                    4.0-5.0 offers extremely low false positive rates while still accurately catching most watermarked text.\n",
    "                - Ignore Bigram Repeats : This alternate detection algorithm only considers the unique bigrams in the text during detection,\n",
    "                                        computing the greenlists based on the first in each pair and checking whether the second falls within the list.\n",
    "                                        This means that `T` is now the unique number of bigrams in the text, which becomes less than the total\n",
    "                                        number of tokens generated if the text contains a lot of repetition. See the paper for a more detailed discussion.\n",
    "                - Normalizations : we implement a few basic normaliations to defend against various adversarial perturbations of the\n",
    "                                    text analyzed during detection. Currently we support converting all chracters to unicode,\n",
    "                                    replacing homoglyphs with a canonical form, and standardizing the capitalization.\n",
    "                                    See the paper for a detailed discussion of input normalization.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        gr.HTML(\"\"\"\n",
    "                <p>For faster inference without waiting in queue, you may duplicate the space and upgrade to GPU in settings.\n",
    "                    Follow the github link at the top and host the demo on your own GPU hardware to test out larger models.\n",
    "                <br/>\n",
    "                <a href=\"https://huggingface.co/spaces/tomg-group-umd/lm-watermarking?duplicate=true\">\n",
    "                <img style=\"margin-top: 0em; margin-bottom: 0em\" src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\"></a>\n",
    "                <p/>\n",
    "                \"\"\")\n",
    "\n",
    "        # Register main generation tab click, outputing generations as well as a the encoded+redecoded+potentially truncated prompt and flag\n",
    "        generate_btn.click(fn=generate_partial, inputs=[prompt, session_args],\n",
    "                           outputs=[redecoded_input, truncation_warning, output_without_watermark,\n",
    "                                    output_with_watermark, session_args])\n",
    "        # Show truncated version of prompt if truncation occurred\n",
    "        redecoded_input.change(fn=truncate_prompt, inputs=[redecoded_input, truncation_warning, prompt, session_args],\n",
    "                               outputs=[prompt, session_args])\n",
    "        # Call detection when the outputs (of the generate function) are updated\n",
    "        output_without_watermark.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                        outputs=[without_watermark_detection_result, session_args])\n",
    "        output_with_watermark.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                     outputs=[with_watermark_detection_result, session_args])\n",
    "        # Register main detection tab click\n",
    "        detect_btn.click(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                         outputs=[detection_result, session_args])\n",
    "\n",
    "        # State management logic\n",
    "        # update callbacks that change the state dict\n",
    "        def update_sampling_temp(session_state, value):\n",
    "            session_state.sampling_temp = float(value); return session_state\n",
    "\n",
    "        def update_generation_seed(session_state, value):\n",
    "            session_state.generation_seed = int(value); return session_state\n",
    "\n",
    "        def update_gamma(session_state, value):\n",
    "            session_state.gamma = float(value); return session_state\n",
    "\n",
    "        def update_delta(session_state, value):\n",
    "            session_state.delta = float(value); return session_state\n",
    "\n",
    "        def update_detection_z_threshold(session_state, value):\n",
    "            session_state.detection_z_threshold = float(value); return session_state\n",
    "\n",
    "        def update_decoding(session_state, value):\n",
    "            if value == \"multinomial\":\n",
    "                session_state.use_sampling = True\n",
    "            elif value == \"greedy\":\n",
    "                session_state.use_sampling = False\n",
    "            return session_state\n",
    "\n",
    "        def toggle_sampling_vis(value):\n",
    "            if value == \"multinomial\":\n",
    "                return gr.update(visible=True)\n",
    "            elif value == \"greedy\":\n",
    "                return gr.update(visible=False)\n",
    "\n",
    "        def toggle_sampling_vis_inv(value):\n",
    "            if value == \"multinomial\":\n",
    "                return gr.update(visible=False)\n",
    "            elif value == \"greedy\":\n",
    "                return gr.update(visible=True)\n",
    "\n",
    "        def update_n_beams(session_state, value):\n",
    "            session_state.n_beams = value; return session_state\n",
    "\n",
    "        def update_max_new_tokens(session_state, value):\n",
    "            session_state.max_new_tokens = int(value); return session_state\n",
    "\n",
    "        def update_ignore_repeated_bigrams(session_state, value):\n",
    "            session_state.ignore_repeated_bigrams = value; return session_state\n",
    "\n",
    "        def update_normalizers(session_state, value):\n",
    "            session_state.normalizers = value; return session_state\n",
    "\n",
    "        def update_seed_separately(session_state, value):\n",
    "            session_state.seed_separately = value; return session_state\n",
    "\n",
    "        def update_select_green_tokens(session_state, value):\n",
    "            session_state.select_green_tokens = value; return session_state\n",
    "\n",
    "        # registering callbacks for toggling the visibilty of certain parameters\n",
    "        decoding.change(toggle_sampling_vis, inputs=[decoding], outputs=[sampling_temp])\n",
    "        decoding.change(toggle_sampling_vis, inputs=[decoding], outputs=[generation_seed])\n",
    "        decoding.change(toggle_sampling_vis_inv, inputs=[decoding], outputs=[n_beams])\n",
    "        # registering all state update callbacks\n",
    "        decoding.change(update_decoding, inputs=[session_args, decoding], outputs=[session_args])\n",
    "        sampling_temp.change(update_sampling_temp, inputs=[session_args, sampling_temp], outputs=[session_args])\n",
    "        generation_seed.change(update_generation_seed, inputs=[session_args, generation_seed], outputs=[session_args])\n",
    "        n_beams.change(update_n_beams, inputs=[session_args, n_beams], outputs=[session_args])\n",
    "        max_new_tokens.change(update_max_new_tokens, inputs=[session_args, max_new_tokens], outputs=[session_args])\n",
    "        gamma.change(update_gamma, inputs=[session_args, gamma], outputs=[session_args])\n",
    "        delta.change(update_delta, inputs=[session_args, delta], outputs=[session_args])\n",
    "        detection_z_threshold.change(update_detection_z_threshold, inputs=[session_args, detection_z_threshold],\n",
    "                                     outputs=[session_args])\n",
    "        ignore_repeated_bigrams.change(update_ignore_repeated_bigrams, inputs=[session_args, ignore_repeated_bigrams],\n",
    "                                       outputs=[session_args])\n",
    "        normalizers.change(update_normalizers, inputs=[session_args, normalizers], outputs=[session_args])\n",
    "        seed_separately.change(update_seed_separately, inputs=[session_args, seed_separately], outputs=[session_args])\n",
    "        select_green_tokens.change(update_select_green_tokens, inputs=[session_args, select_green_tokens],\n",
    "                                   outputs=[session_args])\n",
    "        # register additional callback on button clicks that updates the shown parameters window\n",
    "        generate_btn.click(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        detect_btn.click(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        # When the parameters change, display the update and fire detection, since some detection params dont change the model output.\n",
    "        gamma.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        gamma.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                     outputs=[without_watermark_detection_result, session_args])\n",
    "        gamma.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                     outputs=[with_watermark_detection_result, session_args])\n",
    "        gamma.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                     outputs=[detection_result, session_args])\n",
    "        detection_z_threshold.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                     outputs=[without_watermark_detection_result, session_args])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                     outputs=[with_watermark_detection_result, session_args])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                     outputs=[detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                       outputs=[without_watermark_detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                       outputs=[with_watermark_detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                       outputs=[detection_result, session_args])\n",
    "        normalizers.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        normalizers.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                           outputs=[without_watermark_detection_result, session_args])\n",
    "        normalizers.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                           outputs=[with_watermark_detection_result, session_args])\n",
    "        normalizers.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                           outputs=[detection_result, session_args])\n",
    "        select_green_tokens.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                   outputs=[without_watermark_detection_result, session_args])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                   outputs=[with_watermark_detection_result, session_args])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                   outputs=[detection_result, session_args])\n",
    "\n",
    "    demo.queue(concurrency_count=3)\n",
    "\n",
    "    if args.demo_public:\n",
    "        demo.launch(share=True)  # exposes app to the internet via randomly generated link\n",
    "    else:\n",
    "        demo.launch()\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Run a command line version of the generation and detection operations\n",
    "        and optionally launch and serve the gradio demo\"\"\"\n",
    "    # Initial arg processing and log\n",
    "    args.normalizers = (args.normalizers.split(\",\") if args.normalizers else [])\n",
    "    print(args)\n",
    "\n",
    "    if not args.skip_model_load:\n",
    "        model, tokenizer, device = load_model(args)\n",
    "    else:\n",
    "        model, tokenizer, device = None, None, None\n",
    "\n",
    "    # Generate and detect, report to stdout\n",
    "    if not args.skip_model_load:\n",
    "        input_text = \"The United States has 63 national parks, which are congressionally designated protected areas operated by the National Park Service, an agency of the Department of the Interior.[1] National parks are designated for their natural beauty, unique geological features, diverse ecosystems, and recreational opportunities, typically 'because of some outstanding scenic feature or natural phenomena.'[2] While legislatively all units of the National Park System are considered equal with the same mission, national parks are generally larger and more of a destination, and hunting and extractive activities are prohibited.[3] National monuments, on the other hand, are also frequently protected for their historical or archaeological significance. Eight national parks (including six in Alaska) are paired with a national preserve, areas with different levels of protection that are administered together but considered separate units and whose areas are not included in the figures below. The 433 units of the National Park System can be broadly referred to as national parks, but most have other formal designations.[4]\"\n",
    "\n",
    "        model, tokenizer, device = load_model(args)\n",
    "\n",
    "        args.default_prompt = input_text\n",
    "\n",
    "        term_width = 80\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Prompt:\")\n",
    "        print(input_text)\n",
    "\n",
    "        _, _, decoded_output_without_watermark, decoded_output_with_watermark, _ = generate_kgw(input_text,\n",
    "                                                                                            args,\n",
    "                                                                                            model=model,\n",
    "                                                                                            device=device,\n",
    "                                                                                            tokenizer=tokenizer)\n",
    "        without_watermark_detection_result = detect_kgw(decoded_output_without_watermark,\n",
    "                                                    args,\n",
    "                                                    device=device,\n",
    "                                                    tokenizer=tokenizer)\n",
    "        with_watermark_detection_result = detect_kgw(decoded_output_with_watermark,\n",
    "                                                 args,\n",
    "                                                 device=device,\n",
    "                                                 tokenizer=tokenizer)\n",
    "\n",
    "        def compute_perplexity(model, tokenizer, text: str, device: torch.device) -> float:\n",
    "            \"\"\"\n",
    "            Computes the perplexity of a given text using the provided model.\n",
    "            Assumes a causal language model where providing labels yields the loss.\n",
    "            \"\"\"\n",
    "            model.eval()\n",
    "            # Tokenize the text with special tokens.\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                # When labels are the same as input_ids, the loss is the negative log-likelihood.\n",
    "                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                loss = outputs.loss  # this is the average loss per token\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            return perplexity\n",
    "\n",
    "        # Compute perplexity for both watermarked and non-watermarked outputs:\n",
    "        perplexity_nonwm = compute_perplexity(model, tokenizer, decoded_output_without_watermark, device)\n",
    "        perplexity_wm = compute_perplexity(model, tokenizer, decoded_output_with_watermark, device)\n",
    "\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Output without watermark:\")\n",
    "        print(decoded_output_without_watermark)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Detection result @ {args.detection_z_threshold}:\")\n",
    "        pprint(without_watermark_detection_result)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Perplexity (non-watermarked): {perplexity_nonwm:.2f}\")\n",
    "\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Output with watermark:\")\n",
    "        print(decoded_output_with_watermark)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Detection result @ {args.detection_z_threshold}:\")\n",
    "        print(with_watermark_detection_result[1][0])\n",
    "        pprint(with_watermark_detection_result)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Perplexity (watermarked): {perplexity_wm:.2f}\")\n",
    "\n",
    "    # Launch the app to generate and detect interactively (implements the hf space demo)\n",
    "    #if args.run_gradio:\n",
    "    #    run_gradio(args, model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "    return"
   ],
   "id": "e00d21f73d8d45a3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T20:42:54.142244Z",
     "start_time": "2025-03-23T20:42:54.110409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n",
    "# available at https://arxiv.org/abs/2301.10226\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import annotations\n",
    "import collections\n",
    "from math import sqrt\n",
    "import scipy.stats\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import LogitsProcessor\n",
    "import nltk\n",
    "import ssl\n",
    "from nltk.util import ngrams\n",
    "\n",
    "###############################################\n",
    "# Revised Watermark Classes\n",
    "###############################################\n",
    "\n",
    "class WatermarkBase_kgw:\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab: list[int] = None,\n",
    "            gamma: float = 0.5,\n",
    "            delta: float = 2.0,\n",
    "            seeding_scheme: str = \"simple_1\",\n",
    "            hash_key: int = 15485863,\n",
    "            select_green_tokens: bool = True,\n",
    "            precomputed_pairing: list[tuple[int, int]] = None,\n",
    "            unique_tokens: list[int] = None,\n",
    "    ):\n",
    "        self.vocab = vocab  # list of token IDs (usually 0,...,n-1)\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.gamma = gamma  # fraction of tokens to designate as green (target size)\n",
    "        self.delta = delta  # bias to add to green tokens' logits\n",
    "        self.seeding_scheme = seeding_scheme\n",
    "        self.rng = None\n",
    "        self.hash_key = hash_key\n",
    "        self.select_green_tokens = select_green_tokens\n",
    "        self.pairing = precomputed_pairing  # perfect matching on tokens with synonyms (indices in vocab)\n",
    "        self.unique_tokens = unique_tokens  # list of token IDs that have no synonyms\n",
    "\n",
    "    def _seed_rng(self, input_ids: torch.LongTensor, seeding_scheme: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Seeds the RNG deterministically using the last token in input_ids.\n",
    "        For the \"simple_1\" scheme, seed = hash_key * (last token id).\n",
    "        \"\"\"\n",
    "        if seeding_scheme is None:\n",
    "            seeding_scheme = self.seeding_scheme\n",
    "        # Ensure the RNG is initialized.\n",
    "        if self.rng is None:\n",
    "            self.rng = torch.Generator(device=input_ids.device)\n",
    "        if seeding_scheme == \"simple_1\":\n",
    "            assert input_ids.shape[-1] >= 1, \"Input must have at least one token.\"\n",
    "            prev_token = input_ids[-1].item()\n",
    "            self.rng.manual_seed(self.hash_key * prev_token)\n",
    "            #print(self.hash_key * prev_token)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Seeding scheme {seeding_scheme} not implemented.\")\n",
    "        return\n",
    "\n",
    "    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> list[int]:\n",
    "        \"\"\"\n",
    "        Returns a list of token IDs that form the green list.\n",
    "        If a precomputed pairing exists, then:\n",
    "          - All tokens from the unique set (those with no synonyms) are automatically in the green list.\n",
    "          - For each pair in the perfect matching (on tokens with synonyms), a fair coin flip (using the seeded RNG)\n",
    "            selects one token from the pair.\n",
    "        Otherwise, falls back to a random permutation method.\n",
    "        Optionally, the list may be truncated to a target size (gamma * vocab_size).\n",
    "        \"\"\"\n",
    "        self._seed_rng(input_ids)\n",
    "        # Fallback: use random permutation.\n",
    "        greenlist_size = int(self.vocab_size * self.gamma)\n",
    "        vocab_permutation = torch.randperm(self.vocab_size, device=input_ids.device, generator=self.rng)\n",
    "        if self.select_green_tokens:\n",
    "            return vocab_permutation[:greenlist_size].tolist()\n",
    "        else:\n",
    "            return vocab_permutation[-greenlist_size:].tolist()\n",
    "        #if self.pairing is None or self.unique_tokens is None:\n",
    "            # Fallback: use random permutation.\n",
    "        #    greenlist_size = int(self.vocab_size * self.gamma)\n",
    "        #    vocab_permutation = torch.randperm(self.vocab_size, device=input_ids.device, generator=self.rng)\n",
    "        #    if self.select_green_tokens:\n",
    "        #        return vocab_permutation[:greenlist_size].tolist()\n",
    "        #    else:\n",
    "        #        return vocab_permutation[-greenlist_size:].tolist()\n",
    "        #else:\n",
    "            # Start with all unique tokens.\n",
    "        #    greenlist_ids = self.unique_tokens.copy()\n",
    "            # For tokens that have synonyms (precomputed pairing), randomly assign one from each pair.\n",
    "        #    for pair in self.pairing:\n",
    "        #        coin_flip = (torch.rand(1, generator=self.rng, device=input_ids.device) < 0.5).item()\n",
    "        #        chosen = pair[0] if coin_flip == 1 else pair[1]\n",
    "        #        greenlist_ids.append(chosen)\n",
    "            # Optionally, enforce a maximum size (gamma * vocab_size)\n",
    "        #    desired_size = int(self.vocab_size * self.gamma)\n",
    "        #    if len(greenlist_ids) > desired_size:\n",
    "        #        indices = torch.randperm(len(greenlist_ids), generator=self.rng, device=input_ids.device)[:desired_size].tolist()\n",
    "        #        greenlist_ids = [greenlist_ids[i] for i in indices]\n",
    "        #    return greenlist_ids\n",
    "\n",
    "\n",
    "class WatermarkLogitsProcessor_kgw(WatermarkBase_kgw, LogitsProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _calc_greenlist_mask(self, scores: torch.FloatTensor, greenlist_token_ids) -> torch.BoolTensor:\n",
    "        green_tokens_mask = torch.zeros_like(scores)\n",
    "        for b_idx in range(len(greenlist_token_ids)):\n",
    "            green_tokens_mask[b_idx][greenlist_token_ids[b_idx]] = 1\n",
    "        return green_tokens_mask.bool()\n",
    "\n",
    "    def _bias_greenlist_logits(self, scores: torch.Tensor, greenlist_mask: torch.Tensor,\n",
    "                               greenlist_bias: float) -> torch.Tensor:\n",
    "        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\n",
    "        return scores\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        #print(input_ids.device)\n",
    "        if self.rng is None:\n",
    "            self.rng = torch.Generator(device=input_ids.device)\n",
    "        batched_greenlist_ids = [None for _ in range(input_ids.shape[0])]\n",
    "        for b_idx in range(input_ids.shape[0]):\n",
    "            greenlist_ids = self._get_greenlist_ids(input_ids[b_idx])\n",
    "            batched_greenlist_ids[b_idx] = greenlist_ids\n",
    "        green_tokens_mask = self._calc_greenlist_mask(scores, batched_greenlist_ids)\n",
    "        scores = self._bias_greenlist_logits(scores, green_tokens_mask, self.delta)\n",
    "        #print(torch.max(scores))\n",
    "        return scores\n",
    "\n",
    "\n",
    "class WatermarkDetector_kgw(WatermarkBase_kgw):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            device: torch.device = None,\n",
    "            tokenizer: Tokenizer = None,\n",
    "            z_threshold: float = 4.0,\n",
    "            normalizers: list[str] = [\"unicode\"],\n",
    "            ignore_repeated_bigrams: bool = True,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert device, \"Device must be provided.\"\n",
    "        assert tokenizer, \"A tokenizer is required for detection.\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.z_threshold = z_threshold\n",
    "        self.rng = torch.Generator(device=self.device)\n",
    "        if self.seeding_scheme == \"simple_1\":\n",
    "            self.min_prefix_len = 1\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Seeding scheme {self.seeding_scheme} not implemented.\")\n",
    "        self.normalizers = [normalization_strategy_lookup(norm) for norm in normalizers]\n",
    "        self.ignore_repeated_bigrams = ignore_repeated_bigrams\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            assert self.seeding_scheme == \"simple_1\", \"Repeated bigram variant requires simple_1 seeding.\"\n",
    "\n",
    "    def _compute_z_score(self, observed_count, T):\n",
    "        expected_count = self.gamma\n",
    "        numer = observed_count - expected_count * T\n",
    "        denom = sqrt(T * expected_count * (1 - expected_count))\n",
    "        return numer / denom\n",
    "\n",
    "    def _compute_p_value(self, z):\n",
    "        return scipy.stats.norm.sf(z)\n",
    "\n",
    "    def _score_sequence(\n",
    "            self,\n",
    "            input_ids: Tensor,\n",
    "            return_num_tokens_scored: bool = True,\n",
    "            return_num_green_tokens: bool = True,\n",
    "            return_green_fraction: bool = True,\n",
    "            return_green_token_mask: bool = False,\n",
    "            return_z_score: bool = True,\n",
    "            return_p_value: bool = True,\n",
    "    ):\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            bigram_table = {}\n",
    "            token_bigram_generator = ngrams(input_ids.cpu().tolist(), 2)\n",
    "            freq = collections.Counter(token_bigram_generator)\n",
    "            num_tokens_scored = len(freq.keys())\n",
    "            for bigram in freq.keys():\n",
    "                prefix = torch.tensor([bigram[0]], device=self.device)\n",
    "                greenlist_ids = self._get_greenlist_ids(prefix)\n",
    "                bigram_table[bigram] = True if bigram[1] in greenlist_ids else False\n",
    "            green_token_count = sum(bigram_table.values())\n",
    "        else:\n",
    "            #print(1)\n",
    "            num_tokens_scored = len(input_ids) - self.min_prefix_len\n",
    "            if num_tokens_scored < 1:\n",
    "                score_dict = None\n",
    "                return score_dict #raise ValueError(\"Not enough tokens to score.\")\n",
    "            green_token_count = 0\n",
    "            green_token_mask = []\n",
    "            for idx in range(self.min_prefix_len, len(input_ids)):\n",
    "                curr_token = input_ids[idx]\n",
    "                greenlist_ids = self._get_greenlist_ids(input_ids[:idx])\n",
    "                if curr_token in greenlist_ids:\n",
    "                    green_token_count += 1\n",
    "                    green_token_mask.append(True)\n",
    "                    #print(True)\n",
    "                else:\n",
    "                    green_token_mask.append(False)\n",
    "                    #print(False)\n",
    "        score_dict = {}\n",
    "        if return_num_tokens_scored:\n",
    "            score_dict[\"num_tokens_scored\"] = num_tokens_scored\n",
    "        if return_num_green_tokens:\n",
    "            score_dict[\"num_green_tokens\"] = green_token_count\n",
    "        if return_green_fraction:\n",
    "            score_dict[\"green_fraction\"] = green_token_count / num_tokens_scored\n",
    "        if return_z_score:\n",
    "            score_dict[\"z_score\"] = self._compute_z_score(green_token_count, num_tokens_scored)\n",
    "        if return_p_value:\n",
    "            z = score_dict.get(\"z_score\", self._compute_z_score(green_token_count, num_tokens_scored))\n",
    "            score_dict[\"p_value\"] = self._compute_p_value(z)\n",
    "        if return_green_token_mask:\n",
    "            score_dict[\"green_token_mask\"] = green_token_mask\n",
    "        return score_dict\n",
    "\n",
    "    def detect(\n",
    "            self,\n",
    "            text: str = None,\n",
    "            tokenized_text: list[int] = None,\n",
    "            return_prediction: bool = True,\n",
    "            return_scores: bool = True,\n",
    "            z_threshold: float = None,\n",
    "            **kwargs,\n",
    "    ) -> dict:\n",
    "        assert (text is not None) ^ (tokenized_text is not None), \"Provide either raw or tokenized text.\"\n",
    "        if return_prediction:\n",
    "            kwargs[\"return_p_value\"] = True\n",
    "        for normalizer in self.normalizers:\n",
    "            text = normalizer(text)\n",
    "        if tokenized_text is None:\n",
    "            tokenized_text = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].to(\n",
    "                self.device)\n",
    "            if tokenized_text[0] == self.tokenizer.bos_token_id:\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "        else:\n",
    "            if self.tokenizer is not None and tokenized_text[0] == self.tokenizer.bos_token_id:\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "        output_dict = {}\n",
    "        score_dict = self._score_sequence(tokenized_text, **kwargs)\n",
    "        if return_scores:\n",
    "            output_dict.update(score_dict)\n",
    "        if return_prediction:\n",
    "            z_threshold = z_threshold if z_threshold is not None else self.z_threshold\n",
    "            output_dict[\"prediction\"] = score_dict[\"z_score\"] > z_threshold\n",
    "            if output_dict[\"prediction\"]:\n",
    "                output_dict[\"confidence\"] = 1 - score_dict[\"p_value\"]\n",
    "        return output_dict\n",
    "\n",
    "'''\n",
    "###############################################\n",
    "# Outline of the New Partitioning Method\n",
    "###############################################\n",
    "# 1. Obtain the vocabulary from the model's tokenizer using get_vocabulary(tokenizer).\n",
    "# 2. Use filter_tokens_with_synonyms(vocab_list) to split the vocabulary indices into:\n",
    "#       - unique_indices: tokens with no synonyms (set A)\n",
    "#       - paired_indices: tokens with at least one synonym (set B)\n",
    "# 3. Construct the similarity matrix for tokens in set B using construct_similarity_matrix.\n",
    "# 4. Compute a perfect matching on set B using find_perfect_matching.\n",
    "# 5. In _get_greenlist_ids, use the precomputed pairing (mapped back to original token IDs)\n",
    "#    and return all tokens from set A plus one token per pair (chosen at random by a coin flip).\n",
    "###############################################\n",
    "# Full Code Example for the Revised Watermark Partition\n",
    "###############################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For testing purposes, use a small model like 'distilgpt2' which can run on CPU.\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "    vocab_list = get_vocabulary(tokenizer)\n",
    "    print(f\"Vocabulary size: {len(vocab_list)}\")\n",
    "\n",
    "    # Filter tokens: separate those with no synonyms (set A) and with synonyms (set B)\n",
    "    unique_indices, paired_indices = filter_tokens_with_synonyms(vocab_list)\n",
    "    print(f\"Number of unique tokens (no synonyms, set A): {len(unique_indices)}\")\n",
    "    print(f\"Number of tokens with synonyms (set B): {len(paired_indices)}\")\n",
    "\n",
    "    # Construct similarity matrix for tokens in set B.\n",
    "    similarity_matrix = construct_similarity_matrix(vocab_list, paired_indices)\n",
    "    # Find a perfect matching on the tokens in set B.\n",
    "    matching = find_perfect_matching(similarity_matrix)\n",
    "    # Map matching indices (relative to paired_indices) back to the original vocabulary indices.\n",
    "    mapped_pairing = [(paired_indices[i], paired_indices[j]) for (i, j) in matching]\n",
    "    print(\"Computed perfect matching (pairs) on tokens with synonyms (set B):\")\n",
    "    print(mapped_pairing)\n",
    "\n",
    "    # Initialize WatermarkLogitsProcessor with precomputed pairing and unique tokens.\n",
    "    wm_processor = WatermarkLogitsProcessor(\n",
    "        vocab=list(range(len(vocab_list))),\n",
    "        gamma=0.25,\n",
    "        delta=2.0,\n",
    "        seeding_scheme=\"simple_1\",\n",
    "        select_green_tokens=True,\n",
    "        precomputed_pairing=mapped_pairing,\n",
    "        unique_tokens=unique_indices\n",
    "    )\n",
    "\n",
    "    # Test _get_greenlist_ids with a sample prompt.\n",
    "    sample_prompt = \"This is a good day.\"\n",
    "    input_ids = torch.tensor(tokenizer.encode(sample_prompt))\n",
    "    greenlist_ids = wm_processor._get_greenlist_ids(input_ids)\n",
    "    print(\"Greenlist token IDs for the prompt:\")\n",
    "    print(greenlist_ids)\n",
    "    green_tokens = [vocab_list[tok] for tok in greenlist_ids]\n",
    "    print(\"Greenlist tokens (strings):\")\n",
    "    print(green_tokens)'''\n",
    "\n",
    "\n",
    "'''# coding=utf-8\n",
    "# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n",
    "# available at https://arxiv.org/abs/2301.10226\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import annotations\n",
    "import collections\n",
    "from math import sqrt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import LogitsProcessor\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from normalizers import normalization_strategy_lookup\n",
    "\n",
    "\n",
    "class WatermarkBase:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: list[int] = None,\n",
    "        gamma: float = 0.5,\n",
    "        delta: float = 2.0,\n",
    "        seeding_scheme: str = \"simple_1\",  # mostly unused/always default\n",
    "        hash_key: int = 15485863,  # just a large prime number to create a rng seed with sufficient bit width\n",
    "        select_green_tokens: bool = True,\n",
    "    ):\n",
    "\n",
    "        # watermarking parameters\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.seeding_scheme = seeding_scheme\n",
    "        self.rng = None\n",
    "        self.hash_key = hash_key\n",
    "        self.select_green_tokens = select_green_tokens\n",
    "\n",
    "    def _seed_rng(self, input_ids: torch.LongTensor, seeding_scheme: str = None) -> None:\n",
    "        # can optionally override the seeding scheme,\n",
    "        # but uses the instance attr by default\n",
    "        if seeding_scheme is None:\n",
    "            seeding_scheme = self.seeding_scheme\n",
    "\n",
    "        if seeding_scheme == \"simple_1\":\n",
    "            assert input_ids.shape[-1] >= 1, f\"seeding_scheme={seeding_scheme} requires at least a 1 token prefix sequence to seed rng\"\n",
    "            prev_token = input_ids[-1].item()\n",
    "            self.rng.manual_seed(self.hash_key * prev_token)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unexpected seeding_scheme: {seeding_scheme}\")\n",
    "        return\n",
    "\n",
    "    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> list[int]:\n",
    "        # seed the rng using the previous tokens/prefix\n",
    "        # according to the seeding_scheme\n",
    "        self._seed_rng(input_ids)\n",
    "\n",
    "        greenlist_size = int(self.vocab_size * self.gamma)\n",
    "        vocab_permutation = torch.randperm(self.vocab_size, device=input_ids.device, generator=self.rng)\n",
    "        if self.select_green_tokens:  # directly\n",
    "            greenlist_ids = vocab_permutation[:greenlist_size]  # new\n",
    "        else:  # select green via red\n",
    "            greenlist_ids = vocab_permutation[(self.vocab_size - greenlist_size) :]  # legacy behavior\n",
    "        return greenlist_ids\n",
    "\n",
    "\n",
    "class WatermarkLogitsProcessor(WatermarkBase, LogitsProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _calc_greenlist_mask(self, scores: torch.FloatTensor, greenlist_token_ids) -> torch.BoolTensor:\n",
    "        # TODO lets see if we can lose this loop\n",
    "        green_tokens_mask = torch.zeros_like(scores)\n",
    "        for b_idx in range(len(greenlist_token_ids)):\n",
    "            green_tokens_mask[b_idx][greenlist_token_ids[b_idx]] = 1\n",
    "        final_mask = green_tokens_mask.bool()\n",
    "        return final_mask\n",
    "\n",
    "    def _bias_greenlist_logits(self, scores: torch.Tensor, greenlist_mask: torch.Tensor, greenlist_bias: float) -> torch.Tensor:\n",
    "        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\n",
    "        return scores\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "\n",
    "        # this is lazy to allow us to colocate on the watermarked model's device\n",
    "        if self.rng is None:\n",
    "            self.rng = torch.Generator(device=input_ids.device)\n",
    "\n",
    "        # NOTE, it would be nice to get rid of this batch loop, but currently,\n",
    "        # the seed and partition operations are not tensor/vectorized, thus\n",
    "        # each sequence in the batch needs to be treated separately.\n",
    "        batched_greenlist_ids = [None for _ in range(input_ids.shape[0])]\n",
    "\n",
    "        for b_idx in range(input_ids.shape[0]):\n",
    "            greenlist_ids = self._get_greenlist_ids(input_ids[b_idx])\n",
    "            batched_greenlist_ids[b_idx] = greenlist_ids\n",
    "\n",
    "        green_tokens_mask = self._calc_greenlist_mask(scores=scores, greenlist_token_ids=batched_greenlist_ids)\n",
    "\n",
    "        scores = self._bias_greenlist_logits(scores=scores, greenlist_mask=green_tokens_mask, greenlist_bias=self.delta)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class WatermarkDetector(WatermarkBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        device: torch.device = None,\n",
    "        tokenizer: Tokenizer = None,\n",
    "        z_threshold: float = 4.0,\n",
    "        normalizers: list[str] = [\"unicode\"],  # or also: [\"unicode\", \"homoglyphs\", \"truecase\"]\n",
    "        ignore_repeated_bigrams: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # also configure the metrics returned/preprocessing options\n",
    "        assert device, \"Must pass device\"\n",
    "        assert tokenizer, \"Need an instance of the generating tokenizer to perform detection\"\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.z_threshold = z_threshold\n",
    "        self.rng = torch.Generator(device=self.device)\n",
    "\n",
    "        if self.seeding_scheme == \"simple_1\":\n",
    "            self.min_prefix_len = 1\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unexpected seeding_scheme: {self.seeding_scheme}\")\n",
    "\n",
    "        self.normalizers = []\n",
    "        for normalization_strategy in normalizers:\n",
    "            self.normalizers.append(normalization_strategy_lookup(normalization_strategy))\n",
    "\n",
    "        self.ignore_repeated_bigrams = ignore_repeated_bigrams\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            assert self.seeding_scheme == \"simple_1\", \"No repeated bigram credit variant assumes the single token seeding scheme.\"\n",
    "\n",
    "    def _compute_z_score(self, observed_count, T):\n",
    "        # count refers to number of green tokens, T is total number of tokens\n",
    "        expected_count = self.gamma\n",
    "        numer = observed_count - expected_count * T\n",
    "        denom = sqrt(T * expected_count * (1 - expected_count))\n",
    "        z = numer / denom\n",
    "        return z\n",
    "\n",
    "    def _compute_p_value(self, z):\n",
    "        p_value = scipy.stats.norm.sf(z)\n",
    "        return p_value\n",
    "\n",
    "    def _score_sequence(\n",
    "        self,\n",
    "        input_ids: Tensor,\n",
    "        return_num_tokens_scored: bool = True,\n",
    "        return_num_green_tokens: bool = True,\n",
    "        return_green_fraction: bool = True,\n",
    "        return_green_token_mask: bool = False,\n",
    "        return_z_score: bool = True,\n",
    "        return_p_value: bool = True,\n",
    "    ):\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            # Method that only counts a green/red hit once per unique bigram.\n",
    "            # New num total tokens scored (T) becomes the number unique bigrams.\n",
    "            # We iterate over all unqiue token bigrams in the input, computing the greenlist\n",
    "            # induced by the first token in each, and then checking whether the second\n",
    "            # token falls in that greenlist.\n",
    "            assert return_green_token_mask is False, \"Can't return the green/red mask when ignoring repeats.\"\n",
    "            bigram_table = {}\n",
    "            token_bigram_generator = ngrams(input_ids.cpu().tolist(), 2)\n",
    "            freq = collections.Counter(token_bigram_generator)\n",
    "            num_tokens_scored = len(freq.keys())\n",
    "            for idx, bigram in enumerate(freq.keys()):\n",
    "                prefix = torch.tensor([bigram[0]], device=self.device)  # expects a 1-d prefix tensor on the randperm device\n",
    "                greenlist_ids = self._get_greenlist_ids(prefix)\n",
    "                bigram_table[bigram] = True if bigram[1] in greenlist_ids else False\n",
    "            green_token_count = sum(bigram_table.values())\n",
    "        else:\n",
    "            num_tokens_scored = len(input_ids) - self.min_prefix_len\n",
    "            if num_tokens_scored < 1:\n",
    "                raise ValueError(\n",
    "                    (\n",
    "                        f\"Must have at least {1} token to score after \"\n",
    "                        f\"the first min_prefix_len={self.min_prefix_len} tokens required by the seeding scheme.\"\n",
    "                    )\n",
    "                )\n",
    "            # Standard method.\n",
    "            # Since we generally need at least 1 token (for the simplest scheme)\n",
    "            # we start the iteration over the token sequence with a minimum\n",
    "            # num tokens as the first prefix for the seeding scheme,\n",
    "            # and at each step, compute the greenlist induced by the\n",
    "            # current prefix and check if the current token falls in the greenlist.\n",
    "            green_token_count, green_token_mask = 0, []\n",
    "            for idx in range(self.min_prefix_len, len(input_ids)):\n",
    "                curr_token = input_ids[idx]\n",
    "                greenlist_ids = self._get_greenlist_ids(input_ids[:idx])\n",
    "                if curr_token in greenlist_ids:\n",
    "                    green_token_count += 1\n",
    "                    green_token_mask.append(True)\n",
    "                else:\n",
    "                    green_token_mask.append(False)\n",
    "\n",
    "        score_dict = dict()\n",
    "        if return_num_tokens_scored:\n",
    "            score_dict.update(dict(num_tokens_scored=num_tokens_scored))\n",
    "        if return_num_green_tokens:\n",
    "            score_dict.update(dict(num_green_tokens=green_token_count))\n",
    "        if return_green_fraction:\n",
    "            score_dict.update(dict(green_fraction=(green_token_count / num_tokens_scored)))\n",
    "        if return_z_score:\n",
    "            score_dict.update(dict(z_score=self._compute_z_score(green_token_count, num_tokens_scored)))\n",
    "        if return_p_value:\n",
    "            z_score = score_dict.get(\"z_score\")\n",
    "            if z_score is None:\n",
    "                z_score = self._compute_z_score(green_token_count, num_tokens_scored)\n",
    "            score_dict.update(dict(p_value=self._compute_p_value(z_score)))\n",
    "        if return_green_token_mask:\n",
    "            score_dict.update(dict(green_token_mask=green_token_mask))\n",
    "\n",
    "        return score_dict\n",
    "\n",
    "    def detect(\n",
    "        self,\n",
    "        text: str = None,\n",
    "        tokenized_text: list[int] = None,\n",
    "        return_prediction: bool = True,\n",
    "        return_scores: bool = True,\n",
    "        z_threshold: float = None,\n",
    "        **kwargs,\n",
    "    ) -> dict:\n",
    "\n",
    "        assert (text is not None) ^ (tokenized_text is not None), \"Must pass either the raw or tokenized string\"\n",
    "        if return_prediction:\n",
    "            kwargs[\"return_p_value\"] = True  # to return the \"confidence\":=1-p of positive detections\n",
    "\n",
    "        # run optional normalizers on text\n",
    "        for normalizer in self.normalizers:\n",
    "            text = normalizer(text)\n",
    "        if len(self.normalizers) > 0:\n",
    "            print(f\"Text after normalization:\\n\\n{text}\\n\")\n",
    "\n",
    "        if tokenized_text is None:\n",
    "            assert self.tokenizer is not None, (\n",
    "                \"Watermark detection on raw string \",\n",
    "                \"requires an instance of the tokenizer \",\n",
    "                \"that was used at generation time.\",\n",
    "            )\n",
    "            tokenized_text = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].to(self.device)\n",
    "            if tokenized_text[0] == self.tokenizer.bos_token_id:\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "        else:\n",
    "            # try to remove the bos_tok at beginning if it's there\n",
    "            if (self.tokenizer is not None) and (tokenized_text[0] == self.tokenizer.bos_token_id):\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "\n",
    "        # call score method\n",
    "        output_dict = {}\n",
    "        score_dict = self._score_sequence(tokenized_text, **kwargs)\n",
    "        if return_scores:\n",
    "            output_dict.update(score_dict)\n",
    "        # if passed return_prediction then perform the hypothesis test and return the outcome\n",
    "        if return_prediction:\n",
    "            z_threshold = z_threshold if z_threshold else self.z_threshold\n",
    "            assert z_threshold is not None, \"Need a threshold in order to decide outcome of detection test\"\n",
    "            output_dict[\"prediction\"] = score_dict[\"z_score\"] > z_threshold\n",
    "            if output_dict[\"prediction\"]:\n",
    "                output_dict[\"confidence\"] = 1 - score_dict[\"p_value\"]\n",
    "\n",
    "        return output_dict'''\n"
   ],
   "id": "4e3af3cce42012bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# coding=utf-8\\n# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\\n# available at https://arxiv.org/abs/2301.10226\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nfrom __future__ import annotations\\nimport collections\\nfrom math import sqrt\\n\\nimport scipy.stats\\n\\nimport torch\\nfrom torch import Tensor\\nfrom tokenizers import Tokenizer\\nfrom transformers import LogitsProcessor\\n\\nfrom nltk.util import ngrams\\n\\nfrom normalizers import normalization_strategy_lookup\\n\\n\\nclass WatermarkBase:\\n    def __init__(\\n        self,\\n        vocab: list[int] = None,\\n        gamma: float = 0.5,\\n        delta: float = 2.0,\\n        seeding_scheme: str = \"simple_1\",  # mostly unused/always default\\n        hash_key: int = 15485863,  # just a large prime number to create a rng seed with sufficient bit width\\n        select_green_tokens: bool = True,\\n    ):\\n\\n        # watermarking parameters\\n        self.vocab = vocab\\n        self.vocab_size = len(vocab)\\n        self.gamma = gamma\\n        self.delta = delta\\n        self.seeding_scheme = seeding_scheme\\n        self.rng = None\\n        self.hash_key = hash_key\\n        self.select_green_tokens = select_green_tokens\\n\\n    def _seed_rng(self, input_ids: torch.LongTensor, seeding_scheme: str = None) -> None:\\n        # can optionally override the seeding scheme,\\n        # but uses the instance attr by default\\n        if seeding_scheme is None:\\n            seeding_scheme = self.seeding_scheme\\n\\n        if seeding_scheme == \"simple_1\":\\n            assert input_ids.shape[-1] >= 1, f\"seeding_scheme={seeding_scheme} requires at least a 1 token prefix sequence to seed rng\"\\n            prev_token = input_ids[-1].item()\\n            self.rng.manual_seed(self.hash_key * prev_token)\\n        else:\\n            raise NotImplementedError(f\"Unexpected seeding_scheme: {seeding_scheme}\")\\n        return\\n\\n    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> list[int]:\\n        # seed the rng using the previous tokens/prefix\\n        # according to the seeding_scheme\\n        self._seed_rng(input_ids)\\n\\n        greenlist_size = int(self.vocab_size * self.gamma)\\n        vocab_permutation = torch.randperm(self.vocab_size, device=input_ids.device, generator=self.rng)\\n        if self.select_green_tokens:  # directly\\n            greenlist_ids = vocab_permutation[:greenlist_size]  # new\\n        else:  # select green via red\\n            greenlist_ids = vocab_permutation[(self.vocab_size - greenlist_size) :]  # legacy behavior\\n        return greenlist_ids\\n\\n\\nclass WatermarkLogitsProcessor(WatermarkBase, LogitsProcessor):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n    def _calc_greenlist_mask(self, scores: torch.FloatTensor, greenlist_token_ids) -> torch.BoolTensor:\\n        # TODO lets see if we can lose this loop\\n        green_tokens_mask = torch.zeros_like(scores)\\n        for b_idx in range(len(greenlist_token_ids)):\\n            green_tokens_mask[b_idx][greenlist_token_ids[b_idx]] = 1\\n        final_mask = green_tokens_mask.bool()\\n        return final_mask\\n\\n    def _bias_greenlist_logits(self, scores: torch.Tensor, greenlist_mask: torch.Tensor, greenlist_bias: float) -> torch.Tensor:\\n        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\\n        return scores\\n\\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\\n\\n        # this is lazy to allow us to colocate on the watermarked model\\'s device\\n        if self.rng is None:\\n            self.rng = torch.Generator(device=input_ids.device)\\n\\n        # NOTE, it would be nice to get rid of this batch loop, but currently,\\n        # the seed and partition operations are not tensor/vectorized, thus\\n        # each sequence in the batch needs to be treated separately.\\n        batched_greenlist_ids = [None for _ in range(input_ids.shape[0])]\\n\\n        for b_idx in range(input_ids.shape[0]):\\n            greenlist_ids = self._get_greenlist_ids(input_ids[b_idx])\\n            batched_greenlist_ids[b_idx] = greenlist_ids\\n\\n        green_tokens_mask = self._calc_greenlist_mask(scores=scores, greenlist_token_ids=batched_greenlist_ids)\\n\\n        scores = self._bias_greenlist_logits(scores=scores, greenlist_mask=green_tokens_mask, greenlist_bias=self.delta)\\n        return scores\\n\\n\\nclass WatermarkDetector(WatermarkBase):\\n    def __init__(\\n        self,\\n        *args,\\n        device: torch.device = None,\\n        tokenizer: Tokenizer = None,\\n        z_threshold: float = 4.0,\\n        normalizers: list[str] = [\"unicode\"],  # or also: [\"unicode\", \"homoglyphs\", \"truecase\"]\\n        ignore_repeated_bigrams: bool = True,\\n        **kwargs,\\n    ):\\n        super().__init__(*args, **kwargs)\\n        # also configure the metrics returned/preprocessing options\\n        assert device, \"Must pass device\"\\n        assert tokenizer, \"Need an instance of the generating tokenizer to perform detection\"\\n\\n        self.tokenizer = tokenizer\\n        self.device = device\\n        self.z_threshold = z_threshold\\n        self.rng = torch.Generator(device=self.device)\\n\\n        if self.seeding_scheme == \"simple_1\":\\n            self.min_prefix_len = 1\\n        else:\\n            raise NotImplementedError(f\"Unexpected seeding_scheme: {self.seeding_scheme}\")\\n\\n        self.normalizers = []\\n        for normalization_strategy in normalizers:\\n            self.normalizers.append(normalization_strategy_lookup(normalization_strategy))\\n\\n        self.ignore_repeated_bigrams = ignore_repeated_bigrams\\n        if self.ignore_repeated_bigrams:\\n            assert self.seeding_scheme == \"simple_1\", \"No repeated bigram credit variant assumes the single token seeding scheme.\"\\n\\n    def _compute_z_score(self, observed_count, T):\\n        # count refers to number of green tokens, T is total number of tokens\\n        expected_count = self.gamma\\n        numer = observed_count - expected_count * T\\n        denom = sqrt(T * expected_count * (1 - expected_count))\\n        z = numer / denom\\n        return z\\n\\n    def _compute_p_value(self, z):\\n        p_value = scipy.stats.norm.sf(z)\\n        return p_value\\n\\n    def _score_sequence(\\n        self,\\n        input_ids: Tensor,\\n        return_num_tokens_scored: bool = True,\\n        return_num_green_tokens: bool = True,\\n        return_green_fraction: bool = True,\\n        return_green_token_mask: bool = False,\\n        return_z_score: bool = True,\\n        return_p_value: bool = True,\\n    ):\\n        if self.ignore_repeated_bigrams:\\n            # Method that only counts a green/red hit once per unique bigram.\\n            # New num total tokens scored (T) becomes the number unique bigrams.\\n            # We iterate over all unqiue token bigrams in the input, computing the greenlist\\n            # induced by the first token in each, and then checking whether the second\\n            # token falls in that greenlist.\\n            assert return_green_token_mask is False, \"Can\\'t return the green/red mask when ignoring repeats.\"\\n            bigram_table = {}\\n            token_bigram_generator = ngrams(input_ids.cpu().tolist(), 2)\\n            freq = collections.Counter(token_bigram_generator)\\n            num_tokens_scored = len(freq.keys())\\n            for idx, bigram in enumerate(freq.keys()):\\n                prefix = torch.tensor([bigram[0]], device=self.device)  # expects a 1-d prefix tensor on the randperm device\\n                greenlist_ids = self._get_greenlist_ids(prefix)\\n                bigram_table[bigram] = True if bigram[1] in greenlist_ids else False\\n            green_token_count = sum(bigram_table.values())\\n        else:\\n            num_tokens_scored = len(input_ids) - self.min_prefix_len\\n            if num_tokens_scored < 1:\\n                raise ValueError(\\n                    (\\n                        f\"Must have at least {1} token to score after \"\\n                        f\"the first min_prefix_len={self.min_prefix_len} tokens required by the seeding scheme.\"\\n                    )\\n                )\\n            # Standard method.\\n            # Since we generally need at least 1 token (for the simplest scheme)\\n            # we start the iteration over the token sequence with a minimum\\n            # num tokens as the first prefix for the seeding scheme,\\n            # and at each step, compute the greenlist induced by the\\n            # current prefix and check if the current token falls in the greenlist.\\n            green_token_count, green_token_mask = 0, []\\n            for idx in range(self.min_prefix_len, len(input_ids)):\\n                curr_token = input_ids[idx]\\n                greenlist_ids = self._get_greenlist_ids(input_ids[:idx])\\n                if curr_token in greenlist_ids:\\n                    green_token_count += 1\\n                    green_token_mask.append(True)\\n                else:\\n                    green_token_mask.append(False)\\n\\n        score_dict = dict()\\n        if return_num_tokens_scored:\\n            score_dict.update(dict(num_tokens_scored=num_tokens_scored))\\n        if return_num_green_tokens:\\n            score_dict.update(dict(num_green_tokens=green_token_count))\\n        if return_green_fraction:\\n            score_dict.update(dict(green_fraction=(green_token_count / num_tokens_scored)))\\n        if return_z_score:\\n            score_dict.update(dict(z_score=self._compute_z_score(green_token_count, num_tokens_scored)))\\n        if return_p_value:\\n            z_score = score_dict.get(\"z_score\")\\n            if z_score is None:\\n                z_score = self._compute_z_score(green_token_count, num_tokens_scored)\\n            score_dict.update(dict(p_value=self._compute_p_value(z_score)))\\n        if return_green_token_mask:\\n            score_dict.update(dict(green_token_mask=green_token_mask))\\n\\n        return score_dict\\n\\n    def detect(\\n        self,\\n        text: str = None,\\n        tokenized_text: list[int] = None,\\n        return_prediction: bool = True,\\n        return_scores: bool = True,\\n        z_threshold: float = None,\\n        **kwargs,\\n    ) -> dict:\\n\\n        assert (text is not None) ^ (tokenized_text is not None), \"Must pass either the raw or tokenized string\"\\n        if return_prediction:\\n            kwargs[\"return_p_value\"] = True  # to return the \"confidence\":=1-p of positive detections\\n\\n        # run optional normalizers on text\\n        for normalizer in self.normalizers:\\n            text = normalizer(text)\\n        if len(self.normalizers) > 0:\\n            print(f\"Text after normalization:\\n\\n{text}\\n\")\\n\\n        if tokenized_text is None:\\n            assert self.tokenizer is not None, (\\n                \"Watermark detection on raw string \",\\n                \"requires an instance of the tokenizer \",\\n                \"that was used at generation time.\",\\n            )\\n            tokenized_text = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].to(self.device)\\n            if tokenized_text[0] == self.tokenizer.bos_token_id:\\n                tokenized_text = tokenized_text[1:]\\n        else:\\n            # try to remove the bos_tok at beginning if it\\'s there\\n            if (self.tokenizer is not None) and (tokenized_text[0] == self.tokenizer.bos_token_id):\\n                tokenized_text = tokenized_text[1:]\\n\\n        # call score method\\n        output_dict = {}\\n        score_dict = self._score_sequence(tokenized_text, **kwargs)\\n        if return_scores:\\n            output_dict.update(score_dict)\\n        # if passed return_prediction then perform the hypothesis test and return the outcome\\n        if return_prediction:\\n            z_threshold = z_threshold if z_threshold else self.z_threshold\\n            assert z_threshold is not None, \"Need a threshold in order to decide outcome of detection test\"\\n            output_dict[\"prediction\"] = score_dict[\"z_score\"] > z_threshold\\n            if output_dict[\"prediction\"]:\\n                output_dict[\"confidence\"] = 1 - score_dict[\"p_value\"]\\n\\n        return output_dict'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
