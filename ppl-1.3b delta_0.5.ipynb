{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:05:04.847716Z",
     "start_time": "2025-03-23T06:05:04.808134Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n",
    "# available at https://arxiv.org/abs/2301.10226\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.stats as stats\n",
    "from datasets import load_dataset\n",
    "import collections\n",
    "from math import sqrt\n",
    "import scipy.stats\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import nltk\n",
    "import ssl\n",
    "from nltk.util import ngrams\n",
    "from transformers import LogitsProcessor\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from nltk.corpus import wordnet as wn\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import networkx as nx\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#########################\n",
    "# Helper Functions\n",
    "#########################\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"A demo for watermarking using a revised green/red partition based on WordNet synonyms.\"\n",
    "    )\n",
    "    parser.add_argument(\"--demo_public\", type=str2bool, default=False,\n",
    "                        help=\"Expose the gradio demo publicly.\")\n",
    "    parser.add_argument(\"--model_name_or_path\", type=str, default=\"facebook/opt-1.3b\",\n",
    "                        help=\"Identifier for the pretrained model from Hugging Face.\")\n",
    "    parser.add_argument(\"--prompt_max_length\", type=int, default=None,\n",
    "                        help=\"Truncation length for the prompt.\")\n",
    "    parser.add_argument(\"--max_new_tokens\", type=int, default=200,\n",
    "                        help=\"Maximum number of tokens to generate.\")\n",
    "    parser.add_argument(\"--generation_seed\", type=int, default=123,\n",
    "                        help=\"Seed for generation reproducibility.\")\n",
    "    parser.add_argument(\"--use_sampling\", type=str2bool, default=True,\n",
    "                        help=\"Use multinomial sampling for generation.\")\n",
    "    parser.add_argument(\"--sampling_temp\", type=float, default=0.7,\n",
    "                        help=\"Sampling temperature.\")\n",
    "    parser.add_argument(\"--n_beams\", type=int, default=1,\n",
    "                        help=\"Number of beams for beam search (if not sampling).\")\n",
    "    parser.add_argument(\"--use_gpu\", type=str2bool, default=True,\n",
    "                        help=\"Run inference on GPU if available.\")\n",
    "    parser.add_argument(\"--seeding_scheme\", type=str, default=\"simple_1\",\n",
    "                        help=\"Seeding scheme for watermarking.\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.25,\n",
    "                        help=\"Target fraction of tokens for the green list.\")\n",
    "    parser.add_argument(\"--delta\", type=float, default=2.0,\n",
    "                        help=\"Bias to add to green list token logits.\")\n",
    "    parser.add_argument(\"--normalizers\", type=str, default=\"\",\n",
    "                        help=\"Comma separated normalizer names for detection.\")\n",
    "    parser.add_argument(\"--ignore_repeated_bigrams\", type=str2bool, default=False,\n",
    "                        help=\"Use repeated bigram variant in detection.\")\n",
    "    parser.add_argument(\"--detection_z_threshold\", type=float, default=4.0,\n",
    "                        help=\"Z-score threshold for detection.\")\n",
    "    parser.add_argument(\"--select_green_tokens\", type=str2bool, default=True,\n",
    "                        help=\"Legacy option for selecting green tokens.\")\n",
    "    parser.add_argument(\"--skip_model_load\", type=str2bool, default=False,\n",
    "                        help=\"Skip model loading (for debugging).\")\n",
    "    parser.add_argument(\"--seed_separately\", type=str2bool, default=True,\n",
    "                        help=\"Seed separately for each generation call.\")\n",
    "    parser.add_argument(\"--load_fp16\", type=str2bool, default=False,\n",
    "                        help=\"Load model in FP16 mode.\")\n",
    "    args = parser.parse_args()\n",
    "    # Convert normalizers into a list (if provided)\n",
    "    args.normalizers = args.normalizers.split(\",\") if args.normalizers else []\n",
    "    return args\n",
    "\n",
    "\n",
    "#############################\n",
    "# Preprocessing: Vocabulary & Matching\n",
    "#############################\n",
    "\n",
    "def get_vocabulary(tokenizer) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of tokens (where index corresponds to token ID)\n",
    "    extracted from the tokenizer's vocabulary.\n",
    "    \"\"\"\n",
    "    vocab_dict = tokenizer.get_vocab()\n",
    "    vocab_list = [None] * len(vocab_dict)\n",
    "    for token, idx in vocab_dict.items():\n",
    "        vocab_list[idx] = token\n",
    "    return vocab_list\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_lemma_set(token: str) -> set:\n",
    "    \"\"\"\n",
    "    Given a token string, first strip any leading BPE marker (e.g. \"Ġ\")\n",
    "    and return the set of lowercased lemma names from all its WordNet synsets.\n",
    "    \"\"\"\n",
    "    # Strip off common prefix markers\n",
    "    word = token.lstrip(\"Ġ\")\n",
    "    synsets = wn.synsets(word)\n",
    "    return {lemma.lower() for s in synsets for lemma in s.lemma_names()}\n",
    "\n",
    "\n",
    "def are_synonyms(token1: str, token2: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determines whether two tokens are synonyms by checking if token1 (after stripping)\n",
    "    appears in token2's lemma set and vice versa.\n",
    "    \"\"\"\n",
    "    word1 = token1.lstrip(\"Ġ\")\n",
    "    word2 = token2.lstrip(\"Ġ\")\n",
    "    lemmas1 = get_lemma_set(word1)\n",
    "    lemmas2 = get_lemma_set(word2)\n",
    "    if not lemmas1 or not lemmas2:\n",
    "        return False\n",
    "    return (word1.lower() in lemmas2) and (word2.lower() in lemmas1)\n",
    "\n",
    "\n",
    "def filter_tokens_with_synonyms(vocab_list: list[str]) -> (list[int], list[int]):\n",
    "    \"\"\"\n",
    "    Splits the vocabulary indices into:\n",
    "      - unique_indices: indices of tokens that have no synonym in the vocabulary.\n",
    "      - paired_indices: indices of tokens that have at least one synonym.\n",
    "    Uses a progress bar via tqdm.\n",
    "    \"\"\"\n",
    "    unique_indices = []\n",
    "    paired_indices = []\n",
    "    n = len(vocab_list)\n",
    "    # Precompute lemma sets for each token (stripped of any leading marker)\n",
    "    lemma_sets = [get_lemma_set(token) for token in vocab_list]\n",
    "    for i in tqdm(range(n), desc=\"Filtering vocabulary\"):\n",
    "        token_i = vocab_list[i]\n",
    "        lemmas_i = lemma_sets[i]\n",
    "        has_synonym = False\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if (token_i.lstrip(\"Ġ\")).lower() in lemma_sets[j] and (vocab_list[j].lstrip(\"Ġ\")).lower() in lemmas_i:\n",
    "                has_synonym = True\n",
    "                break\n",
    "        if has_synonym:\n",
    "            paired_indices.append(i)\n",
    "        else:\n",
    "            unique_indices.append(i)\n",
    "    return unique_indices, paired_indices\n",
    "\n",
    "\n",
    "def construct_similarity_matrix(vocab_list: list[str], indices: list[int]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Constructs an m x m similarity matrix for tokens specified by indices.\n",
    "    Entry [i][j] is 1.0 if the tokens are synonyms, 0 otherwise.\n",
    "    Uses nested loops with tqdm progress bar.\n",
    "    \"\"\"\n",
    "    m = len(indices)\n",
    "    C = [[0.0 for _ in range(m)] for _ in range(m)]\n",
    "    # Precompute lemma sets for tokens in indices\n",
    "    lemma_dict = {i: get_lemma_set(vocab_list[i].lstrip(\"Ġ\")) for i in indices}\n",
    "    for a in tqdm(range(m), desc=\"Constructing similarity matrix (outer loop)\"):\n",
    "        for b in range(a + 1, m):\n",
    "            token_a = vocab_list[indices[a]].lstrip(\"Ġ\")\n",
    "            token_b = vocab_list[indices[b]].lstrip(\"Ġ\")\n",
    "            lemmas_a = lemma_dict[indices[a]]\n",
    "            lemmas_b = lemma_dict[indices[b]]\n",
    "            weight = 1.0 if (token_a.lower() in lemmas_b and token_b.lower() in lemmas_a) else 0.0\n",
    "            C[a][b] = weight\n",
    "            C[b][a] = weight\n",
    "    return C\n",
    "\n",
    "\n",
    "def find_perfect_matching(similarity_matrix: list[list[float]]) -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Constructs an undirected graph from the similarity matrix (only edges with weight>0)\n",
    "    and returns a maximum–weight matching (as a list of index pairs relative to the input list).\n",
    "    Uses tqdm over the pairs.\n",
    "    \"\"\"\n",
    "    m = len(similarity_matrix)\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(m))\n",
    "    pairs = list(itertools.combinations(range(m), 2))\n",
    "    for i, j in tqdm(pairs, total=len(pairs), desc=\"Building graph for matching\"):\n",
    "        weight = similarity_matrix[i][j]\n",
    "        if weight > 0:\n",
    "            G.add_edge(i, j, weight=weight)\n",
    "    matching = nx.max_weight_matching(G, maxcardinality=True)\n",
    "    pairing = [tuple(sorted(pair)) for pair in matching]\n",
    "    return pairing\n",
    "\n",
    "\n",
    "#############################\n",
    "# Revised Watermark Processor Classes\n",
    "#############################\n",
    "\n",
    "class WatermarkBase:\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab: list[int] = None,\n",
    "            gamma: float = 0.25,\n",
    "            delta: float = 2.0,\n",
    "            seeding_scheme: str = \"simple_1\",\n",
    "            hash_key: int = 15485863,\n",
    "            select_green_tokens: bool = True,\n",
    "            precomputed_pairing: list[tuple[int, int]] = None,\n",
    "            unique_tokens: list[int] = None,\n",
    "    ):\n",
    "        self.vocab = vocab  # list of token IDs (usually 0, ..., n-1)\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.gamma = gamma  # target fraction of tokens for the green list\n",
    "        self.delta = delta  # bias added to green token logits\n",
    "        self.seeding_scheme = seeding_scheme\n",
    "        self.rng = None\n",
    "        self.hash_key = hash_key\n",
    "        self.select_green_tokens = select_green_tokens\n",
    "        self.pairing = precomputed_pairing  # perfect matching (pairs) for tokens with synonyms\n",
    "        self.unique_tokens = unique_tokens  # token IDs that have no synonyms\n",
    "\n",
    "    #debug: replace input.device to device in _seed_rng, _get_greenlist_ids\n",
    "    def _seed_rng(self, input_ids: torch.LongTensor, seeding_scheme: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Seeds the RNG deterministically using the last token in input_ids.\n",
    "        For the \"simple_1\" scheme, the seed is hash_key * (last token id).\n",
    "        \"\"\"\n",
    "        if seeding_scheme is None:\n",
    "            seeding_scheme = self.seeding_scheme\n",
    "        if self.rng is None:\n",
    "            self.rng = torch.Generator(device=device)\n",
    "        if seeding_scheme == \"simple_1\":\n",
    "            assert input_ids.shape[-1] >= 1, \"Input must have at least one token.\"\n",
    "            prev_token = input_ids[-1].item()\n",
    "            self.rng.manual_seed(self.hash_key * prev_token)\n",
    "            #print(self.hash_key * prev_token)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Seeding scheme {seeding_scheme} not implemented.\")\n",
    "        return\n",
    "\n",
    "    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> list[int]:\n",
    "        \"\"\"\n",
    "        Returns the green list token IDs.\n",
    "        If precomputed pairing and unique_tokens are provided, then:\n",
    "          - All unique tokens (set A) are in the green list.\n",
    "          - For each pair in the perfect matching, one token is chosen by a coin flip.\n",
    "        Otherwise, falls back to a random permutation method.\n",
    "        The final list is optionally truncated to a target size (gamma * vocab_size).\n",
    "        \"\"\"\n",
    "        self._seed_rng(input_ids)\n",
    "        \n",
    "        if self.pairing is None or self.unique_tokens is None:\n",
    "            # Fallback: use random permutation.\n",
    "            greenlist_size = int(self.vocab_size * self.gamma)\n",
    "            vocab_permutation = torch.randperm(self.vocab_size, device=device, generator=self.rng)\n",
    "            if self.select_green_tokens:\n",
    "                return vocab_permutation[:greenlist_size].tolist()\n",
    "            else:\n",
    "                return vocab_permutation[-greenlist_size:].tolist()\n",
    "        else:\n",
    "            greenlist_ids = self.unique_tokens.copy()\n",
    "            for pair in self.pairing:\n",
    "                coin_flip = (torch.rand(1, generator=self.rng, device=device).item() < 0.5)\n",
    "                chosen = pair[0] if coin_flip else pair[1]\n",
    "                greenlist_ids.append(chosen)\n",
    "            # desired_size = int(self.vocab_size * self.gamma)\n",
    "            # if len(greenlist_ids) > desired_size:\n",
    "            #    perm = torch.randperm(len(greenlist_ids), generator=self.rng).tolist()\n",
    "            #    indices = perm[:desired_size]\n",
    "            #    greenlist_ids = [greenlist_ids[i] for i in indices]\n",
    "            return greenlist_ids\n",
    "\n",
    "\n",
    "class WatermarkLogitsProcessor(WatermarkBase, LogitsProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _calc_greenlist_mask(self, scores: torch.FloatTensor, greenlist_token_ids) -> torch.BoolTensor:\n",
    "        green_tokens_mask = torch.zeros_like(scores)\n",
    "        for b_idx in range(len(greenlist_token_ids)):\n",
    "            green_tokens_mask[b_idx][greenlist_token_ids[b_idx]] = 1\n",
    "        return green_tokens_mask.bool()\n",
    "\n",
    "    def _bias_greenlist_logits(self, scores: torch.Tensor, greenlist_mask: torch.Tensor,\n",
    "                               greenlist_bias: float) -> torch.Tensor:\n",
    "        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\n",
    "        return scores\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        if self.rng is None:\n",
    "            self.rng = torch.Generator(device=input_ids.device)\n",
    "        batched_greenlist_ids = []\n",
    "        for b_idx in range(input_ids.shape[0]):\n",
    "            greenlist_ids = self._get_greenlist_ids(input_ids[b_idx])\n",
    "            batched_greenlist_ids.append(greenlist_ids)\n",
    "        green_tokens_mask = self._calc_greenlist_mask(scores, batched_greenlist_ids)\n",
    "        scores = self._bias_greenlist_logits(scores, green_tokens_mask, self.delta)\n",
    "        #print(torch.max(scores))\n",
    "        return scores\n",
    "\n",
    "\n",
    "class WatermarkDetector(WatermarkBase):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            device: torch.device = None,\n",
    "            tokenizer=None,\n",
    "            z_threshold: float = 4.0,\n",
    "            normalizers: list[str] = [\"unicode\"],\n",
    "            ignore_repeated_bigrams: bool = True,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert device, \"Device must be provided.\"\n",
    "        assert tokenizer, \"A tokenizer is required for detection.\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.z_threshold = z_threshold\n",
    "        self.rng = torch.Generator(device=self.device)\n",
    "        if self.seeding_scheme == \"simple_1\":\n",
    "            self.min_prefix_len = 1\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Seeding scheme {self.seeding_scheme} not implemented.\")\n",
    "        self.normalizers = [normalization_strategy_lookup(norm) for norm in normalizers]\n",
    "        self.ignore_repeated_bigrams = ignore_repeated_bigrams\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            assert self.seeding_scheme == \"simple_1\", \"Repeated bigram variant requires simple_1 seeding.\"\n",
    "\n",
    "    def _compute_z_score(self, observed_count, T):\n",
    "        expected_count = self.gamma\n",
    "        numer = observed_count - expected_count * T\n",
    "        denom = sqrt(T * expected_count * (1 - expected_count))\n",
    "        return numer / denom\n",
    "\n",
    "    def _compute_p_value(self, z):\n",
    "        return scipy.stats.norm.sf(z)\n",
    "\n",
    "    def _score_sequence(\n",
    "            self,\n",
    "            input_ids: Tensor,\n",
    "            return_num_tokens_scored: bool = True,\n",
    "            return_num_green_tokens: bool = True,\n",
    "            return_green_fraction: bool = True,\n",
    "            return_green_token_mask: bool = False,\n",
    "            return_z_score: bool = True,\n",
    "            return_p_value: bool = True,\n",
    "    ):\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            # Repeated bigram variant: T = number of unique bigrams.\n",
    "            bigram_table = {}\n",
    "            token_bigram_generator = ngrams(input_ids.cpu().tolist(), 2)\n",
    "            freq = collections.Counter(token_bigram_generator)\n",
    "            num_tokens_scored = len(freq.keys())\n",
    "            for bigram in freq.keys():\n",
    "                prefix = torch.tensor([bigram[0]], device=self.device)\n",
    "                greenlist_ids = self._get_greenlist_ids(prefix)\n",
    "                bigram_table[bigram] = bigram[1] in greenlist_ids\n",
    "            green_token_count = sum(bigram_table.values())\n",
    "        else:\n",
    "            # Standard variant: T = total tokens (after min_prefix_len)\n",
    "            num_tokens_scored = len(input_ids) - self.min_prefix_len\n",
    "            if num_tokens_scored < 1:\n",
    "                raise ValueError(\"Not enough tokens to score.\")\n",
    "            green_token_count = 0\n",
    "            green_token_mask = []\n",
    "            for idx in range(self.min_prefix_len, len(input_ids)):\n",
    "                curr_token = input_ids[idx]\n",
    "                greenlist_ids = self._get_greenlist_ids(input_ids[:idx])\n",
    "                if curr_token in greenlist_ids:\n",
    "                    green_token_count += 1\n",
    "                    green_token_mask.append(True)\n",
    "                else:\n",
    "                    green_token_mask.append(False)\n",
    "        # Debug prints:\n",
    "        print(f\"Total tokens scored (T): {num_tokens_scored}\")\n",
    "        print(f\"Green token count: {green_token_count}\")\n",
    "        print(f\"Green fraction: {green_token_count / num_tokens_scored:.2%}\")\n",
    "\n",
    "        score_dict = {}\n",
    "        if return_num_tokens_scored:\n",
    "            score_dict[\"num_tokens_scored\"] = num_tokens_scored\n",
    "        if return_num_green_tokens:\n",
    "            score_dict[\"num_green_tokens\"] = green_token_count\n",
    "        if return_green_fraction:\n",
    "            score_dict[\"green_fraction\"] = green_token_count / num_tokens_scored\n",
    "        if return_z_score:\n",
    "            score_dict[\"z_score\"] = self._compute_z_score(green_token_count, num_tokens_scored)\n",
    "        if return_p_value:\n",
    "            z = score_dict.get(\"z_score\", self._compute_z_score(green_token_count, num_tokens_scored))\n",
    "            score_dict[\"p_value\"] = self._compute_p_value(z)\n",
    "        if return_green_token_mask:\n",
    "            score_dict[\"green_token_mask\"] = green_token_mask\n",
    "        return score_dict\n",
    "\n",
    "    def detect(\n",
    "            self,\n",
    "            text: str = None,\n",
    "            tokenized_text: list[int] = None,\n",
    "            return_prediction: bool = True,\n",
    "            return_scores: bool = True,\n",
    "            z_threshold: float = None,\n",
    "            **kwargs,\n",
    "    ) -> dict:\n",
    "        assert (text is not None) ^ (tokenized_text is not None), \"Provide either raw or tokenized text.\"\n",
    "        if return_prediction:\n",
    "            kwargs[\"return_p_value\"] = True\n",
    "        for normalizer in self.normalizers:\n",
    "            text = normalizer(text)\n",
    "        if tokenized_text is None:\n",
    "            tokenized_text = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].to(\n",
    "                self.device)\n",
    "            if tokenized_text[0] == self.tokenizer.bos_token_id:\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "        else:\n",
    "            if self.tokenizer is not None and tokenized_text[0] == self.tokenizer.bos_token_id:\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "        output_dict = {}\n",
    "        score_dict = self._score_sequence(tokenized_text, **kwargs)\n",
    "        if return_scores:\n",
    "            output_dict.update(score_dict)\n",
    "        if return_prediction:\n",
    "            z_threshold = z_threshold if z_threshold is not None else self.z_threshold\n",
    "            output_dict[\"prediction\"] = score_dict[\"z_score\"] > z_threshold\n",
    "            if output_dict[\"prediction\"]:\n",
    "                output_dict[\"confidence\"] = 1 - score_dict[\"p_value\"]\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "#############################\n",
    "# Demo Code (Generation & Detection)\n",
    "#############################\n",
    "\n",
    "def load_model(args):\n",
    "    # Set model type attributes on args.\n",
    "    args.is_seq2seq_model = any(model_type in args.model_name_or_path for model_type in [\"t5\", \"T0\"])\n",
    "    args.is_decoder_only_model = any(model_type in args.model_name_or_path for model_type in [\"gpt\", \"opt\", \"bloom\"])\n",
    "\n",
    "    if args.is_seq2seq_model:\n",
    "        from transformers import AutoModelForSeq2SeqLM\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n",
    "    elif args.is_decoder_only_model:\n",
    "        if args.load_fp16:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, torch_dtype=torch.float16,\n",
    "                                                         device_map='auto')\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {args.model_name_or_path}\")\n",
    "\n",
    "    if args.use_gpu:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if not args.load_fp16:\n",
    "            model = model.to(device)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def generate(prompt, args, model=None, device=None, tokenizer=None):\n",
    "    # print(f\"Generating with {args}\")\n",
    "    # Set model type attributes on args.\n",
    "    args.is_seq2seq_model = any(model_type in args.model_name_or_path for model_type in [\"t5\", \"T0\"])\n",
    "    args.is_decoder_only_model = any(model_type in args.model_name_or_path for model_type in [\"gpt\", \"opt\", \"bloom\"])\n",
    "\n",
    "    # Instantiate the watermark processor with precomputed pairing/unique tokens.\n",
    "    # Assume that in main() we precomputed these values (see below).\n",
    "    watermark_processor = WatermarkLogitsProcessor(\n",
    "        vocab=list(tokenizer.get_vocab().values()),\n",
    "        gamma=args.gamma,\n",
    "        delta=args.delta,\n",
    "        seeding_scheme=args.seeding_scheme,\n",
    "        select_green_tokens=args.select_green_tokens,\n",
    "        precomputed_pairing=args.precomputed_pairing,\n",
    "        unique_tokens=args.unique_tokens\n",
    "    )\n",
    "\n",
    "    gen_kwargs = dict(max_new_tokens=args.max_new_tokens)\n",
    "    if args.use_sampling:\n",
    "        gen_kwargs.update(dict(\n",
    "            do_sample=True,\n",
    "            top_k=0,\n",
    "            temperature=args.sampling_temp\n",
    "        ))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(\n",
    "            num_beams=args.n_beams\n",
    "        ))\n",
    "\n",
    "    generate_without_watermark = partial(\n",
    "        model.generate,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    generate_with_watermark = partial(\n",
    "        model.generate,\n",
    "        logits_processor=LogitsProcessorList([watermark_processor]),\n",
    "        **gen_kwargs\n",
    "    )\n",
    "\n",
    "    if not args.prompt_max_length:\n",
    "        if hasattr(model.config, \"max_position_embedding\"):\n",
    "            args.prompt_max_length = model.config.max_position_embeddings - args.max_new_tokens\n",
    "        else:\n",
    "            args.prompt_max_length = 2048 - args.max_new_tokens\n",
    "\n",
    "    tokd_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True, truncation=True,\n",
    "                           max_length=args.prompt_max_length).to(device)\n",
    "    truncation_warning = tokd_input[\"input_ids\"].shape[-1] == args.prompt_max_length\n",
    "    redecoded_input = tokenizer.batch_decode(tokd_input[\"input_ids\"], skip_special_tokens=True)[0]\n",
    "\n",
    "    torch.manual_seed(args.generation_seed)\n",
    "    output_without_watermark = generate_without_watermark(**tokd_input)\n",
    "    if args.seed_separately:\n",
    "        torch.manual_seed(args.generation_seed)\n",
    "    output_with_watermark = generate_with_watermark(**tokd_input)\n",
    "\n",
    "    if args.is_decoder_only_model:\n",
    "        output_without_watermark = output_without_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "        output_with_watermark = output_with_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    decoded_output_without_watermark = tokenizer.batch_decode(output_without_watermark, skip_special_tokens=True)[0]\n",
    "    decoded_output_with_watermark = tokenizer.batch_decode(output_with_watermark, skip_special_tokens=True)[0]\n",
    "\n",
    "    return (\n",
    "        redecoded_input, int(truncation_warning), decoded_output_without_watermark, decoded_output_with_watermark, args)\n",
    "\n",
    "\n",
    "def format_names(s):\n",
    "    s = s.replace(\"num_tokens_scored\", \"Tokens Counted (T)\")\n",
    "    s = s.replace(\"num_green_tokens\", \"# Tokens in Greenlist\")\n",
    "    s = s.replace(\"green_fraction\", \"Fraction of T in Greenlist\")\n",
    "    s = s.replace(\"z_score\", \"z-score\")\n",
    "    s = s.replace(\"p_value\", \"p value\")\n",
    "    s = s.replace(\"prediction\", \"Prediction\")\n",
    "    s = s.replace(\"confidence\", \"Confidence\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def list_format_scores(score_dict, detection_threshold):\n",
    "    lst_2d = []\n",
    "    for k, v in score_dict.items():\n",
    "        if k == 'green_fraction':\n",
    "            lst_2d.append([format_names(k), f\"{v:.1%}\"])\n",
    "        elif k == 'confidence':\n",
    "            lst_2d.append([format_names(k), f\"{v:.3%}\"])\n",
    "        elif isinstance(v, float):\n",
    "            lst_2d.append([format_names(k), f\"{v:.3g}\"])\n",
    "        elif isinstance(v, bool):\n",
    "            lst_2d.append([format_names(k), (\"Watermarked\" if v else \"Human/Unwatermarked\")])\n",
    "        else:\n",
    "            lst_2d.append([format_names(k), f\"{v}\"])\n",
    "    if \"confidence\" in score_dict:\n",
    "        lst_2d.insert(-2, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    else:\n",
    "        lst_2d.insert(-1, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    return lst_2d\n",
    "\n",
    "\n",
    "def detect(input_text, args, device=None, tokenizer=None):\n",
    "    watermark_detector = WatermarkDetector(\n",
    "        vocab=list(tokenizer.get_vocab().values()),\n",
    "        gamma=args.gamma,\n",
    "        seeding_scheme=args.seeding_scheme,\n",
    "        device=device,\n",
    "        tokenizer=tokenizer,\n",
    "        z_threshold=args.detection_z_threshold,\n",
    "        normalizers=args.normalizers,\n",
    "        ignore_repeated_bigrams=args.ignore_repeated_bigrams,\n",
    "        select_green_tokens=args.select_green_tokens\n",
    "    )\n",
    "    if len(input_text) - 1 > watermark_detector.min_prefix_len:\n",
    "        score_dict = watermark_detector.detect(input_text)\n",
    "        output = list_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "    else:\n",
    "        output = [[\"Error\", \"string too short to compute metrics\"]]\n",
    "        output += [[\"\", \"\"] for _ in range(6)]\n",
    "    return output, args\n",
    "\n",
    "\n",
    "def run_gradio(args, model=None, device=None, tokenizer=None):\n",
    "    generate_partial = partial(generate, model=model, device=device, tokenizer=tokenizer)\n",
    "    detect_partial = partial(detect, device=device, tokenizer=tokenizer)\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"Gradio demo not shown in command-line mode.\")\n",
    "        demo.launch()\n",
    "\n",
    "\n",
    "def compute_p_value(green_count: int, tokens_scored: int) -> (float, float):\n",
    "    \"\"\"\n",
    "    Given the number of paired tokens that appear in the green list (green_count)\n",
    "    out of tokens_scored (only paired tokens are scored), compute a z–score and p–value.\n",
    "    Under the null hypothesis, each paired token is green with probability 0.5.\n",
    "    \"\"\"\n",
    "    import math\n",
    "    expected = 0.5 * tokens_scored\n",
    "    std = math.sqrt(0.25 * tokens_scored)\n",
    "    z = (green_count - expected) / std if std > 0 else 0.0\n",
    "    p = scipy.stats.norm.sf(z)\n",
    "    return z, p\n",
    "\n",
    "\n",
    "def compute_perplexity(model, tokenizer, text: str, device):\n",
    "    \"\"\"\n",
    "    Computes perplexity of the text using the provided model and tokenizer.\n",
    "    Here we use the language-modeling loss computed by the model.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)[\"input_ids\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity\n",
    "\n",
    "# === Cos similarity and greedy matching ===\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def construct_similarity_matrix_cos(vocab_list: list[str], indices: list[int], embedding_matrix: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Constructs an m x m similarity matrix for tokens specified by indices,\n",
    "    using cosine similarity between token embeddings.\n",
    "\n",
    "    Args:\n",
    "      vocab_list: List of all tokens (strings).\n",
    "      indices: List of token indices corresponding to the non-unique tokens (set B).\n",
    "      embedding_matrix: A torch.Tensor of shape (vocab_size, hidden_dim) representing the token embeddings.\n",
    "\n",
    "    Returns:\n",
    "      A torch.Tensor of shape (m, m) where each entry [i][j] is the cosine similarity between\n",
    "      the embeddings of vocab_list[indices[i]] and vocab_list[indices[j]], with the diagonal entries set to 0.\n",
    "    \"\"\"\n",
    "    # Extract embeddings for the selected tokens (non-unique tokens).\n",
    "    selected_embeddings = embedding_matrix[indices]  # shape: (m, hidden_dim)\n",
    "\n",
    "    # Normalize the embeddings along the feature dimension.\n",
    "    norm_embeddings = F.normalize(selected_embeddings, p=2, dim=1)\n",
    "\n",
    "    # Compute the cosine similarity matrix as the dot product between normalized embeddings.\n",
    "    sim_matrix = torch.mm(norm_embeddings, norm_embeddings.t())\n",
    "\n",
    "    # Set the diagonal entries to 0 (to ignore self-similarity).\n",
    "    sim_matrix.fill_diagonal_(0)\n",
    "\n",
    "    return sim_matrix\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "def find_perfect_matching_greedy_random(similarity_matrix: list[list[float]]) -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Constructs a greedy matching from the similarity matrix using random sampling.\n",
    "    In each iteration, a random token i is selected from the unmatched set.\n",
    "    Then, approximately ceil(log2(n)) tokens (where n is the current number of unmatched tokens)\n",
    "    are randomly sampled from the remaining tokens, and the token j with the highest similarity\n",
    "    (i.e. highest value in similarity_matrix[i][j]) is selected as a match.\n",
    "\n",
    "    The function returns a list of tuples (i, j) (with i < j) representing the matched token indices.\n",
    "    Note: The similarity matrix should have 0 on its diagonal.\n",
    "    \"\"\"\n",
    "    m = len(similarity_matrix)\n",
    "    unmatched = list(range(m))\n",
    "    matching = []\n",
    "\n",
    "    pbar = tqdm(total=len(unmatched)//2, desc=\"Greedy random matching\")\n",
    "    while len(unmatched) > 1:\n",
    "        n = len(unmatched)\n",
    "        # Set sample size to ceil(log2(n)); ensure at least one candidate.\n",
    "        sample_size = math.ceil(math.log(n, 2)) if n > 1 else 1\n",
    "        # Randomly choose one token i from the unmatched set.\n",
    "        i = random.choice(unmatched)\n",
    "        # Build a list of candidates (all unmatched tokens except i).\n",
    "        remaining = [x for x in unmatched if x != i]\n",
    "        # Adjust sample_size if there are fewer candidates than sample_size.\n",
    "        sample_size = min(sample_size, len(remaining))\n",
    "        # Randomly sample sample_size candidates.\n",
    "        candidates = random.sample(remaining, sample_size)\n",
    "        # Find the candidate j with maximum similarity with i.\n",
    "        best_j = candidates[0]\n",
    "        best_weight = similarity_matrix[i][best_j]\n",
    "        for j in candidates:\n",
    "            w = similarity_matrix[i][j]\n",
    "            if w > best_weight:\n",
    "                best_weight = w\n",
    "                best_j = j\n",
    "        # Add the pair (min(i, best_j), max(i, best_j)) for consistency.\n",
    "        matching.append((min(i, best_j), max(i, best_j)))\n",
    "        # Remove both tokens from the unmatched set.\n",
    "        unmatched.remove(i)\n",
    "        if best_j in unmatched:\n",
    "            unmatched.remove(best_j)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return matching\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# --- Helper: Check if both outputs meet length condition (≥195 tokens) ---\n",
    "def valid_length(wm_text, nw_text, tokenizer, min_tokens=195):\n",
    "    len_wm = len(tokenizer(wm_text)[\"input_ids\"])\n",
    "    len_nw = len(tokenizer(nw_text)[\"input_ids\"])\n",
    "    return (len_wm >= min_tokens) and (len_nw >= min_tokens)\n",
    "\n",
    "# ---- Main Evaluation Function ----\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc  # Instead of roc_auc_score\n",
    "\n",
    "def evaluate_watermarking(truncated_texts, model, tokenizer, args, args_cos=None):\n",
    "    \"\"\"\n",
    "    For each prompt in truncated_texts, generate completions using three methods:\n",
    "      1. Default watermark method (our method)\n",
    "      2. KGW method (via generate_kgw and detect_kgw)\n",
    "      3. Cosine–similarity based method (via generate with args_cos and detect_cos)\n",
    "\n",
    "    For each method, compute:\n",
    "      - Number of tokens generated\n",
    "      - Detection metrics (only on paired tokens): green token count, tokens scored, green fraction,\n",
    "        z–score and p–value, judgement (watermarked vs. non-watermarked), perplexity.\n",
    "\n",
    "    Only prompts where all outputs (for all three methods) have length ≥195 tokens are considered.\n",
    "    We then compute aggregated metrics (average perplexity) and an ROC–AUC by computing\n",
    "    the false-positive and true-positive rates (via roc_curve) and passing them to auc().\n",
    "\n",
    "    Returns:\n",
    "      results: a list of per–prompt result dictionaries.\n",
    "      aggregated: a dictionary with aggregated metrics for each method.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Accumulators for default method:\n",
    "    wm_z_default_acc, nw_z_default_acc, labels_default = [], [], []\n",
    "    ppl_wm_default_acc, ppl_nw_default_acc = [], []\n",
    "    green_counts_w_default_acc, tokens_scored_w_default_acc, props_w_default_acc = [], [], []\n",
    "\n",
    "    # For KGW method:\n",
    "    wm_z_kgw_acc, nw_z_kgw_acc, labels_kgw = [], [], []\n",
    "    ppl_wm_kgw_acc, ppl_nw_kgw_acc = [], []\n",
    "    green_counts_w_kgw_acc, tokens_scored_w_kgw_acc, props_w_kgw_acc = [], [], []\n",
    "\n",
    "    # For Cosine-based method:\n",
    "    wm_z_cos_acc, nw_z_cos_acc, labels_cos = [], [], []\n",
    "    ppl_wm_cos_acc, ppl_nw_cos_acc = [], []\n",
    "    green_counts_w_cos_acc, tokens_scored_w_cos_acc, props_w_cos_acc = [], [], []\n",
    "\n",
    "    for prompt in tqdm(truncated_texts, desc=\"Evaluating prompts\"):\n",
    "        # --- Default Watermark Generation ---\n",
    "        redecoded_input, truncation_warning, decoded_nw, decoded_wm, _ = generate(\n",
    "            prompt, args, model=model, device=device, tokenizer=tokenizer\n",
    "        )\n",
    "        #print('generate finish')\n",
    "        \n",
    "        wm_processor_default = WatermarkLogitsProcessor(\n",
    "            vocab=list(tokenizer.get_vocab().values()),\n",
    "            gamma=args.gamma,\n",
    "            delta=args.delta,\n",
    "            seeding_scheme=args.seeding_scheme,\n",
    "            select_green_tokens=args.select_green_tokens,\n",
    "            precomputed_pairing=args.precomputed_pairing,\n",
    "            unique_tokens=args.unique_tokens\n",
    "        )\n",
    "\n",
    "        # Count green tokens (only among paired tokens) for watermarked text:\n",
    "        green_count_w, tokens_scored_w, prop_w = count_green_tokens_paired(\n",
    "            tokenizer, wm_processor_default, decoded_wm\n",
    "        )\n",
    "        z_default, p_default = compute_p_value(green_count_w, tokens_scored_w)\n",
    "\n",
    "        # Non-watermarked text:\n",
    "        green_count_nw, tokens_scored_nw, prop_nw = count_green_tokens_paired(\n",
    "            tokenizer, wm_processor_default, decoded_nw\n",
    "        )\n",
    "        z_nw_default, p_nw_default = compute_p_value(green_count_nw, tokens_scored_nw)\n",
    "\n",
    "        judgement_default = (\n",
    "            \"LLM-generated (watermarked)\" if z_default > args.detection_z_threshold else \"Human-generated (non-watermarked)\"\n",
    "        )\n",
    "        perplexity_wm = compute_perplexity(model, tokenizer, decoded_wm, device)\n",
    "        perplexity_nw = compute_perplexity(model, tokenizer, decoded_nw, device)\n",
    "        tokens_generated_default = len(tokenizer(decoded_wm)[\"input_ids\"])\n",
    "\n",
    "        #print('detect finish')\n",
    "\n",
    "        # --- KGW Method ---\n",
    "        redecoded_input_kgw, truncation_warning_kgw, decoded_nw_kgw, decoded_wm_kgw, _ = generate_kgw(\n",
    "            prompt, args, model=model, device=device, tokenizer=tokenizer\n",
    "        )\n",
    "        #print('generate finish')\n",
    "        detect_result_w_kgw = detect_kgw(decoded_wm_kgw, args, device=device, tokenizer=tokenizer)[1]\n",
    "        if len(decoded_wm_kgw) < 195:\n",
    "            continue\n",
    "        z_kgw = detect_result_w_kgw[\"z_score\"]\n",
    "        green_count_w_kgw = detect_result_w_kgw[\"num_green_tokens\"]\n",
    "        tokens_scored_w_kgw_local = detect_result_w_kgw[\"num_tokens_scored\"]\n",
    "        prop_w_kgw = detect_result_w_kgw[\"green_fraction\"]\n",
    "\n",
    "        detect_result_nw_kgw = detect_kgw(decoded_nw_kgw, args, device=device, tokenizer=tokenizer)[1]\n",
    "        print(decoded_nw_kgw)\n",
    "        if len(decoded_nw_kgw) < 195:\n",
    "            continue\n",
    "        z_nw_kgw = detect_result_nw_kgw[\"z_score\"]\n",
    "        green_count_nw_kgw = detect_result_nw_kgw[\"num_green_tokens\"]\n",
    "\n",
    "        judgement_kgw = (\n",
    "            \"LLM-generated (watermarked)\" if z_kgw > args.detection_z_threshold else \"Human-generated (non-watermarked)\"\n",
    "        )\n",
    "        perplexity_wm_kgw = compute_perplexity(model, tokenizer, decoded_wm_kgw, device)\n",
    "        perplexity_nw_kgw = compute_perplexity(model, tokenizer, decoded_nw_kgw, device)\n",
    "        tokens_generated_kgw = len(tokenizer(decoded_wm_kgw)[\"input_ids\"])\n",
    "\n",
    "        #print('detect finish')\n",
    "        # --- Cosine-based Method ---\n",
    "        redecoded_input_cos, truncation_warning_cos, decoded_nw_cos, decoded_wm_cos, _ = generate(\n",
    "            prompt, args_cos, model=model, device=device, tokenizer=tokenizer\n",
    "        )\n",
    "        wm_processor_cos = WatermarkLogitsProcessor(\n",
    "            vocab=list(tokenizer.get_vocab().values()),\n",
    "            gamma=args_cos.gamma,\n",
    "            delta=args_cos.delta,\n",
    "            seeding_scheme=args_cos.seeding_scheme,\n",
    "            select_green_tokens=args_cos.select_green_tokens,\n",
    "            precomputed_pairing=args_cos.precomputed_pairing,\n",
    "            unique_tokens=args_cos.unique_tokens\n",
    "        )\n",
    "        green_count_w_cos, tokens_scored_w_cos, prop_w_cos = count_green_tokens_paired(\n",
    "            tokenizer, wm_processor_cos, decoded_wm_cos\n",
    "        )\n",
    "        z_cos, p_cos = compute_p_value(green_count_w_cos, tokens_scored_w_cos)\n",
    "\n",
    "        green_count_nw_cos, tokens_scored_nw_cos, prop_nw_cos = count_green_tokens_paired(\n",
    "            tokenizer, wm_processor_cos, decoded_nw_cos\n",
    "        )\n",
    "        z_nw_cos, p_nw_cos = compute_p_value(green_count_nw_cos, tokens_scored_nw_cos)\n",
    "        judgement_cos = (\n",
    "            \"LLM-generated (watermarked)\" if z_cos > args_cos.detection_z_threshold else \"Human-generated (non-watermarked)\"\n",
    "        )\n",
    "        perplexity_wm_cos = compute_perplexity(model, tokenizer, decoded_wm_cos, device)\n",
    "        perplexity_nw_cos = compute_perplexity(model, tokenizer, decoded_nw_cos, device)\n",
    "        tokens_generated_cos = len(tokenizer(decoded_wm_cos)[\"input_ids\"])\n",
    "\n",
    "        # Store per-prompt results\n",
    "        result = {\n",
    "            \"prompt\": redecoded_input,\n",
    "            \"default\": {\n",
    "                \"decoded_wm\": decoded_wm,\n",
    "                \"decoded_nw\": decoded_nw,\n",
    "                \"green_count_w\": green_count_w,\n",
    "                \"green_count_nw\": green_count_nw,\n",
    "                \"tokens_scored_w\": tokens_scored_w,\n",
    "                \"tokens_scored_nw\": tokens_scored_nw,\n",
    "                \"prop_w\": prop_w,\n",
    "                \"z_w\": z_default,\n",
    "                \"z_nw\": z_nw_default,\n",
    "                \"p_w\": p_default,\n",
    "                \"judgement\": judgement_default,\n",
    "                \"ppl_wm\": perplexity_wm,\n",
    "                \"ppl_nw\": perplexity_nw,\n",
    "                \"tokens_generated\": tokens_generated_default\n",
    "            },\n",
    "            \"kgw\": {\n",
    "                \"decoded_wm\": decoded_wm_kgw,\n",
    "                \"decoded_nw\": decoded_nw_kgw,\n",
    "                \"green_count_w\": green_count_w_kgw,\n",
    "                \"tokens_scored_w\": tokens_scored_w_kgw_local,\n",
    "                \"prop_w\": prop_w_kgw,\n",
    "                \"z_w\": z_kgw,\n",
    "                \"p_w\": detect_result_w_kgw.get(\"p_value\", None),\n",
    "                \"judgement\": judgement_kgw,\n",
    "                \"ppl_wm\": perplexity_wm_kgw,\n",
    "                \"ppl_nw\": perplexity_nw_kgw,\n",
    "                \"tokens_generated\": tokens_generated_kgw\n",
    "            },\n",
    "            \"cos\": {\n",
    "                \"decoded_wm\": decoded_wm_cos,\n",
    "                \"decoded_nw\": decoded_nw_cos,\n",
    "                \"green_count_w\": green_count_w_cos,\n",
    "                \"tokens_scored_w\": tokens_scored_w_cos,\n",
    "                \"prop_w\": prop_w_cos,\n",
    "                \"z_w\": z_cos,\n",
    "                \"p_w\": p_cos,\n",
    "                \"judgement\": judgement_cos,\n",
    "                \"ppl_wm\": perplexity_wm_cos,\n",
    "                \"ppl_nw\": perplexity_nw_cos,\n",
    "                \"tokens_generated\": tokens_generated_cos\n",
    "            }\n",
    "        }\n",
    "        results.append(result)\n",
    "        # Optional prompt-level printing:\n",
    "        print(result)\n",
    "\n",
    "        # Only consider prompts where all three methods produce outputs of length ≥ 195 tokens.\n",
    "        if (valid_length(decoded_wm, decoded_nw, tokenizer) and\n",
    "            valid_length(decoded_wm_kgw, decoded_nw_kgw, tokenizer) and\n",
    "            valid_length(decoded_wm_cos, decoded_nw_cos, tokenizer)):\n",
    "\n",
    "            # Accumulate Default method metrics.\n",
    "            wm_z_default_acc.append(z_default)\n",
    "            nw_z_default_acc.append(z_nw_default)\n",
    "            labels_default = [1] * len(wm_z_default_acc) + [0] * len(nw_z_default_acc)\n",
    "            ppl_wm_default_acc.append(perplexity_wm)\n",
    "            ppl_nw_default_acc.append(perplexity_nw)\n",
    "            green_counts_w_default_acc.append(green_count_w)\n",
    "            tokens_scored_w_default_acc.append(tokens_scored_w)\n",
    "\n",
    "            # KGW method accumulators.\n",
    "            wm_z_kgw_acc.append(z_kgw)\n",
    "            nw_z_kgw_acc.append(z_nw_kgw)\n",
    "            labels_kgw = [1] * len(wm_z_kgw_acc) + [0] * len(nw_z_kgw_acc)\n",
    "            ppl_wm_kgw_acc.append(perplexity_wm_kgw)\n",
    "            ppl_nw_kgw_acc.append(perplexity_nw_kgw)\n",
    "            green_counts_w_kgw_acc.append(green_count_w_kgw)\n",
    "            tokens_scored_w_kgw_acc.append(tokens_scored_w_kgw_local)\n",
    "\n",
    "            # Cosine-based method accumulators.\n",
    "            wm_z_cos_acc.append(z_cos)\n",
    "            nw_z_cos_acc.append(z_nw_cos)\n",
    "            labels_cos = [1] * len(wm_z_cos_acc) + [0] * len(nw_z_cos_acc)\n",
    "            ppl_wm_cos_acc.append(perplexity_wm_cos)\n",
    "            ppl_nw_cos_acc.append(perplexity_nw_cos)\n",
    "            green_counts_w_cos_acc.append(green_count_w_cos)\n",
    "            tokens_scored_w_cos_acc.append(tokens_scored_w_cos)\n",
    "\n",
    "            num_valid = len(ppl_wm_default_acc)\n",
    "            curr_avg_ppl_wm_default = np.mean(ppl_wm_default_acc)\n",
    "            curr_avg_ppl_nw_default = np.mean(ppl_nw_default_acc)\n",
    "            curr_avg_ppl_wm_kgw = np.mean(ppl_wm_kgw_acc)\n",
    "            curr_avg_ppl_nw_kgw = np.mean(ppl_nw_kgw_acc)\n",
    "            curr_avg_ppl_wm_cos = np.mean(ppl_wm_cos_acc)\n",
    "            curr_avg_ppl_nw_cos = np.mean(ppl_nw_cos_acc)\n",
    "            curr_avg_z_wm_default = np.mean(wm_z_default_acc)\n",
    "            curr_avg_z_nw_default = np.mean(nw_z_default_acc)\n",
    "            curr_avg_z_wm_kgw = np.mean(wm_z_kgw_acc)\n",
    "            curr_avg_z_nw_kgw = np.mean(nw_z_kgw_acc)\n",
    "            curr_avg_z_wm_cos = np.mean(wm_z_cos_acc)\n",
    "            curr_avg_z_nw_cos = np.mean(nw_z_cos_acc)\n",
    "\n",
    "            # -- Use roc_curve() and auc() here instead of roc_auc_score() --\n",
    "            try:\n",
    "                # For default method:\n",
    "                all_scores_default = wm_z_default_acc + nw_z_default_acc\n",
    "                fpr_def, tpr_def, _ = roc_curve(labels_default, all_scores_default)\n",
    "                curr_auc_default = auc(fpr_def, tpr_def)\n",
    "                #print(all_scores_default)\n",
    "                #print(wm_z_default_acc)\n",
    "                #print(nw_z_default_acc)\n",
    "            except Exception:\n",
    "                curr_auc_default = float('nan')\n",
    "\n",
    "            try:\n",
    "                # For KGW method:\n",
    "                all_scores_kgw = wm_z_kgw_acc + nw_z_kgw_acc\n",
    "                fpr_kgw, tpr_kgw, _ = roc_curve(labels_kgw, all_scores_kgw)\n",
    "                curr_auc_kgw = auc(fpr_kgw, tpr_kgw)\n",
    "                #print(all_scores_kgw)\n",
    "                #print(wm_z_kgw_acc)\n",
    "                #print(nw_z_kgw_acc)\n",
    "            except Exception:\n",
    "                curr_auc_kgw = float('nan')\n",
    "\n",
    "            try:\n",
    "                # For Cosine-based method:\n",
    "                all_scores_cos = wm_z_cos_acc + nw_z_cos_acc\n",
    "                fpr_cos, tpr_cos, _ = roc_curve(labels_cos, all_scores_cos)\n",
    "                curr_auc_cos = auc(fpr_cos, tpr_cos)\n",
    "                #print(all_scores_cos)\n",
    "                #print(wm_z_cos_acc)\n",
    "                #print(nw_z_cos_acc)\n",
    "            except Exception:\n",
    "                curr_auc_cos = float('nan')\n",
    "\n",
    "            print(f\"\\nAfter {num_valid} valid prompts:\")\n",
    "            print(\" Default Method:    avg ppl (wm) = {0:.2f}, avg ppl (nw) = {1:.2f}, AUC = {2:.3f}, avg z (wm) = {3:.2f}, avg z (nw) = {4:.2f}\".format(\n",
    "                curr_avg_ppl_wm_default, curr_avg_ppl_nw_default, curr_auc_default, curr_avg_z_wm_default, curr_avg_z_nw_default))\n",
    "            print(\" KGW Method:        avg ppl (wm) = {0:.2f}, avg ppl (nw) = {1:.2f}, AUC = {2:.3f}, avg z (wm) = {3:.2f}, avg z (nw) = {4:.2f}\".format(\n",
    "                curr_avg_ppl_wm_kgw, curr_avg_ppl_nw_kgw, curr_auc_kgw, curr_avg_z_wm_kgw, curr_avg_z_nw_kgw))\n",
    "            print(\" Cosine-based Method: avg ppl (wm) = {0:.2f}, avg ppl (nw) = {1:.2f}, AUC = {2:.3f}, avg z (wm) = {3:.2f}, avg z (nw) = {4:.2f}\\n\".format(\n",
    "                curr_avg_ppl_wm_cos, curr_avg_ppl_nw_cos, curr_auc_cos, curr_avg_z_wm_cos, curr_avg_z_nw_cos))\n",
    "\n",
    "    # Final aggregated metrics\n",
    "    aggregated = {}\n",
    "\n",
    "    # Summaries for each method:\n",
    "    def finalize_metrics(labels, wm_z_list, nw_z_list, ppl_wm_list, ppl_nw_list, method_name):\n",
    "        out_dict = {}\n",
    "        if len(ppl_wm_list) > 0:\n",
    "            out_dict[\"avg_ppl_wm\"] = np.mean(ppl_wm_list)\n",
    "            out_dict[\"avg_ppl_nw\"] = np.mean(ppl_nw_list)\n",
    "            out_dict[\"avg_z_wm\"] = np.mean(wm_z_list)\n",
    "            out_dict[\"avg_z_nw\"] = np.mean(nw_z_list)\n",
    "            out_dict[\"num_valid\"] = len(ppl_wm_list)\n",
    "        else:\n",
    "            out_dict[\"avg_ppl_wm\"] = None\n",
    "            out_dict[\"avg_ppl_nw\"] = None\n",
    "            out_dict[\"avg_z_wm\"] = None\n",
    "            out_dict[\"avg_z_nw\"] = None\n",
    "            out_dict[\"num_valid\"] = 0\n",
    "\n",
    "        if wm_z_list and nw_z_list:\n",
    "            all_scores = wm_z_list + nw_z_list\n",
    "            try:\n",
    "                fpr, tpr, _ = roc_curve(labels, all_scores)\n",
    "                out_dict[\"auc\"] = auc(fpr, tpr)\n",
    "            except Exception:\n",
    "                out_dict[\"auc\"] = None\n",
    "        else:\n",
    "            out_dict[\"auc\"] = None\n",
    "\n",
    "        aggregated[method_name] = out_dict\n",
    "\n",
    "    # Default\n",
    "    finalize_metrics(labels_default,\n",
    "                     wm_z_default_acc, nw_z_default_acc,\n",
    "                     ppl_wm_default_acc, ppl_nw_default_acc,\n",
    "                     \"default\")\n",
    "\n",
    "    # KGW\n",
    "    finalize_metrics(labels_kgw,\n",
    "                     wm_z_kgw_acc, nw_z_kgw_acc,\n",
    "                     ppl_wm_kgw_acc, ppl_nw_kgw_acc,\n",
    "                     \"kgw\")\n",
    "\n",
    "    # Cosine\n",
    "    finalize_metrics(labels_cos,\n",
    "                     wm_z_cos_acc, nw_z_cos_acc,\n",
    "                     ppl_wm_cos_acc, ppl_nw_cos_acc,\n",
    "                     \"cos\")\n",
    "\n",
    "    return results, aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8e6f4c18e4bb0122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T05:37:44.431237Z",
     "start_time": "2025-03-23T05:37:44.428294Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# === Helper functions for detection metrics ===\n",
    "\n",
    "def count_green_tokens_paired(tokenizer, watermark_processor, text: str) -> (int, int, float):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text and for each token (after a minimum prefix),\n",
    "    computes the green list based on the current prefix (using watermark_processor).\n",
    "    Only paired tokens (i.e. tokens that have a synonym pair) are counted (unique tokens are excluded).\n",
    "\n",
    "    Returns:\n",
    "      green_count: number of paired tokens that appear in the green list.\n",
    "      tokens_scored: number of tokens scored (only tokens that are paired).\n",
    "      proportion: green_count / tokens_scored.\n",
    "    \"\"\"\n",
    "    # Tokenize without special tokens\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    total_tokens = len(input_ids)\n",
    "    # Use a minimum prefix length; if not defined in the processor, default to 1.\n",
    "    start_idx = getattr(watermark_processor, \"min_prefix_len\", 1)\n",
    "    green_count = 0\n",
    "    tokens_scored = 0\n",
    "\n",
    "    # For each token position starting at start_idx, consider only if the token is a paired token.\n",
    "    for idx in range(start_idx, total_tokens):\n",
    "        token = input_ids[idx].item()\n",
    "        greenlist_ids = watermark_processor._get_greenlist_ids(input_ids[:idx])\n",
    "        #print(token)\n",
    "        # Only count if token is NOT in the unique set (i.e. token is paired)\n",
    "        if watermark_processor.unique_tokens is not None and token not in watermark_processor.unique_tokens:\n",
    "            tokens_scored += 1\n",
    "            #print(token in greenlist_ids)\n",
    "            if token in greenlist_ids:\n",
    "                green_count += 1\n",
    "                \n",
    "    proportion = green_count / tokens_scored if tokens_scored > 0 else 0.0\n",
    "    return green_count, tokens_scored, proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e27ed0bb1e940b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T05:17:49.688233Z",
     "start_time": "2025-03-23T05:10:13.058929Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]]\n",
    "    args = parse_args()\n",
    "    model, tokenizer, device = load_model(args)\n",
    "\n",
    "    # Assume truncated_texts is a list of 500 prompt strings (each truncated to less than 200 words).\n",
    "    # For example, you may load them from a file or sample from a dataset.\n",
    "    # Here we assume truncated_texts is already defined.\n",
    "\n",
    "    # --- Precompute Vocabulary and Perfect Matching via Dictionary ---\n",
    "    tokenizer_for_vocab = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    vocab_list = get_vocabulary(tokenizer_for_vocab)\n",
    "    print(f\"Vocabulary size: {len(vocab_list)}\")\n",
    "    unique_indices, paired_indices = filter_tokens_with_synonyms(vocab_list)\n",
    "    print(f\"Unique tokens (set A): {len(unique_indices)}\")\n",
    "    print(f\"Tokens with synonyms (set B): {len(paired_indices)}\")\n",
    "    similarity_matrix = construct_similarity_matrix(vocab_list, paired_indices)\n",
    "    matching = find_perfect_matching(similarity_matrix)\n",
    "    mapped_pairing = [(paired_indices[i], paired_indices[j]) for (i, j) in matching]\n",
    "    args.precomputed_pairing = mapped_pairing\n",
    "    args.unique_tokens = unique_indices\n",
    "\n",
    "    # --- Precompute Vocabulary and Perfect Matching via Cosine Similarity ---\n",
    "    args_cos = parse_args()\n",
    "    embedding_matrix = model.get_input_embeddings().weight  # shape: (vocab_size, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a037881-72ef-407e-b6da-00ddf2279f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy random matching: 100%|██████████| 11078/11078 [00:09<00:00, 1185.56it/s]\n"
     ]
    }
   ],
   "source": [
    "    similarity_matrix_cos = construct_similarity_matrix_cos(vocab_list, paired_indices,embedding_matrix)\n",
    "    matching_cos = find_perfect_matching_greedy_random(similarity_matrix_cos)\n",
    "    mapped_pairing_cos = [(paired_indices[i], paired_indices[j]) for (i, j) in matching_cos]\n",
    "    args_cos.precomputed_pairing = mapped_pairing_cos\n",
    "    args_cos.unique_tokens = unique_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337677706a99db70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T05:17:51.504050Z",
     "start_time": "2025-03-23T05:17:49.695598Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    # --- Load the \"realnewslike\" subset of C4 (English) and Shuffle the dataset with a fixed seed for reproducibility ---\n",
    "    c4_realnewslike = load_dataset(\"c4\", \"realnewslike\", split=\"train\", streaming=False, trust_remote_code=True)\n",
    "    shuffled_dataset = c4_realnewslike.shuffle(seed=45)\n",
    "    sampled_examples = shuffled_dataset.select(range(300))\n",
    "    sampled_texts = [example[\"text\"] for example in sampled_examples]\n",
    "    print(f\"Sampled {len(sampled_texts)} news-like texts from C4.\")\n",
    "    max_words = 150\n",
    "    truncated_texts = []\n",
    "    for text in sampled_texts:\n",
    "        words = text.split()  # split text into words\n",
    "        truncated_text = \" \".join(words[:max_words])\n",
    "        truncated_texts.append(truncated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7709d-4cad-421d-a335-a4f403cdc03c",
   "metadata": {},
   "source": [
    "**Delta = 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2e1f68c3e014d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_delta = 5\n",
    "args.delta = tune_delta\n",
    "args_cos.delta = tune_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7215689f-872f-4c53-a453-1eda4c9f758f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\n",
      "\n",
      "The most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\n",
      "\n",
      "Picking the winners and losers in this increasingly volatile market is a challenge. The market goes up, then it goes down, then it goes up again, and then it goes down. How can you tell the difference? Here are a few simple tips.\n",
      "\n",
      "The most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\n",
      "\n",
      "This is the second in a series of articles that\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   0%|          | 1/300 [02:36<13:00:49, 156.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Choose any of the INOV videos above to watch, by clicking the associated image or headline. These results are drawn from the library of videos produced here at Market News Video, that have been tagged by an editor with the inov symbol. The date of each video is listed underneath the headline. Beneath the listing of inov videos is a current stock quote for inov and performance chart. At the bottom of the page, you will find related articles mentioning inov. From all of us here at Market News Video, we hope you will enjoy these inov videos and articles.', 'default': {'decoded_wm': '\\n\\nThe most recent inov topic was created on February, 27, 2013.\\n\\nThe most recent inov stock quote and inov stock performance was created on February, 27, 2013.\\n\\nMarket News Video\\n\\nMarket News Video is the source for millions of videos, covering various market events, including news, market analysis, technical insights, and other topics. Inov videos are created and updated by the inov Team, and this site is dedicated to providing you with the most accurate and up-to-date videos. We hope you enjoy the videos.', 'decoded_nw': \"\\n\\nThe most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\\n\\nThe most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\\n\\nPicking the winners and losers in this increasingly volatile market is a challenge. The market goes up, then it goes down, then it goes up again, and then it goes down. How can you tell the difference? Here are a few simple tips.\\n\\nThe most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\\n\\nThis is the second in a series of articles that\", 'green_count_w': 46, 'green_count_nw': 42, 'tokens_scored_w': 50, 'tokens_scored_nw': 100, 'prop_w': 0.92, 'z_w': 5.939696961966999, 'z_nw': -1.6, 'p_w': np.float64(1.427747089796094e-09), 'judgement': 'LLM-generated (watermarked)', 'ppl_wm': 10.86153507232666, 'ppl_nw': 2.919301748275757, 'tokens_generated': 117}, 'kgw': {'decoded_wm': ' If there are inov you would want featured here on Market News Vimeo, or other videos in other categories at our Vimeo site - contact the site. If there are inov you would want featured here or other videos in other categories at our Vimeo site - contact the site. Be well. - Markets Daily', 'decoded_nw': \"\\n\\nThe most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\\n\\nThe most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\\n\\nPicking the winners and losers in this increasingly volatile market is a challenge. The market goes up, then it goes down, then it goes up again, and then it goes down. How can you tell the difference? Here are a few simple tips.\\n\\nThe most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\\n\\nThis is the second in a series of articles that\", 'green_count_w': 59, 'tokens_scored_w': 63, 'prop_w': 0.9365079365079365, 'z_w': 12.583898336942227, 'p_w': np.float64(1.2946492746758875e-36), 'judgement': 'LLM-generated (watermarked)', 'ppl_wm': 26.84709930419922, 'ppl_nw': 2.919301748275757, 'tokens_generated': 65}, 'cos': {'decoded_wm': '\\n\\nThe most basic rule of thumb is to invest what you are willing to \"waste\". (The better rule is to invest what you are willing to \"earn\") There is a wealth of books and websites available that tell you how to invest, and how to \"earn\" your profits.\\n\\nBetween the three of us, we have seen how many of these \"how to invest\" books actually work, and what they really teach. For those of you that are not aware, the \"how to invest\" book industry is a multibillion-dollar industry. The title of this book is \"How to Invest\", and the author is Doug Casey.\\n\\nThe main difference with this book is that, instead of you having to \"earn\" your profits, you have a \"lead\" that you can work with. You can \"earn\" your profits, and then \"spend\" your profits. This is what the author does, and this is how', 'decoded_nw': \"\\n\\nThe most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\\n\\nThe most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\\n\\nPicking the winners and losers in this increasingly volatile market is a challenge. The market goes up, then it goes down, then it goes up again, and then it goes down. How can you tell the difference? Here are a few simple tips.\\n\\nThe most basic rule in investing is to invest only what you can afford to lose, but you can't afford to lose a lot. Researching companies is a great way to understand them and make your investing decisions.\\n\\nThis is the second in a series of articles that\", 'green_count_w': 63, 'tokens_scored_w': 65, 'prop_w': 0.9692307692307692, 'z_w': 7.566118809941717, 'p_w': np.float64(1.922701867818613e-14), 'judgement': 'LLM-generated (watermarked)', 'ppl_wm': 6.400722503662109, 'ppl_nw': 2.919301748275757, 'tokens_generated': 201}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    results, aggregated = evaluate_watermarking(truncated_texts, model, tokenizer, args, args_cos)\n",
    "\n",
    "    # Print per-prompt results for the first 5 prompts.\n",
    "    for r in results[:5]:\n",
    "        print(\"=== Prompt ===\")\n",
    "        print(r[\"prompt\"])\n",
    "        print(\"--- Default Method ---\")\n",
    "        print(\"Watermarked Text:\")\n",
    "        print(r[\"default\"][\"decoded_wm\"])\n",
    "        print(\"Detection (Default):\")\n",
    "        print(f\"  Green tokens (paired): {r['default']['green_count_w']} / {r['default']['tokens_scored_w']} ({r['default']['prop_w']:.2%})\")\n",
    "        print(f\"  z–score: {r['default']['z_w']:.2f}, p–value: {r['default']['p_w']:.4f}\")\n",
    "        print(f\"  Judgement: {r['default']['judgement']}\")\n",
    "        print(f\"  Perplexity: {r['default']['ppl_wm']:.2f}\")\n",
    "        print(\"--- KGW Method ---\")\n",
    "        print(\"Watermarked Text:\")\n",
    "        print(r[\"kgw\"][\"decoded_wm\"])\n",
    "        print(\"Detection (KGW):\")\n",
    "        print(f\"  Green tokens (paired): {r['kgw']['green_count_w']} / {r['kgw']['tokens_scored_w']} ({r['kgw']['prop_w']:.2%})\")\n",
    "        print(f\"  z–score: {r['kgw']['z_w']:.2f}, p–value: {r['kgw']['p_w']:.4f}\")\n",
    "        print(f\"  Judgement: {r['kgw']['judgement']}\")\n",
    "        print(f\"  Perplexity: {r['kgw']['ppl_wm']:.2f}\")\n",
    "        print(\"--- Cosine-based Method ---\")\n",
    "        print(\"Watermarked Text:\")\n",
    "        print(r[\"cos\"][\"decoded_wm\"])\n",
    "        print(\"Detection (Cos):\")\n",
    "        print(f\"  Green tokens (paired): {r['cos']['green_count_w']} / {r['cos']['tokens_scored_w']} ({r['cos']['prop_w']:.2%})\")\n",
    "        print(f\"  z–score: {r['cos']['z_w']:.2f}, p–value: {r['cos']['p_w']:.4f}\")\n",
    "        print(f\"  Judgement: {r['cos']['judgement']}\")\n",
    "        print(f\"  Perplexity: {r['cos']['ppl_wm']:.2f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Print aggregated metrics.\n",
    "    print(\"=== Aggregated Metrics ===\")\n",
    "    print(\"Default Method:\")\n",
    "    print(\"  Average Perplexity (Watermarked):\", aggregated[\"default\"][\"avg_ppl_wm\"])\n",
    "    print(\"  Average Perplexity (Non-watermarked):\", aggregated[\"default\"][\"avg_ppl_nw\"])\n",
    "    print(\"  Average z-score (Watermarked):\", aggregated[\"default\"][\"avg_z_wm\"])\n",
    "    print(\"  Average z-score (Non-watermarked):\", aggregated[\"default\"][\"avg_z_nw\"])\n",
    "    print(\"  AUC:\", aggregated[\"default\"][\"auc\"])\n",
    "    print(\"  Valid prompts:\", aggregated[\"default\"][\"num_valid\"])\n",
    "\n",
    "    print(\"\\nKGW Method:\")\n",
    "    print(\"  Average Perplexity (Watermarked):\", aggregated[\"kgw\"][\"avg_ppl_wm\"])\n",
    "    print(\"  Average Perplexity (Non-watermarked):\", aggregated[\"kgw\"][\"avg_ppl_nw\"])\n",
    "    print(\"  Average z-score (Watermarked):\", aggregated[\"kgw\"][\"avg_z_wm\"])\n",
    "    print(\"  Average z-score (Non-watermarked):\", aggregated[\"kgw\"][\"avg_z_nw\"])\n",
    "    print(\"  AUC:\", aggregated[\"kgw\"][\"auc\"])\n",
    "    print(\"  Valid prompts:\", aggregated[\"kgw\"][\"num_valid\"])\n",
    "\n",
    "    print(\"\\nCosine-based Method:\")\n",
    "    print(\"  Average Perplexity (Watermarked):\", aggregated[\"cos\"][\"avg_ppl_wm\"])\n",
    "    print(\"  Average Perplexity (Non-watermarked):\", aggregated[\"cos\"][\"avg_ppl_nw\"])\n",
    "    print(\"  Average z-score (Watermarked):\", aggregated[\"cos\"][\"avg_z_wm\"])\n",
    "    print(\"  Average z-score (Non-watermarked):\", aggregated[\"cos\"][\"avg_z_nw\"])\n",
    "    print(\"  AUC:\", aggregated[\"cos\"][\"auc\"])\n",
    "    print(\"  Valid prompts:\", aggregated[\"cos\"][\"num_valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f629390-93ee-428d-a71d-1ba4719dbd6a",
   "metadata": {},
   "source": [
    "**Delta = 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdf7917-5386-4ed5-9b1d-1fa9b3e756d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_delta = 10\n",
    "args.delta = tune_delta\n",
    "args_cos.delta = tune_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93090041c11d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    results, aggregated = evaluate_watermarking(truncated_texts, model, tokenizer, args, args_cos)\n",
    "\n",
    "    # Print per-prompt results for the first 5 prompts.\n",
    "    for r in results[:5]:\n",
    "        print(\"=== Prompt ===\")\n",
    "        print(r[\"prompt\"])\n",
    "        print(\"--- Default Method ---\")\n",
    "        print(\"Watermarked Text:\")\n",
    "        print(r[\"default\"][\"decoded_wm\"])\n",
    "        print(\"Detection (Default):\")\n",
    "        print(f\"  Green tokens (paired): {r['default']['green_count_w']} / {r['default']['tokens_scored_w']} ({r['default']['prop_w']:.2%})\")\n",
    "        print(f\"  z–score: {r['default']['z_w']:.2f}, p–value: {r['default']['p_w']:.4f}\")\n",
    "        print(f\"  Judgement: {r['default']['judgement']}\")\n",
    "        print(f\"  Perplexity: {r['default']['ppl_wm']:.2f}\")\n",
    "        print(\"--- KGW Method ---\")\n",
    "        print(\"Watermarked Text:\")\n",
    "        print(r[\"kgw\"][\"decoded_wm\"])\n",
    "        print(\"Detection (KGW):\")\n",
    "        print(f\"  Green tokens (paired): {r['kgw']['green_count_w']} / {r['kgw']['tokens_scored_w']} ({r['kgw']['prop_w']:.2%})\")\n",
    "        print(f\"  z–score: {r['kgw']['z_w']:.2f}, p–value: {r['kgw']['p_w']:.4f}\")\n",
    "        print(f\"  Judgement: {r['kgw']['judgement']}\")\n",
    "        print(f\"  Perplexity: {r['kgw']['ppl_wm']:.2f}\")\n",
    "        print(\"--- Cosine-based Method ---\")\n",
    "        print(\"Watermarked Text:\")\n",
    "        print(r[\"cos\"][\"decoded_wm\"])\n",
    "        print(\"Detection (Cos):\")\n",
    "        print(f\"  Green tokens (paired): {r['cos']['green_count_w']} / {r['cos']['tokens_scored_w']} ({r['cos']['prop_w']:.2%})\")\n",
    "        print(f\"  z–score: {r['cos']['z_w']:.2f}, p–value: {r['cos']['p_w']:.4f}\")\n",
    "        print(f\"  Judgement: {r['cos']['judgement']}\")\n",
    "        print(f\"  Perplexity: {r['cos']['ppl_wm']:.2f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Print aggregated metrics.\n",
    "    print(\"=== Aggregated Metrics ===\")\n",
    "    print(\"Default Method:\")\n",
    "    print(\"  Average Perplexity (Watermarked):\", aggregated[\"default\"][\"avg_ppl_wm\"])\n",
    "    print(\"  Average Perplexity (Non-watermarked):\", aggregated[\"default\"][\"avg_ppl_nw\"])\n",
    "    print(\"  Average z-score (Watermarked):\", aggregated[\"default\"][\"avg_z_wm\"])\n",
    "    print(\"  Average z-score (Non-watermarked):\", aggregated[\"default\"][\"avg_z_nw\"])\n",
    "    print(\"  AUC:\", aggregated[\"default\"][\"auc\"])\n",
    "    print(\"  Valid prompts:\", aggregated[\"default\"][\"num_valid\"])\n",
    "\n",
    "    print(\"\\nKGW Method:\")\n",
    "    print(\"  Average Perplexity (Watermarked):\", aggregated[\"kgw\"][\"avg_ppl_wm\"])\n",
    "    print(\"  Average Perplexity (Non-watermarked):\", aggregated[\"kgw\"][\"avg_ppl_nw\"])\n",
    "    print(\"  Average z-score (Watermarked):\", aggregated[\"kgw\"][\"avg_z_wm\"])\n",
    "    print(\"  Average z-score (Non-watermarked):\", aggregated[\"kgw\"][\"avg_z_nw\"])\n",
    "    print(\"  AUC:\", aggregated[\"kgw\"][\"auc\"])\n",
    "    print(\"  Valid prompts:\", aggregated[\"kgw\"][\"num_valid\"])\n",
    "\n",
    "    print(\"\\nCosine-based Method:\")\n",
    "    print(\"  Average Perplexity (Watermarked):\", aggregated[\"cos\"][\"avg_ppl_wm\"])\n",
    "    print(\"  Average Perplexity (Non-watermarked):\", aggregated[\"cos\"][\"avg_ppl_nw\"])\n",
    "    print(\"  Average z-score (Watermarked):\", aggregated[\"cos\"][\"avg_z_wm\"])\n",
    "    print(\"  Average z-score (Non-watermarked):\", aggregated[\"cos\"][\"avg_z_nw\"])\n",
    "    print(\"  AUC:\", aggregated[\"cos\"][\"auc\"])\n",
    "    print(\"  Valid prompts:\", aggregated[\"cos\"][\"num_valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840dac728ad6d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f8ed6-cfe1-47fe-bc8d-63b67e731c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d36b50-6ae8-4b11-a953-e4c6e8040435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c95b89-492d-4b67-9624-ec01481b6a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f40f6-10d7-4b06-a9c3-552304ac36b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd114ce-8c2f-4591-be5a-bcf1cb11be17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ef77cbc917f61c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T05:09:30.176673Z",
     "start_time": "2025-03-23T05:09:30.145054Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n",
    "# available at https://arxiv.org/abs/2301.10226\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "\n",
    "import numpy \n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSeq2SeqLM,\n",
    "                          AutoModelForCausalLM,\n",
    "                          LogitsProcessorList)\n",
    "\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"Util function for user friendly boolean flag args\"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Command line argument specification\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"A minimum working example of applying the watermark to any LLM that supports the huggingface 🤗 `generate` API\")\n",
    "\n",
    "    #parser.add_argument(\n",
    "    #    \"--run_gradio\",\n",
    "    #    type=str2bool,\n",
    "    #    default=False,\n",
    "    #    help=\"Whether to launch as a gradio demo. Set to False if not installed and want to just run the stdout version.\",\n",
    "    #)\n",
    "    parser.add_argument(\n",
    "        \"--demo_public\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to expose the gradio demo to the internet.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        default=\"facebook-opt1.3b\",#\"gpt2-medium\",#\n",
    "        help=\"Main model, path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prompt_max_length\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Truncation length for prompt, overrides model config's max length field.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_new_tokens\",\n",
    "        type=int,\n",
    "        default=200,\n",
    "        help=\"Maximmum number of new tokens to generate.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generation_seed\",\n",
    "        type=int,\n",
    "        default=123,\n",
    "        help=\"Seed for setting the torch global rng prior to generation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_sampling\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to generate using multinomial sampling.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sampling_temp\",\n",
    "        type=float,\n",
    "        default=0.7,\n",
    "        help=\"Sampling temperature to use when generating using multinomial sampling.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_beams\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of beams to use for beam search. 1 is normal greedy decoding\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_gpu\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to run inference and watermark hashing/seeding/permutation on gpu.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seeding_scheme\",\n",
    "        type=str,\n",
    "        default=\"simple_1\",\n",
    "        help=\"Seeding scheme to use to generate the greenlists at each generation and verification step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gamma\",\n",
    "        type=float,\n",
    "        default=0.25,\n",
    "        help=\"The fraction of the vocabulary to partition into the greenlist at each generation and verification step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--delta\",\n",
    "        type=float,\n",
    "        default=2.0,\n",
    "        help=\"The amount/bias to add to each of the greenlist token logits before each token sampling step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--normalizers\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Single or comma separated list of the preprocessors/normalizer names to use when performing watermark detection.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ignore_repeated_bigrams\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to use the detection method that only counts each unqiue bigram once as either a green or red hit.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--detection_z_threshold\",\n",
    "        type=float,\n",
    "        default=4.0,\n",
    "        help=\"The test statistic threshold for the detection hypothesis test.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--select_green_tokens\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"How to treat the permuation when selecting the greenlist tokens at each step. Legacy is (False) to pick the complement/reds first.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip_model_load\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Skip the model loading to debug the interface.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed_separately\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to call the torch seed function before both the unwatermarked and watermarked generate calls.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_fp16\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to run model in float16 precsion.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_model(args):\n",
    "    \"\"\"Load and return the model and tokenizer\"\"\"\n",
    "\n",
    "    args.is_seq2seq_model = any([(model_type in args.model_name_or_path) for model_type in [\"t5\", \"T0\"]])\n",
    "    args.is_decoder_only_model = any(\n",
    "        [(model_type in args.model_name_or_path) for model_type in [\"gpt\", \"opt\", \"bloom\"]])\n",
    "    if args.is_seq2seq_model:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n",
    "    elif args.is_decoder_only_model:\n",
    "        if args.load_fp16:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, torch_dtype=torch.float16,\n",
    "                                                         device_map='auto')\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {args.model_name_or_path}\")\n",
    "\n",
    "    if args.use_gpu:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if args.load_fp16:\n",
    "            pass\n",
    "        else:\n",
    "            model = model.to(device)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def generate_kgw(prompt, args, model=None, device=None, tokenizer=None):\n",
    "    \"\"\"Instatiate the WatermarkLogitsProcessor according to the watermark parameters\n",
    "       and generate watermarked text by passing it to the generate method of the model\n",
    "       as a logits processor. \"\"\"\n",
    "\n",
    "    #print(f\"Generating with {args}\")\n",
    "\n",
    "    watermark_processor = WatermarkLogitsProcessor_kgw(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                                   gamma=args.gamma,\n",
    "                                                   delta=args.delta,\n",
    "                                                   seeding_scheme=args.seeding_scheme,\n",
    "                                                   select_green_tokens=args.select_green_tokens)\n",
    "\n",
    "    gen_kwargs = dict(max_new_tokens=args.max_new_tokens)\n",
    "\n",
    "    if args.use_sampling:\n",
    "        gen_kwargs.update(dict(\n",
    "            do_sample=True,\n",
    "            top_k=0,\n",
    "            temperature=args.sampling_temp\n",
    "        ))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(\n",
    "            num_beams=args.n_beams\n",
    "        ))\n",
    "\n",
    "    generate_without_watermark = partial(\n",
    "        model.generate,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    generate_with_watermark = partial(\n",
    "        model.generate,\n",
    "        logits_processor=LogitsProcessorList([watermark_processor]),\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    if args.prompt_max_length:\n",
    "        pass\n",
    "    elif hasattr(model.config, \"max_position_embedding\"):\n",
    "        args.prompt_max_length = model.config.max_position_embeddings - args.max_new_tokens\n",
    "    else:\n",
    "        args.prompt_max_length = 2048 - args.max_new_tokens\n",
    "\n",
    "    tokd_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True, truncation=True,\n",
    "                           max_length=args.prompt_max_length).to(device)\n",
    "    truncation_warning = True if tokd_input[\"input_ids\"].shape[-1] == args.prompt_max_length else False\n",
    "    redecoded_input = tokenizer.batch_decode(tokd_input[\"input_ids\"], skip_special_tokens=True)[0]\n",
    "\n",
    "    torch.manual_seed(args.generation_seed)\n",
    "    output_without_watermark = generate_without_watermark(**tokd_input)\n",
    "\n",
    "    # optional to seed before second generation, but will not be the same again generally, unless delta==0.0, no-op watermark\n",
    "    if args.seed_separately:\n",
    "        torch.manual_seed(args.generation_seed)\n",
    "    output_with_watermark = generate_with_watermark(**tokd_input)\n",
    "\n",
    "    if args.is_decoder_only_model:\n",
    "        # need to isolate the newly generated tokens\n",
    "        output_without_watermark = output_without_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "        output_with_watermark = output_with_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    decoded_output_without_watermark = tokenizer.batch_decode(output_without_watermark, skip_special_tokens=True)[0]\n",
    "    decoded_output_with_watermark = tokenizer.batch_decode(output_with_watermark, skip_special_tokens=True)[0]\n",
    "\n",
    "    return (redecoded_input,\n",
    "            int(truncation_warning),\n",
    "            decoded_output_without_watermark,\n",
    "            decoded_output_with_watermark,\n",
    "            args)\n",
    "    # decoded_output_with_watermark)\n",
    "\n",
    "\n",
    "def format_names(s):\n",
    "    \"\"\"Format names for the gradio demo interface\"\"\"\n",
    "    s = s.replace(\"num_tokens_scored\", \"Tokens Counted (T)\")\n",
    "    s = s.replace(\"num_green_tokens\", \"# Tokens in Greenlist\")\n",
    "    s = s.replace(\"green_fraction\", \"Fraction of T in Greenlist\")\n",
    "    s = s.replace(\"z_score\", \"z-score\")\n",
    "    s = s.replace(\"p_value\", \"p value\")\n",
    "    s = s.replace(\"prediction\", \"Prediction\")\n",
    "    s = s.replace(\"confidence\", \"Confidence\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def list_format_scores(score_dict, detection_threshold):\n",
    "    \"\"\"Format the detection metrics into a gradio dataframe input format\"\"\"\n",
    "    lst_2d = []\n",
    "    # lst_2d.append([\"z-score threshold\", f\"{detection_threshold}\"])\n",
    "    for k, v in score_dict.items():\n",
    "        if k == 'green_fraction':\n",
    "            lst_2d.append([format_names(k), f\"{v:.1%}\"])\n",
    "        elif k == 'confidence':\n",
    "            lst_2d.append([format_names(k), f\"{v:.3%}\"])\n",
    "        elif isinstance(v, float):\n",
    "            lst_2d.append([format_names(k), f\"{v:.3g}\"])\n",
    "        elif isinstance(v, bool):\n",
    "            lst_2d.append([format_names(k), (\"Watermarked\" if v else \"Human/Unwatermarked\")])\n",
    "        else:\n",
    "            lst_2d.append([format_names(k), f\"{v}\"])\n",
    "    if \"confidence\" in score_dict:\n",
    "        lst_2d.insert(-2, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    else:\n",
    "        lst_2d.insert(-1, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    return lst_2d\n",
    "\n",
    "\n",
    "def detect_kgw(input_text, args, device=None, tokenizer=None):\n",
    "    \"\"\"Instantiate the WatermarkDetection object and call detect on\n",
    "        the input text returning the scores and outcome of the test\"\"\"\n",
    "    watermark_detector = WatermarkDetector_kgw(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                           gamma=args.gamma,\n",
    "                                           seeding_scheme=args.seeding_scheme,\n",
    "                                           device=device,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           z_threshold=args.detection_z_threshold,\n",
    "                                           normalizers=args.normalizers,\n",
    "                                           ignore_repeated_bigrams=args.ignore_repeated_bigrams,\n",
    "                                           select_green_tokens=args.select_green_tokens)\n",
    "    if len(input_text) - 1 > watermark_detector.min_prefix_len:\n",
    "        score_dict = watermark_detector.detect(input_text)\n",
    "        # output = str_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "        output = list_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "    else:\n",
    "        score_dict = None\n",
    "        # output = (f\"Error: string not long enough to compute watermark presence.\")\n",
    "        output = [[\"Error\", \"string too short to compute metrics\"]]\n",
    "        output += [[\"\", \"\"] for _ in range(6)]\n",
    "    return output, score_dict, args\n",
    "\n",
    "\n",
    "def run_gradio(args, model=None, device=None, tokenizer=None):\n",
    "    \"\"\"Define and launch the gradio demo interface\"\"\"\n",
    "    generate_partial = partial(generate_kgw, model=model, device=device, tokenizer=tokenizer)\n",
    "    detect_partial = partial(detect_kgw, device=device, tokenizer=tokenizer)\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        # Top section, greeting and instructions\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=9):\n",
    "                gr.Markdown(\n",
    "                    \"\"\"\n",
    "                    ## 💧 [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226) 🔍\n",
    "                    \"\"\"\n",
    "                )\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\n",
    "                    \"\"\"\n",
    "                    [![](https://badgen.net/badge/icon/GitHub?icon=github&label)](https://github.com/jwkirchenbauer/lm-watermarking)\n",
    "                    \"\"\"\n",
    "                )\n",
    "            # with gr.Column(scale=2):\n",
    "            #     pass\n",
    "            # ![visitor badge](https://visitor-badge.glitch.me/badge?page_id=tomg-group-umd_lm-watermarking) # buggy\n",
    "\n",
    "        with gr.Accordion(\"Understanding the output metrics\", open=False):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                - `z-score threshold` : The cuttoff for the hypothesis test\n",
    "                - `Tokens Counted (T)` : The number of tokens in the output that were counted by the detection algorithm. \n",
    "                    The first token is ommitted in the simple, single token seeding scheme since there is no way to generate\n",
    "                    a greenlist for it as it has no prefix token(s). Under the \"Ignore Bigram Repeats\" detection algorithm, \n",
    "                    described in the bottom panel, this can be much less than the total number of tokens generated if there is a lot of repetition.\n",
    "                - `# Tokens in Greenlist` : The number of tokens that were observed to fall in their respective greenlist\n",
    "                - `Fraction of T in Greenlist` : The `# Tokens in Greenlist` / `T`. This is expected to be approximately `gamma` for human/unwatermarked text.\n",
    "                - `z-score` : The test statistic for the detection hypothesis test. If larger than the `z-score threshold` \n",
    "                    we \"reject the null hypothesis\" that the text is human/unwatermarked, and conclude it is watermarked\n",
    "                - `p value` : The likelihood of observing the computed `z-score` under the null hypothesis. This is the likelihood of \n",
    "                    observing the `Fraction of T in Greenlist` given that the text was generated without knowledge of the watermark procedure/greenlists.\n",
    "                    If this is extremely _small_ we are confident that this many green tokens was not chosen by random chance.\n",
    "                -  `prediction` : The outcome of the hypothesis test - whether the observed `z-score` was higher than the `z-score threshold`\n",
    "                - `confidence` : If we reject the null hypothesis, and the `prediction` is \"Watermarked\", then we report 1-`p value` to represent \n",
    "                    the confidence of the detection based on the unlikeliness of this `z-score` observation.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        with gr.Accordion(\"A note on model capability\", open=True):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                This demo uses open-source language models that fit on a single GPU. These models are less powerful than proprietary commercial tools like ChatGPT, Claude, or Bard. \n",
    "\n",
    "                Importantly, we use a language model that is designed to \"complete\" your prompt, and not a model this is fine-tuned to follow instructions. \n",
    "                For best results, prompt the model with a few sentences that form the beginning of a paragraph, and then allow it to \"continue\" your paragraph. \n",
    "                Some examples include the opening paragraph of a wikipedia article, or the first few sentences of a story. \n",
    "                Longer prompts that end mid-sentence will result in more fluent generations.\n",
    "                \"\"\"\n",
    "            )\n",
    "        gr.Markdown(f\"Language model: {args.model_name_or_path} {'(float16 mode)' if args.load_fp16 else ''}\")\n",
    "\n",
    "        # Construct state for parameters, define updates and toggles\n",
    "        default_prompt = args.__dict__.pop(\"default_prompt\")\n",
    "        session_args = gr.State(value=args)\n",
    "\n",
    "        with gr.Tab(\"Generate and Detect\"):\n",
    "\n",
    "            with gr.Row():\n",
    "                prompt = gr.Textbox(label=f\"Prompt\", interactive=True, lines=10, max_lines=10, value=default_prompt)\n",
    "            with gr.Row():\n",
    "                generate_btn = gr.Button(\"Generate\")\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    output_without_watermark = gr.Textbox(label=\"Output Without Watermark\", interactive=False, lines=14,\n",
    "                                                          max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # without_watermark_detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    without_watermark_detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False,\n",
    "                                                                      row_count=7, col_count=2)\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    output_with_watermark = gr.Textbox(label=\"Output With Watermark\", interactive=False, lines=14,\n",
    "                                                       max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # with_watermark_detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    with_watermark_detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False,\n",
    "                                                                   row_count=7, col_count=2)\n",
    "\n",
    "            redecoded_input = gr.Textbox(visible=False)\n",
    "            truncation_warning = gr.Number(visible=False)\n",
    "\n",
    "            def truncate_prompt(redecoded_input, truncation_warning, orig_prompt, args):\n",
    "                if truncation_warning:\n",
    "                    return redecoded_input + f\"\\n\\n[Prompt was truncated before generation due to length...]\", args\n",
    "                else:\n",
    "                    return orig_prompt, args\n",
    "\n",
    "        with gr.Tab(\"Detector Only\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    detection_input = gr.Textbox(label=\"Text to Analyze\", interactive=True, lines=14, max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False, row_count=7,\n",
    "                                                    col_count=2)\n",
    "            with gr.Row():\n",
    "                detect_btn = gr.Button(\"Detect\")\n",
    "\n",
    "        # Parameter selection group\n",
    "        with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(f\"#### Generation Parameters\")\n",
    "                    with gr.Row():\n",
    "                        decoding = gr.Radio(label=\"Decoding Method\", choices=[\"multinomial\", \"greedy\"],\n",
    "                                            value=(\"multinomial\" if args.use_sampling else \"greedy\"))\n",
    "                    with gr.Row():\n",
    "                        sampling_temp = gr.Slider(label=\"Sampling Temperature\", minimum=0.1, maximum=1.0, step=0.1,\n",
    "                                                  value=args.sampling_temp, visible=True)\n",
    "                    with gr.Row():\n",
    "                        generation_seed = gr.Number(label=\"Generation Seed\", value=args.generation_seed,\n",
    "                                                    interactive=True)\n",
    "                    with gr.Row():\n",
    "                        n_beams = gr.Dropdown(label=\"Number of Beams\", choices=list(range(1, 11, 1)),\n",
    "                                              value=args.n_beams, visible=(not args.use_sampling))\n",
    "                    with gr.Row():\n",
    "                        max_new_tokens = gr.Slider(label=\"Max Generated Tokens\", minimum=10, maximum=1000, step=10,\n",
    "                                                   value=args.max_new_tokens)\n",
    "\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(f\"#### Watermark Parameters\")\n",
    "                    with gr.Row():\n",
    "                        gamma = gr.Slider(label=\"gamma\", minimum=0.1, maximum=0.9, step=0.05, value=args.gamma)\n",
    "                    with gr.Row():\n",
    "                        delta = gr.Slider(label=\"delta\", minimum=0.0, maximum=10.0, step=0.1, value=args.delta)\n",
    "                    gr.Markdown(f\"#### Detector Parameters\")\n",
    "                    with gr.Row():\n",
    "                        detection_z_threshold = gr.Slider(label=\"z-score threshold\", minimum=0.0, maximum=10.0,\n",
    "                                                          step=0.1, value=args.detection_z_threshold)\n",
    "                    with gr.Row():\n",
    "                        ignore_repeated_bigrams = gr.Checkbox(label=\"Ignore Bigram Repeats\")\n",
    "                    with gr.Row():\n",
    "                        normalizers = gr.CheckboxGroup(label=\"Normalizations\",\n",
    "                                                       choices=[\"unicode\", \"homoglyphs\", \"truecase\"],\n",
    "                                                       value=args.normalizers)\n",
    "            # with gr.Accordion(\"Actual submitted parameters:\",open=False):\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\n",
    "                    f\"_Note: sliders don't always update perfectly. Clicking on the bar or using the number window to the right can help. Window below shows the current settings._\")\n",
    "            with gr.Row():\n",
    "                current_parameters = gr.Textbox(label=\"Current Parameters\", value=args)\n",
    "            with gr.Accordion(\"Legacy Settings\", open=False):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        seed_separately = gr.Checkbox(label=\"Seed both generations separately\",\n",
    "                                                      value=args.seed_separately)\n",
    "                    with gr.Column(scale=1):\n",
    "                        select_green_tokens = gr.Checkbox(label=\"Select 'greenlist' from partition\",\n",
    "                                                          value=args.select_green_tokens)\n",
    "\n",
    "        with gr.Accordion(\"Understanding the settings\", open=False):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                #### Generation Parameters:\n",
    "    \n",
    "                - Decoding Method : We can generate tokens from the model using either multinomial sampling or we can generate using greedy decoding.\n",
    "                - Sampling Temperature : If using multinomial sampling we can set the temperature of the sampling distribution. \n",
    "                                    0.0 is equivalent to greedy decoding, and 1.0 is the maximum amount of variability/entropy in the next token distribution.\n",
    "                                    0.7 strikes a nice balance between faithfulness to the model's estimate of top candidates while adding variety. Does not apply for greedy decoding.\n",
    "                - Generation Seed : The integer to pass to the torch random number generator before running generation. Makes the multinomial sampling strategy\n",
    "                                    outputs reproducible. Does not apply for greedy decoding.\n",
    "                - Number of Beams : When using greedy decoding, we can also set the number of beams to > 1 to enable beam search. \n",
    "                                    This is not implemented/excluded from paper for multinomial sampling but may be added in future.\n",
    "                - Max Generated Tokens : The `max_new_tokens` parameter passed to the generation method to stop the output at a certain number of new tokens. \n",
    "                                        Note that the model is free to generate fewer tokens depending on the prompt. \n",
    "                                        Implicitly this sets the maximum number of prompt tokens possible as the model's maximum input length minus `max_new_tokens`,\n",
    "                                        and inputs will be truncated accordingly.\n",
    "    \n",
    "                #### Watermark Parameters:\n",
    "    \n",
    "                - gamma : The fraction of the vocabulary to be partitioned into the greenlist at each generation step. \n",
    "                         Smaller gamma values create a stronger watermark by enabling the watermarked model to achieve \n",
    "                         a greater differentiation from human/unwatermarked text because it is preferentially sampling \n",
    "                         from a smaller green set making those tokens less likely to occur by chance.\n",
    "                - delta : The amount of positive bias to add to the logits of every token in the greenlist \n",
    "                            at each generation step before sampling/choosing the next token. Higher delta values \n",
    "                            mean that the greenlist tokens are more heavily preferred by the watermarked model\n",
    "                            and as the bias becomes very large the watermark transitions from \"soft\" to \"hard\". \n",
    "                            For a hard watermark, nearly all tokens are green, but this can have a detrimental effect on\n",
    "                            generation quality, especially when there is not a lot of flexibility in the distribution.\n",
    "    \n",
    "                #### Detector Parameters:\n",
    "    \n",
    "                - z-score threshold : the z-score cuttoff for the hypothesis test. Higher thresholds (such as 4.0) make\n",
    "                                    _false positives_ (predicting that human/unwatermarked text is watermarked) very unlikely\n",
    "                                    as a genuine human text with a significant number of tokens will almost never achieve \n",
    "                                    that high of a z-score. Lower thresholds will capture more _true positives_ as some watermarked\n",
    "                                    texts will contain less green tokens and achive a lower z-score, but still pass the lower bar and \n",
    "                                    be flagged as \"watermarked\". However, a lowere threshold will increase the chance that human text \n",
    "                                    that contains a slightly higher than average number of green tokens is erroneously flagged. \n",
    "                                    4.0-5.0 offers extremely low false positive rates while still accurately catching most watermarked text.\n",
    "                - Ignore Bigram Repeats : This alternate detection algorithm only considers the unique bigrams in the text during detection, \n",
    "                                        computing the greenlists based on the first in each pair and checking whether the second falls within the list.\n",
    "                                        This means that `T` is now the unique number of bigrams in the text, which becomes less than the total\n",
    "                                        number of tokens generated if the text contains a lot of repetition. See the paper for a more detailed discussion.\n",
    "                - Normalizations : we implement a few basic normaliations to defend against various adversarial perturbations of the\n",
    "                                    text analyzed during detection. Currently we support converting all chracters to unicode, \n",
    "                                    replacing homoglyphs with a canonical form, and standardizing the capitalization. \n",
    "                                    See the paper for a detailed discussion of input normalization. \n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        gr.HTML(\"\"\"\n",
    "                <p>For faster inference without waiting in queue, you may duplicate the space and upgrade to GPU in settings. \n",
    "                    Follow the github link at the top and host the demo on your own GPU hardware to test out larger models.\n",
    "                <br/>\n",
    "                <a href=\"https://huggingface.co/spaces/tomg-group-umd/lm-watermarking?duplicate=true\">\n",
    "                <img style=\"margin-top: 0em; margin-bottom: 0em\" src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\"></a>\n",
    "                <p/>\n",
    "                \"\"\")\n",
    "\n",
    "        # Register main generation tab click, outputing generations as well as a the encoded+redecoded+potentially truncated prompt and flag\n",
    "        generate_btn.click(fn=generate_partial, inputs=[prompt, session_args],\n",
    "                           outputs=[redecoded_input, truncation_warning, output_without_watermark,\n",
    "                                    output_with_watermark, session_args])\n",
    "        # Show truncated version of prompt if truncation occurred\n",
    "        redecoded_input.change(fn=truncate_prompt, inputs=[redecoded_input, truncation_warning, prompt, session_args],\n",
    "                               outputs=[prompt, session_args])\n",
    "        # Call detection when the outputs (of the generate function) are updated\n",
    "        output_without_watermark.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                        outputs=[without_watermark_detection_result, session_args])\n",
    "        output_with_watermark.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                     outputs=[with_watermark_detection_result, session_args])\n",
    "        # Register main detection tab click\n",
    "        detect_btn.click(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                         outputs=[detection_result, session_args])\n",
    "\n",
    "        # State management logic\n",
    "        # update callbacks that change the state dict\n",
    "        def update_sampling_temp(session_state, value):\n",
    "            session_state.sampling_temp = float(value); return session_state\n",
    "\n",
    "        def update_generation_seed(session_state, value):\n",
    "            session_state.generation_seed = int(value); return session_state\n",
    "\n",
    "        def update_gamma(session_state, value):\n",
    "            session_state.gamma = float(value); return session_state\n",
    "\n",
    "        def update_delta(session_state, value):\n",
    "            session_state.delta = float(value); return session_state\n",
    "\n",
    "        def update_detection_z_threshold(session_state, value):\n",
    "            session_state.detection_z_threshold = float(value); return session_state\n",
    "\n",
    "        def update_decoding(session_state, value):\n",
    "            if value == \"multinomial\":\n",
    "                session_state.use_sampling = True\n",
    "            elif value == \"greedy\":\n",
    "                session_state.use_sampling = False\n",
    "            return session_state\n",
    "\n",
    "        def toggle_sampling_vis(value):\n",
    "            if value == \"multinomial\":\n",
    "                return gr.update(visible=True)\n",
    "            elif value == \"greedy\":\n",
    "                return gr.update(visible=False)\n",
    "\n",
    "        def toggle_sampling_vis_inv(value):\n",
    "            if value == \"multinomial\":\n",
    "                return gr.update(visible=False)\n",
    "            elif value == \"greedy\":\n",
    "                return gr.update(visible=True)\n",
    "\n",
    "        def update_n_beams(session_state, value):\n",
    "            session_state.n_beams = value; return session_state\n",
    "\n",
    "        def update_max_new_tokens(session_state, value):\n",
    "            session_state.max_new_tokens = int(value); return session_state\n",
    "\n",
    "        def update_ignore_repeated_bigrams(session_state, value):\n",
    "            session_state.ignore_repeated_bigrams = value; return session_state\n",
    "\n",
    "        def update_normalizers(session_state, value):\n",
    "            session_state.normalizers = value; return session_state\n",
    "\n",
    "        def update_seed_separately(session_state, value):\n",
    "            session_state.seed_separately = value; return session_state\n",
    "\n",
    "        def update_select_green_tokens(session_state, value):\n",
    "            session_state.select_green_tokens = value; return session_state\n",
    "\n",
    "        # registering callbacks for toggling the visibilty of certain parameters\n",
    "        decoding.change(toggle_sampling_vis, inputs=[decoding], outputs=[sampling_temp])\n",
    "        decoding.change(toggle_sampling_vis, inputs=[decoding], outputs=[generation_seed])\n",
    "        decoding.change(toggle_sampling_vis_inv, inputs=[decoding], outputs=[n_beams])\n",
    "        # registering all state update callbacks\n",
    "        decoding.change(update_decoding, inputs=[session_args, decoding], outputs=[session_args])\n",
    "        sampling_temp.change(update_sampling_temp, inputs=[session_args, sampling_temp], outputs=[session_args])\n",
    "        generation_seed.change(update_generation_seed, inputs=[session_args, generation_seed], outputs=[session_args])\n",
    "        n_beams.change(update_n_beams, inputs=[session_args, n_beams], outputs=[session_args])\n",
    "        max_new_tokens.change(update_max_new_tokens, inputs=[session_args, max_new_tokens], outputs=[session_args])\n",
    "        gamma.change(update_gamma, inputs=[session_args, gamma], outputs=[session_args])\n",
    "        delta.change(update_delta, inputs=[session_args, delta], outputs=[session_args])\n",
    "        detection_z_threshold.change(update_detection_z_threshold, inputs=[session_args, detection_z_threshold],\n",
    "                                     outputs=[session_args])\n",
    "        ignore_repeated_bigrams.change(update_ignore_repeated_bigrams, inputs=[session_args, ignore_repeated_bigrams],\n",
    "                                       outputs=[session_args])\n",
    "        normalizers.change(update_normalizers, inputs=[session_args, normalizers], outputs=[session_args])\n",
    "        seed_separately.change(update_seed_separately, inputs=[session_args, seed_separately], outputs=[session_args])\n",
    "        select_green_tokens.change(update_select_green_tokens, inputs=[session_args, select_green_tokens],\n",
    "                                   outputs=[session_args])\n",
    "        # register additional callback on button clicks that updates the shown parameters window\n",
    "        generate_btn.click(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        detect_btn.click(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        # When the parameters change, display the update and fire detection, since some detection params dont change the model output.\n",
    "        gamma.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        gamma.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                     outputs=[without_watermark_detection_result, session_args])\n",
    "        gamma.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                     outputs=[with_watermark_detection_result, session_args])\n",
    "        gamma.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                     outputs=[detection_result, session_args])\n",
    "        detection_z_threshold.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                     outputs=[without_watermark_detection_result, session_args])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                     outputs=[with_watermark_detection_result, session_args])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                     outputs=[detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                       outputs=[without_watermark_detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                       outputs=[with_watermark_detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                       outputs=[detection_result, session_args])\n",
    "        normalizers.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        normalizers.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                           outputs=[without_watermark_detection_result, session_args])\n",
    "        normalizers.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                           outputs=[with_watermark_detection_result, session_args])\n",
    "        normalizers.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                           outputs=[detection_result, session_args])\n",
    "        select_green_tokens.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                   outputs=[without_watermark_detection_result, session_args])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                   outputs=[with_watermark_detection_result, session_args])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                   outputs=[detection_result, session_args])\n",
    "\n",
    "    demo.queue(concurrency_count=3)\n",
    "\n",
    "    if args.demo_public:\n",
    "        demo.launch(share=True)  # exposes app to the internet via randomly generated link\n",
    "    else:\n",
    "        demo.launch()\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Run a command line version of the generation and detection operations\n",
    "        and optionally launch and serve the gradio demo\"\"\"\n",
    "    # Initial arg processing and log\n",
    "    args.normalizers = (args.normalizers.split(\",\") if args.normalizers else [])\n",
    "    print(args)\n",
    "\n",
    "    if not args.skip_model_load:\n",
    "        model, tokenizer, device = load_model(args)\n",
    "    else:\n",
    "        model, tokenizer, device = None, None, None\n",
    "\n",
    "    # Generate and detect, report to stdout\n",
    "    if not args.skip_model_load:\n",
    "        input_text = \"The United States has 63 national parks, which are congressionally designated protected areas operated by the National Park Service, an agency of the Department of the Interior.[1] National parks are designated for their natural beauty, unique geological features, diverse ecosystems, and recreational opportunities, typically 'because of some outstanding scenic feature or natural phenomena.'[2] While legislatively all units of the National Park System are considered equal with the same mission, national parks are generally larger and more of a destination, and hunting and extractive activities are prohibited.[3] National monuments, on the other hand, are also frequently protected for their historical or archaeological significance. Eight national parks (including six in Alaska) are paired with a national preserve, areas with different levels of protection that are administered together but considered separate units and whose areas are not included in the figures below. The 433 units of the National Park System can be broadly referred to as national parks, but most have other formal designations.[4]\"\n",
    "\n",
    "        model, tokenizer, device = load_model(args)\n",
    "\n",
    "        args.default_prompt = input_text\n",
    "\n",
    "        term_width = 80\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Prompt:\")\n",
    "        print(input_text)\n",
    "\n",
    "        _, _, decoded_output_without_watermark, decoded_output_with_watermark, _ = generate_kgw(input_text,\n",
    "                                                                                            args,\n",
    "                                                                                            model=model,\n",
    "                                                                                            device=device,\n",
    "                                                                                            tokenizer=tokenizer)\n",
    "        without_watermark_detection_result = detect_kgw(decoded_output_without_watermark,\n",
    "                                                    args,\n",
    "                                                    device=device,\n",
    "                                                    tokenizer=tokenizer)\n",
    "        with_watermark_detection_result = detect_kgw(decoded_output_with_watermark,\n",
    "                                                 args,\n",
    "                                                 device=device,\n",
    "                                                 tokenizer=tokenizer)\n",
    "\n",
    "        def compute_perplexity(model, tokenizer, text: str, device: torch.device) -> float:\n",
    "            \"\"\"\n",
    "            Computes the perplexity of a given text using the provided model.\n",
    "            Assumes a causal language model where providing labels yields the loss.\n",
    "            \"\"\"\n",
    "            model.eval()\n",
    "            # Tokenize the text with special tokens.\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                # When labels are the same as input_ids, the loss is the negative log-likelihood.\n",
    "                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                loss = outputs.loss  # this is the average loss per token\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            return perplexity\n",
    "\n",
    "        # Compute perplexity for both watermarked and non-watermarked outputs:\n",
    "        perplexity_nonwm = compute_perplexity(model, tokenizer, decoded_output_without_watermark, device)\n",
    "        perplexity_wm = compute_perplexity(model, tokenizer, decoded_output_with_watermark, device)\n",
    "\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Output without watermark:\")\n",
    "        print(decoded_output_without_watermark)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Detection result @ {args.detection_z_threshold}:\")\n",
    "        pprint(without_watermark_detection_result)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Perplexity (non-watermarked): {perplexity_nonwm:.2f}\")\n",
    "\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Output with watermark:\")\n",
    "        print(decoded_output_with_watermark)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Detection result @ {args.detection_z_threshold}:\")\n",
    "        print(with_watermark_detection_result[1][0])\n",
    "        pprint(with_watermark_detection_result)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Perplexity (watermarked): {perplexity_wm:.2f}\")\n",
    "\n",
    "    # Launch the app to generate and detect interactively (implements the hf space demo)\n",
    "    #if args.run_gradio:\n",
    "    #    run_gradio(args, model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "    return\n",
    "\n",
    "def normalization_strategy_lookup(strategy_name: str) -> object:\n",
    "    if strategy_name == \"unicode\":\n",
    "        return UnicodeSanitizer()\n",
    "    elif strategy_name == \"homoglyphs\":\n",
    "        return HomoglyphCanonizer()\n",
    "    elif strategy_name == \"truecase\":\n",
    "        return TrueCaser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b312695c2940c5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T05:09:39.991935Z",
     "start_time": "2025-03-23T05:09:39.961702Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n",
    "# available at https://arxiv.org/abs/2301.10226\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "\n",
    "import numpy  # for gradio hot reload\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSeq2SeqLM,\n",
    "                          AutoModelForCausalLM,\n",
    "                          LogitsProcessorList)\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    \"\"\"Util function for user friendly boolean flag args\"\"\"\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Command line argument specification\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"A minimum working example of applying the watermark to any LLM that supports the huggingface 🤗 `generate` API\")\n",
    "\n",
    "    #parser.add_argument(\n",
    "    #    \"--run_gradio\",\n",
    "    #    type=str2bool,\n",
    "    #    default=False,\n",
    "    #    help=\"Whether to launch as a gradio demo. Set to False if not installed and want to just run the stdout version.\",\n",
    "    #)\n",
    "    parser.add_argument(\n",
    "        \"--demo_public\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to expose the gradio demo to the internet.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        default=\"facebook/opt-1.3b\",#\"gpt2-medium\",#\n",
    "        help=\"Main model, path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prompt_max_length\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Truncation length for prompt, overrides model config's max length field.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_new_tokens\",\n",
    "        type=int,\n",
    "        default=200,\n",
    "        help=\"Maximmum number of new tokens to generate.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generation_seed\",\n",
    "        type=int,\n",
    "        default=123,\n",
    "        help=\"Seed for setting the torch global rng prior to generation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_sampling\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to generate using multinomial sampling.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sampling_temp\",\n",
    "        type=float,\n",
    "        default=0.7,\n",
    "        help=\"Sampling temperature to use when generating using multinomial sampling.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_beams\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of beams to use for beam search. 1 is normal greedy decoding\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_gpu\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to run inference and watermark hashing/seeding/permutation on gpu.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seeding_scheme\",\n",
    "        type=str,\n",
    "        default=\"simple_1\",\n",
    "        help=\"Seeding scheme to use to generate the greenlists at each generation and verification step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gamma\",\n",
    "        type=float,\n",
    "        default=0.25,\n",
    "        help=\"The fraction of the vocabulary to partition into the greenlist at each generation and verification step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--delta\",\n",
    "        type=float,\n",
    "        default=2.0,\n",
    "        help=\"The amount/bias to add to each of the greenlist token logits before each token sampling step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--normalizers\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Single or comma separated list of the preprocessors/normalizer names to use when performing watermark detection.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ignore_repeated_bigrams\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to use the detection method that only counts each unqiue bigram once as either a green or red hit.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--detection_z_threshold\",\n",
    "        type=float,\n",
    "        default=4.0,\n",
    "        help=\"The test statistic threshold for the detection hypothesis test.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--select_green_tokens\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"How to treat the permuation when selecting the greenlist tokens at each step. Legacy is (False) to pick the complement/reds first.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip_model_load\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Skip the model loading to debug the interface.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed_separately\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Whether to call the torch seed function before both the unwatermarked and watermarked generate calls.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_fp16\",\n",
    "        type=str2bool,\n",
    "        default=False,\n",
    "        help=\"Whether to run model in float16 precsion.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_model(args):\n",
    "    \"\"\"Load and return the model and tokenizer\"\"\"\n",
    "\n",
    "    args.is_seq2seq_model = any([(model_type in args.model_name_or_path) for model_type in [\"t5\", \"T0\"]])\n",
    "    args.is_decoder_only_model = any(\n",
    "        [(model_type in args.model_name_or_path) for model_type in [\"gpt\", \"opt\", \"bloom\"]])\n",
    "    if args.is_seq2seq_model:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n",
    "    elif args.is_decoder_only_model:\n",
    "        if args.load_fp16:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, torch_dtype=torch.float16,\n",
    "                                                         device_map='auto')\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {args.model_name_or_path}\")\n",
    "\n",
    "    if args.use_gpu:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if args.load_fp16:\n",
    "            pass\n",
    "        else:\n",
    "            model = model.to(device)\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "def generate_kgw(prompt, args, model=None, device=None, tokenizer=None):\n",
    "    \"\"\"Instatiate the WatermarkLogitsProcessor according to the watermark parameters\n",
    "       and generate watermarked text by passing it to the generate method of the model\n",
    "       as a logits processor. \"\"\"\n",
    "\n",
    "    #print(f\"Generating with {args}\")\n",
    "\n",
    "    watermark_processor = WatermarkLogitsProcessor_kgw(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                                   gamma=args.gamma,\n",
    "                                                   delta=args.delta,\n",
    "                                                   seeding_scheme=args.seeding_scheme,\n",
    "                                                   select_green_tokens=args.select_green_tokens)\n",
    "\n",
    "    gen_kwargs = dict(max_new_tokens=args.max_new_tokens)\n",
    "\n",
    "    if args.use_sampling:\n",
    "        gen_kwargs.update(dict(\n",
    "            do_sample=True,\n",
    "            top_k=0,\n",
    "            temperature=args.sampling_temp\n",
    "        ))\n",
    "    else:\n",
    "        gen_kwargs.update(dict(\n",
    "            num_beams=args.n_beams\n",
    "        ))\n",
    "\n",
    "    generate_without_watermark = partial(\n",
    "        model.generate,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    generate_with_watermark = partial(\n",
    "        model.generate,\n",
    "        logits_processor=LogitsProcessorList([watermark_processor]),\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    if args.prompt_max_length:\n",
    "        pass\n",
    "    elif hasattr(model.config, \"max_position_embedding\"):\n",
    "        args.prompt_max_length = model.config.max_position_embeddings - args.max_new_tokens\n",
    "    else:\n",
    "        args.prompt_max_length = 2048 - args.max_new_tokens\n",
    "\n",
    "    tokd_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True, truncation=True,\n",
    "                           max_length=args.prompt_max_length).to(device)\n",
    "    truncation_warning = True if tokd_input[\"input_ids\"].shape[-1] == args.prompt_max_length else False\n",
    "    redecoded_input = tokenizer.batch_decode(tokd_input[\"input_ids\"], skip_special_tokens=True)[0]\n",
    "\n",
    "    torch.manual_seed(args.generation_seed)\n",
    "    output_without_watermark = generate_without_watermark(**tokd_input)\n",
    "\n",
    "    # optional to seed before second generation, but will not be the same again generally, unless delta==0.0, no-op watermark\n",
    "    if args.seed_separately:\n",
    "        torch.manual_seed(args.generation_seed)\n",
    "    output_with_watermark = generate_with_watermark(**tokd_input)\n",
    "\n",
    "    if args.is_decoder_only_model:\n",
    "        # need to isolate the newly generated tokens\n",
    "        output_without_watermark = output_without_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "        output_with_watermark = output_with_watermark[:, tokd_input[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    decoded_output_without_watermark = tokenizer.batch_decode(output_without_watermark, skip_special_tokens=True)[0]\n",
    "    decoded_output_with_watermark = tokenizer.batch_decode(output_with_watermark, skip_special_tokens=True)[0]\n",
    "\n",
    "    return (redecoded_input,\n",
    "            int(truncation_warning),\n",
    "            decoded_output_without_watermark,\n",
    "            decoded_output_with_watermark,\n",
    "            args)\n",
    "    # decoded_output_with_watermark)\n",
    "\n",
    "\n",
    "def format_names(s):\n",
    "    \"\"\"Format names for the gradio demo interface\"\"\"\n",
    "    s = s.replace(\"num_tokens_scored\", \"Tokens Counted (T)\")\n",
    "    s = s.replace(\"num_green_tokens\", \"# Tokens in Greenlist\")\n",
    "    s = s.replace(\"green_fraction\", \"Fraction of T in Greenlist\")\n",
    "    s = s.replace(\"z_score\", \"z-score\")\n",
    "    s = s.replace(\"p_value\", \"p value\")\n",
    "    s = s.replace(\"prediction\", \"Prediction\")\n",
    "    s = s.replace(\"confidence\", \"Confidence\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def list_format_scores(score_dict, detection_threshold):\n",
    "    \"\"\"Format the detection metrics into a gradio dataframe input format\"\"\"\n",
    "    lst_2d = []\n",
    "    # lst_2d.append([\"z-score threshold\", f\"{detection_threshold}\"])\n",
    "    for k, v in score_dict.items():\n",
    "        if k == 'green_fraction':\n",
    "            lst_2d.append([format_names(k), f\"{v:.1%}\"])\n",
    "        elif k == 'confidence':\n",
    "            lst_2d.append([format_names(k), f\"{v:.3%}\"])\n",
    "        elif isinstance(v, float):\n",
    "            lst_2d.append([format_names(k), f\"{v:.3g}\"])\n",
    "        elif isinstance(v, bool):\n",
    "            lst_2d.append([format_names(k), (\"Watermarked\" if v else \"Human/Unwatermarked\")])\n",
    "        else:\n",
    "            lst_2d.append([format_names(k), f\"{v}\"])\n",
    "    if \"confidence\" in score_dict:\n",
    "        lst_2d.insert(-2, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    else:\n",
    "        lst_2d.insert(-1, [\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    return lst_2d\n",
    "\n",
    "\n",
    "def detect_kgw(input_text, args, device=None, tokenizer=None):\n",
    "    \"\"\"Instantiate the WatermarkDetection object and call detect on\n",
    "        the input text returning the scores and outcome of the test\"\"\"\n",
    "    watermark_detector = WatermarkDetector_kgw(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                           gamma=args.gamma,\n",
    "                                           seeding_scheme=args.seeding_scheme,\n",
    "                                           device=device,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           z_threshold=args.detection_z_threshold,\n",
    "                                           normalizers=args.normalizers,\n",
    "                                           ignore_repeated_bigrams=args.ignore_repeated_bigrams,\n",
    "                                           select_green_tokens=args.select_green_tokens)\n",
    "    if len(input_text) - 1 > watermark_detector.min_prefix_len:\n",
    "        score_dict = watermark_detector.detect(input_text)\n",
    "        # output = str_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "        output = list_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "    else:\n",
    "        score_dict = None\n",
    "        # output = (f\"Error: string not long enough to compute watermark presence.\")\n",
    "        output = [[\"Error\", \"string too short to compute metrics\"]]\n",
    "        output += [[\"\", \"\"] for _ in range(6)]\n",
    "    return output, score_dict, args\n",
    "\n",
    "\n",
    "def run_gradio(args, model=None, device=None, tokenizer=None):\n",
    "    \"\"\"Define and launch the gradio demo interface\"\"\"\n",
    "    generate_partial = partial(generate_kgw, model=model, device=device, tokenizer=tokenizer)\n",
    "    detect_partial = partial(detect_kgw, device=device, tokenizer=tokenizer)\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        # Top section, greeting and instructions\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=9):\n",
    "                gr.Markdown(\n",
    "                    \"\"\"\n",
    "                    ## 💧 [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226) 🔍\n",
    "                    \"\"\"\n",
    "                )\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\n",
    "                    \"\"\"\n",
    "                    [![](https://badgen.net/badge/icon/GitHub?icon=github&label)](https://github.com/jwkirchenbauer/lm-watermarking)\n",
    "                    \"\"\"\n",
    "                )\n",
    "            # with gr.Column(scale=2):\n",
    "            #     pass\n",
    "            # ![visitor badge](https://visitor-badge.glitch.me/badge?page_id=tomg-group-umd_lm-watermarking) # buggy\n",
    "\n",
    "        with gr.Accordion(\"Understanding the output metrics\", open=False):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                - `z-score threshold` : The cuttoff for the hypothesis test\n",
    "                - `Tokens Counted (T)` : The number of tokens in the output that were counted by the detection algorithm. \n",
    "                    The first token is ommitted in the simple, single token seeding scheme since there is no way to generate\n",
    "                    a greenlist for it as it has no prefix token(s). Under the \"Ignore Bigram Repeats\" detection algorithm, \n",
    "                    described in the bottom panel, this can be much less than the total number of tokens generated if there is a lot of repetition.\n",
    "                - `# Tokens in Greenlist` : The number of tokens that were observed to fall in their respective greenlist\n",
    "                - `Fraction of T in Greenlist` : The `# Tokens in Greenlist` / `T`. This is expected to be approximately `gamma` for human/unwatermarked text.\n",
    "                - `z-score` : The test statistic for the detection hypothesis test. If larger than the `z-score threshold` \n",
    "                    we \"reject the null hypothesis\" that the text is human/unwatermarked, and conclude it is watermarked\n",
    "                - `p value` : The likelihood of observing the computed `z-score` under the null hypothesis. This is the likelihood of \n",
    "                    observing the `Fraction of T in Greenlist` given that the text was generated without knowledge of the watermark procedure/greenlists.\n",
    "                    If this is extremely _small_ we are confident that this many green tokens was not chosen by random chance.\n",
    "                -  `prediction` : The outcome of the hypothesis test - whether the observed `z-score` was higher than the `z-score threshold`\n",
    "                - `confidence` : If we reject the null hypothesis, and the `prediction` is \"Watermarked\", then we report 1-`p value` to represent \n",
    "                    the confidence of the detection based on the unlikeliness of this `z-score` observation.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        with gr.Accordion(\"A note on model capability\", open=True):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                This demo uses open-source language models that fit on a single GPU. These models are less powerful than proprietary commercial tools like ChatGPT, Claude, or Bard. \n",
    "\n",
    "                Importantly, we use a language model that is designed to \"complete\" your prompt, and not a model this is fine-tuned to follow instructions. \n",
    "                For best results, prompt the model with a few sentences that form the beginning of a paragraph, and then allow it to \"continue\" your paragraph. \n",
    "                Some examples include the opening paragraph of a wikipedia article, or the first few sentences of a story. \n",
    "                Longer prompts that end mid-sentence will result in more fluent generations.\n",
    "                \"\"\"\n",
    "            )\n",
    "        gr.Markdown(f\"Language model: {args.model_name_or_path} {'(float16 mode)' if args.load_fp16 else ''}\")\n",
    "\n",
    "        # Construct state for parameters, define updates and toggles\n",
    "        default_prompt = args.__dict__.pop(\"default_prompt\")\n",
    "        session_args = gr.State(value=args)\n",
    "\n",
    "        with gr.Tab(\"Generate and Detect\"):\n",
    "\n",
    "            with gr.Row():\n",
    "                prompt = gr.Textbox(label=f\"Prompt\", interactive=True, lines=10, max_lines=10, value=default_prompt)\n",
    "            with gr.Row():\n",
    "                generate_btn = gr.Button(\"Generate\")\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    output_without_watermark = gr.Textbox(label=\"Output Without Watermark\", interactive=False, lines=14,\n",
    "                                                          max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # without_watermark_detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    without_watermark_detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False,\n",
    "                                                                      row_count=7, col_count=2)\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    output_with_watermark = gr.Textbox(label=\"Output With Watermark\", interactive=False, lines=14,\n",
    "                                                       max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # with_watermark_detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    with_watermark_detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False,\n",
    "                                                                   row_count=7, col_count=2)\n",
    "\n",
    "            redecoded_input = gr.Textbox(visible=False)\n",
    "            truncation_warning = gr.Number(visible=False)\n",
    "\n",
    "            def truncate_prompt(redecoded_input, truncation_warning, orig_prompt, args):\n",
    "                if truncation_warning:\n",
    "                    return redecoded_input + f\"\\n\\n[Prompt was truncated before generation due to length...]\", args\n",
    "                else:\n",
    "                    return orig_prompt, args\n",
    "\n",
    "        with gr.Tab(\"Detector Only\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    detection_input = gr.Textbox(label=\"Text to Analyze\", interactive=True, lines=14, max_lines=14)\n",
    "                with gr.Column(scale=1):\n",
    "                    # detection_result = gr.Textbox(label=\"Detection Result\", interactive=False,lines=14,max_lines=14)\n",
    "                    detection_result = gr.Dataframe(headers=[\"Metric\", \"Value\"], interactive=False, row_count=7,\n",
    "                                                    col_count=2)\n",
    "            with gr.Row():\n",
    "                detect_btn = gr.Button(\"Detect\")\n",
    "\n",
    "        # Parameter selection group\n",
    "        with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(f\"#### Generation Parameters\")\n",
    "                    with gr.Row():\n",
    "                        decoding = gr.Radio(label=\"Decoding Method\", choices=[\"multinomial\", \"greedy\"],\n",
    "                                            value=(\"multinomial\" if args.use_sampling else \"greedy\"))\n",
    "                    with gr.Row():\n",
    "                        sampling_temp = gr.Slider(label=\"Sampling Temperature\", minimum=0.1, maximum=1.0, step=0.1,\n",
    "                                                  value=args.sampling_temp, visible=True)\n",
    "                    with gr.Row():\n",
    "                        generation_seed = gr.Number(label=\"Generation Seed\", value=args.generation_seed,\n",
    "                                                    interactive=True)\n",
    "                    with gr.Row():\n",
    "                        n_beams = gr.Dropdown(label=\"Number of Beams\", choices=list(range(1, 11, 1)),\n",
    "                                              value=args.n_beams, visible=(not args.use_sampling))\n",
    "                    with gr.Row():\n",
    "                        max_new_tokens = gr.Slider(label=\"Max Generated Tokens\", minimum=10, maximum=1000, step=10,\n",
    "                                                   value=args.max_new_tokens)\n",
    "\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(f\"#### Watermark Parameters\")\n",
    "                    with gr.Row():\n",
    "                        gamma = gr.Slider(label=\"gamma\", minimum=0.1, maximum=0.9, step=0.05, value=args.gamma)\n",
    "                    with gr.Row():\n",
    "                        delta = gr.Slider(label=\"delta\", minimum=0.0, maximum=10.0, step=0.1, value=args.delta)\n",
    "                    gr.Markdown(f\"#### Detector Parameters\")\n",
    "                    with gr.Row():\n",
    "                        detection_z_threshold = gr.Slider(label=\"z-score threshold\", minimum=0.0, maximum=10.0,\n",
    "                                                          step=0.1, value=args.detection_z_threshold)\n",
    "                    with gr.Row():\n",
    "                        ignore_repeated_bigrams = gr.Checkbox(label=\"Ignore Bigram Repeats\")\n",
    "                    with gr.Row():\n",
    "                        normalizers = gr.CheckboxGroup(label=\"Normalizations\",\n",
    "                                                       choices=[\"unicode\", \"homoglyphs\", \"truecase\"],\n",
    "                                                       value=args.normalizers)\n",
    "            # with gr.Accordion(\"Actual submitted parameters:\",open=False):\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\n",
    "                    f\"_Note: sliders don't always update perfectly. Clicking on the bar or using the number window to the right can help. Window below shows the current settings._\")\n",
    "            with gr.Row():\n",
    "                current_parameters = gr.Textbox(label=\"Current Parameters\", value=args)\n",
    "            with gr.Accordion(\"Legacy Settings\", open=False):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        seed_separately = gr.Checkbox(label=\"Seed both generations separately\",\n",
    "                                                      value=args.seed_separately)\n",
    "                    with gr.Column(scale=1):\n",
    "                        select_green_tokens = gr.Checkbox(label=\"Select 'greenlist' from partition\",\n",
    "                                                          value=args.select_green_tokens)\n",
    "\n",
    "        with gr.Accordion(\"Understanding the settings\", open=False):\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                #### Generation Parameters:\n",
    "    \n",
    "                - Decoding Method : We can generate tokens from the model using either multinomial sampling or we can generate using greedy decoding.\n",
    "                - Sampling Temperature : If using multinomial sampling we can set the temperature of the sampling distribution. \n",
    "                                    0.0 is equivalent to greedy decoding, and 1.0 is the maximum amount of variability/entropy in the next token distribution.\n",
    "                                    0.7 strikes a nice balance between faithfulness to the model's estimate of top candidates while adding variety. Does not apply for greedy decoding.\n",
    "                - Generation Seed : The integer to pass to the torch random number generator before running generation. Makes the multinomial sampling strategy\n",
    "                                    outputs reproducible. Does not apply for greedy decoding.\n",
    "                - Number of Beams : When using greedy decoding, we can also set the number of beams to > 1 to enable beam search. \n",
    "                                    This is not implemented/excluded from paper for multinomial sampling but may be added in future.\n",
    "                - Max Generated Tokens : The `max_new_tokens` parameter passed to the generation method to stop the output at a certain number of new tokens. \n",
    "                                        Note that the model is free to generate fewer tokens depending on the prompt. \n",
    "                                        Implicitly this sets the maximum number of prompt tokens possible as the model's maximum input length minus `max_new_tokens`,\n",
    "                                        and inputs will be truncated accordingly.\n",
    "    \n",
    "                #### Watermark Parameters:\n",
    "    \n",
    "                - gamma : The fraction of the vocabulary to be partitioned into the greenlist at each generation step. \n",
    "                         Smaller gamma values create a stronger watermark by enabling the watermarked model to achieve \n",
    "                         a greater differentiation from human/unwatermarked text because it is preferentially sampling \n",
    "                         from a smaller green set making those tokens less likely to occur by chance.\n",
    "                - delta : The amount of positive bias to add to the logits of every token in the greenlist \n",
    "                            at each generation step before sampling/choosing the next token. Higher delta values \n",
    "                            mean that the greenlist tokens are more heavily preferred by the watermarked model\n",
    "                            and as the bias becomes very large the watermark transitions from \"soft\" to \"hard\". \n",
    "                            For a hard watermark, nearly all tokens are green, but this can have a detrimental effect on\n",
    "                            generation quality, especially when there is not a lot of flexibility in the distribution.\n",
    "    \n",
    "                #### Detector Parameters:\n",
    "    \n",
    "                - z-score threshold : the z-score cuttoff for the hypothesis test. Higher thresholds (such as 4.0) make\n",
    "                                    _false positives_ (predicting that human/unwatermarked text is watermarked) very unlikely\n",
    "                                    as a genuine human text with a significant number of tokens will almost never achieve \n",
    "                                    that high of a z-score. Lower thresholds will capture more _true positives_ as some watermarked\n",
    "                                    texts will contain less green tokens and achive a lower z-score, but still pass the lower bar and \n",
    "                                    be flagged as \"watermarked\". However, a lowere threshold will increase the chance that human text \n",
    "                                    that contains a slightly higher than average number of green tokens is erroneously flagged. \n",
    "                                    4.0-5.0 offers extremely low false positive rates while still accurately catching most watermarked text.\n",
    "                - Ignore Bigram Repeats : This alternate detection algorithm only considers the unique bigrams in the text during detection, \n",
    "                                        computing the greenlists based on the first in each pair and checking whether the second falls within the list.\n",
    "                                        This means that `T` is now the unique number of bigrams in the text, which becomes less than the total\n",
    "                                        number of tokens generated if the text contains a lot of repetition. See the paper for a more detailed discussion.\n",
    "                - Normalizations : we implement a few basic normaliations to defend against various adversarial perturbations of the\n",
    "                                    text analyzed during detection. Currently we support converting all chracters to unicode, \n",
    "                                    replacing homoglyphs with a canonical form, and standardizing the capitalization. \n",
    "                                    See the paper for a detailed discussion of input normalization. \n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        gr.HTML(\"\"\"\n",
    "                <p>For faster inference without waiting in queue, you may duplicate the space and upgrade to GPU in settings. \n",
    "                    Follow the github link at the top and host the demo on your own GPU hardware to test out larger models.\n",
    "                <br/>\n",
    "                <a href=\"https://huggingface.co/spaces/tomg-group-umd/lm-watermarking?duplicate=true\">\n",
    "                <img style=\"margin-top: 0em; margin-bottom: 0em\" src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\"></a>\n",
    "                <p/>\n",
    "                \"\"\")\n",
    "\n",
    "        # Register main generation tab click, outputing generations as well as a the encoded+redecoded+potentially truncated prompt and flag\n",
    "        generate_btn.click(fn=generate_partial, inputs=[prompt, session_args],\n",
    "                           outputs=[redecoded_input, truncation_warning, output_without_watermark,\n",
    "                                    output_with_watermark, session_args])\n",
    "        # Show truncated version of prompt if truncation occurred\n",
    "        redecoded_input.change(fn=truncate_prompt, inputs=[redecoded_input, truncation_warning, prompt, session_args],\n",
    "                               outputs=[prompt, session_args])\n",
    "        # Call detection when the outputs (of the generate function) are updated\n",
    "        output_without_watermark.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                        outputs=[without_watermark_detection_result, session_args])\n",
    "        output_with_watermark.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                     outputs=[with_watermark_detection_result, session_args])\n",
    "        # Register main detection tab click\n",
    "        detect_btn.click(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                         outputs=[detection_result, session_args])\n",
    "\n",
    "        # State management logic\n",
    "        # update callbacks that change the state dict\n",
    "        def update_sampling_temp(session_state, value):\n",
    "            session_state.sampling_temp = float(value); return session_state\n",
    "\n",
    "        def update_generation_seed(session_state, value):\n",
    "            session_state.generation_seed = int(value); return session_state\n",
    "\n",
    "        def update_gamma(session_state, value):\n",
    "            session_state.gamma = float(value); return session_state\n",
    "\n",
    "        def update_delta(session_state, value):\n",
    "            session_state.delta = float(value); return session_state\n",
    "\n",
    "        def update_detection_z_threshold(session_state, value):\n",
    "            session_state.detection_z_threshold = float(value); return session_state\n",
    "\n",
    "        def update_decoding(session_state, value):\n",
    "            if value == \"multinomial\":\n",
    "                session_state.use_sampling = True\n",
    "            elif value == \"greedy\":\n",
    "                session_state.use_sampling = False\n",
    "            return session_state\n",
    "\n",
    "        def toggle_sampling_vis(value):\n",
    "            if value == \"multinomial\":\n",
    "                return gr.update(visible=True)\n",
    "            elif value == \"greedy\":\n",
    "                return gr.update(visible=False)\n",
    "\n",
    "        def toggle_sampling_vis_inv(value):\n",
    "            if value == \"multinomial\":\n",
    "                return gr.update(visible=False)\n",
    "            elif value == \"greedy\":\n",
    "                return gr.update(visible=True)\n",
    "\n",
    "        def update_n_beams(session_state, value):\n",
    "            session_state.n_beams = value; return session_state\n",
    "\n",
    "        def update_max_new_tokens(session_state, value):\n",
    "            session_state.max_new_tokens = int(value); return session_state\n",
    "\n",
    "        def update_ignore_repeated_bigrams(session_state, value):\n",
    "            session_state.ignore_repeated_bigrams = value; return session_state\n",
    "\n",
    "        def update_normalizers(session_state, value):\n",
    "            session_state.normalizers = value; return session_state\n",
    "\n",
    "        def update_seed_separately(session_state, value):\n",
    "            session_state.seed_separately = value; return session_state\n",
    "\n",
    "        def update_select_green_tokens(session_state, value):\n",
    "            session_state.select_green_tokens = value; return session_state\n",
    "\n",
    "        # registering callbacks for toggling the visibilty of certain parameters\n",
    "        decoding.change(toggle_sampling_vis, inputs=[decoding], outputs=[sampling_temp])\n",
    "        decoding.change(toggle_sampling_vis, inputs=[decoding], outputs=[generation_seed])\n",
    "        decoding.change(toggle_sampling_vis_inv, inputs=[decoding], outputs=[n_beams])\n",
    "        # registering all state update callbacks\n",
    "        decoding.change(update_decoding, inputs=[session_args, decoding], outputs=[session_args])\n",
    "        sampling_temp.change(update_sampling_temp, inputs=[session_args, sampling_temp], outputs=[session_args])\n",
    "        generation_seed.change(update_generation_seed, inputs=[session_args, generation_seed], outputs=[session_args])\n",
    "        n_beams.change(update_n_beams, inputs=[session_args, n_beams], outputs=[session_args])\n",
    "        max_new_tokens.change(update_max_new_tokens, inputs=[session_args, max_new_tokens], outputs=[session_args])\n",
    "        gamma.change(update_gamma, inputs=[session_args, gamma], outputs=[session_args])\n",
    "        delta.change(update_delta, inputs=[session_args, delta], outputs=[session_args])\n",
    "        detection_z_threshold.change(update_detection_z_threshold, inputs=[session_args, detection_z_threshold],\n",
    "                                     outputs=[session_args])\n",
    "        ignore_repeated_bigrams.change(update_ignore_repeated_bigrams, inputs=[session_args, ignore_repeated_bigrams],\n",
    "                                       outputs=[session_args])\n",
    "        normalizers.change(update_normalizers, inputs=[session_args, normalizers], outputs=[session_args])\n",
    "        seed_separately.change(update_seed_separately, inputs=[session_args, seed_separately], outputs=[session_args])\n",
    "        select_green_tokens.change(update_select_green_tokens, inputs=[session_args, select_green_tokens],\n",
    "                                   outputs=[session_args])\n",
    "        # register additional callback on button clicks that updates the shown parameters window\n",
    "        generate_btn.click(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        detect_btn.click(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        # When the parameters change, display the update and fire detection, since some detection params dont change the model output.\n",
    "        gamma.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        gamma.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                     outputs=[without_watermark_detection_result, session_args])\n",
    "        gamma.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                     outputs=[with_watermark_detection_result, session_args])\n",
    "        gamma.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                     outputs=[detection_result, session_args])\n",
    "        detection_z_threshold.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                     outputs=[without_watermark_detection_result, session_args])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                     outputs=[with_watermark_detection_result, session_args])\n",
    "        detection_z_threshold.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                     outputs=[detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                       outputs=[without_watermark_detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                       outputs=[with_watermark_detection_result, session_args])\n",
    "        ignore_repeated_bigrams.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                       outputs=[detection_result, session_args])\n",
    "        normalizers.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        normalizers.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                           outputs=[without_watermark_detection_result, session_args])\n",
    "        normalizers.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                           outputs=[with_watermark_detection_result, session_args])\n",
    "        normalizers.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                           outputs=[detection_result, session_args])\n",
    "        select_green_tokens.change(lambda value: str(value), inputs=[session_args], outputs=[current_parameters])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[output_without_watermark, session_args],\n",
    "                                   outputs=[without_watermark_detection_result, session_args])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[output_with_watermark, session_args],\n",
    "                                   outputs=[with_watermark_detection_result, session_args])\n",
    "        select_green_tokens.change(fn=detect_partial, inputs=[detection_input, session_args],\n",
    "                                   outputs=[detection_result, session_args])\n",
    "\n",
    "    demo.queue(concurrency_count=3)\n",
    "\n",
    "    if args.demo_public:\n",
    "        demo.launch(share=True)  # exposes app to the internet via randomly generated link\n",
    "    else:\n",
    "        demo.launch()\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Run a command line version of the generation and detection operations\n",
    "        and optionally launch and serve the gradio demo\"\"\"\n",
    "    # Initial arg processing and log\n",
    "    args.normalizers = (args.normalizers.split(\",\") if args.normalizers else [])\n",
    "    print(args)\n",
    "\n",
    "    if not args.skip_model_load:\n",
    "        model, tokenizer, device = load_model(args)\n",
    "    else:\n",
    "        model, tokenizer, device = None, None, None\n",
    "\n",
    "    # Generate and detect, report to stdout\n",
    "    if not args.skip_model_load:\n",
    "        input_text = \"The United States has 63 national parks, which are congressionally designated protected areas operated by the National Park Service, an agency of the Department of the Interior.[1] National parks are designated for their natural beauty, unique geological features, diverse ecosystems, and recreational opportunities, typically 'because of some outstanding scenic feature or natural phenomena.'[2] While legislatively all units of the National Park System are considered equal with the same mission, national parks are generally larger and more of a destination, and hunting and extractive activities are prohibited.[3] National monuments, on the other hand, are also frequently protected for their historical or archaeological significance. Eight national parks (including six in Alaska) are paired with a national preserve, areas with different levels of protection that are administered together but considered separate units and whose areas are not included in the figures below. The 433 units of the National Park System can be broadly referred to as national parks, but most have other formal designations.[4]\"\n",
    "\n",
    "        model, tokenizer, device = load_model(args)\n",
    "\n",
    "        args.default_prompt = input_text\n",
    "\n",
    "        term_width = 80\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Prompt:\")\n",
    "        print(input_text)\n",
    "\n",
    "        _, _, decoded_output_without_watermark, decoded_output_with_watermark, _ = generate_kgw(input_text,\n",
    "                                                                                            args,\n",
    "                                                                                            model=model,\n",
    "                                                                                            device=device,\n",
    "                                                                                            tokenizer=tokenizer)\n",
    "        without_watermark_detection_result = detect_kgw(decoded_output_without_watermark,\n",
    "                                                    args,\n",
    "                                                    device=device,\n",
    "                                                    tokenizer=tokenizer)\n",
    "        with_watermark_detection_result = detect_kgw(decoded_output_with_watermark,\n",
    "                                                 args,\n",
    "                                                 device=device,\n",
    "                                                 tokenizer=tokenizer)\n",
    "\n",
    "        def compute_perplexity(model, tokenizer, text: str, device: torch.device) -> float:\n",
    "            \"\"\"\n",
    "            Computes the perplexity of a given text using the provided model.\n",
    "            Assumes a causal language model where providing labels yields the loss.\n",
    "            \"\"\"\n",
    "            model.eval()\n",
    "            # Tokenize the text with special tokens.\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                # When labels are the same as input_ids, the loss is the negative log-likelihood.\n",
    "                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                loss = outputs.loss  # this is the average loss per token\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            return perplexity\n",
    "\n",
    "        # Compute perplexity for both watermarked and non-watermarked outputs:\n",
    "        perplexity_nonwm = compute_perplexity(model, tokenizer, decoded_output_without_watermark, device)\n",
    "        perplexity_wm = compute_perplexity(model, tokenizer, decoded_output_with_watermark, device)\n",
    "\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Output without watermark:\")\n",
    "        print(decoded_output_without_watermark)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Detection result @ {args.detection_z_threshold}:\")\n",
    "        pprint(without_watermark_detection_result)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Perplexity (non-watermarked): {perplexity_nonwm:.2f}\")\n",
    "\n",
    "        print(\"#\" * term_width)\n",
    "        print(\"Output with watermark:\")\n",
    "        print(decoded_output_with_watermark)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Detection result @ {args.detection_z_threshold}:\")\n",
    "        print(with_watermark_detection_result[1][0])\n",
    "        pprint(with_watermark_detection_result)\n",
    "        print(\"-\" * term_width)\n",
    "        print(f\"Perplexity (watermarked): {perplexity_wm:.2f}\")\n",
    "\n",
    "    # Launch the app to generate and detect interactively (implements the hf space demo)\n",
    "    #if args.run_gradio:\n",
    "    #    run_gradio(args, model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "74c738e93e3c37c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T05:44:00.842463Z",
     "start_time": "2025-03-23T05:44:00.803215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# coding=utf-8\\n# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\\n# available at https://arxiv.org/abs/2301.10226\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\nfrom __future__ import annotations\\nimport collections\\nfrom math import sqrt\\n\\nimport scipy.stats\\n\\nimport torch\\nfrom torch import Tensor\\nfrom tokenizers import Tokenizer\\nfrom transformers import LogitsProcessor\\n\\nfrom nltk.util import ngrams\\n\\nfrom normalizers import normalization_strategy_lookup\\n\\n\\nclass WatermarkBase:\\n    def __init__(\\n        self,\\n        vocab: list[int] = None,\\n        gamma: float = 0.5,\\n        delta: float = 2.0,\\n        seeding_scheme: str = \"simple_1\",  # mostly unused/always default\\n        hash_key: int = 15485863,  # just a large prime number to create a rng seed with sufficient bit width\\n        select_green_tokens: bool = True,\\n    ):\\n\\n        # watermarking parameters\\n        self.vocab = vocab\\n        self.vocab_size = len(vocab)\\n        self.gamma = gamma\\n        self.delta = delta\\n        self.seeding_scheme = seeding_scheme\\n        self.rng = None\\n        self.hash_key = hash_key\\n        self.select_green_tokens = select_green_tokens\\n\\n    def _seed_rng(self, input_ids: torch.LongTensor, seeding_scheme: str = None) -> None:\\n        # can optionally override the seeding scheme,\\n        # but uses the instance attr by default\\n        if seeding_scheme is None:\\n            seeding_scheme = self.seeding_scheme\\n\\n        if seeding_scheme == \"simple_1\":\\n            assert input_ids.shape[-1] >= 1, f\"seeding_scheme={seeding_scheme} requires at least a 1 token prefix sequence to seed rng\"\\n            prev_token = input_ids[-1].item()\\n            self.rng.manual_seed(self.hash_key * prev_token)\\n        else:\\n            raise NotImplementedError(f\"Unexpected seeding_scheme: {seeding_scheme}\")\\n        return\\n\\n    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> list[int]:\\n        # seed the rng using the previous tokens/prefix\\n        # according to the seeding_scheme\\n        self._seed_rng(input_ids)\\n\\n        greenlist_size = int(self.vocab_size * self.gamma)\\n        vocab_permutation = torch.randperm(self.vocab_size, device=input_ids.device, generator=self.rng)\\n        if self.select_green_tokens:  # directly\\n            greenlist_ids = vocab_permutation[:greenlist_size]  # new\\n        else:  # select green via red\\n            greenlist_ids = vocab_permutation[(self.vocab_size - greenlist_size) :]  # legacy behavior\\n        return greenlist_ids\\n\\n\\nclass WatermarkLogitsProcessor(WatermarkBase, LogitsProcessor):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n    def _calc_greenlist_mask(self, scores: torch.FloatTensor, greenlist_token_ids) -> torch.BoolTensor:\\n        # TODO lets see if we can lose this loop\\n        green_tokens_mask = torch.zeros_like(scores)\\n        for b_idx in range(len(greenlist_token_ids)):\\n            green_tokens_mask[b_idx][greenlist_token_ids[b_idx]] = 1\\n        final_mask = green_tokens_mask.bool()\\n        return final_mask\\n\\n    def _bias_greenlist_logits(self, scores: torch.Tensor, greenlist_mask: torch.Tensor, greenlist_bias: float) -> torch.Tensor:\\n        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\\n        return scores\\n\\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\\n\\n        # this is lazy to allow us to colocate on the watermarked model\\'s device\\n        if self.rng is None:\\n            self.rng = torch.Generator(device=input_ids.device)\\n\\n        # NOTE, it would be nice to get rid of this batch loop, but currently,\\n        # the seed and partition operations are not tensor/vectorized, thus\\n        # each sequence in the batch needs to be treated separately.\\n        batched_greenlist_ids = [None for _ in range(input_ids.shape[0])]\\n\\n        for b_idx in range(input_ids.shape[0]):\\n            greenlist_ids = self._get_greenlist_ids(input_ids[b_idx])\\n            batched_greenlist_ids[b_idx] = greenlist_ids\\n\\n        green_tokens_mask = self._calc_greenlist_mask(scores=scores, greenlist_token_ids=batched_greenlist_ids)\\n\\n        scores = self._bias_greenlist_logits(scores=scores, greenlist_mask=green_tokens_mask, greenlist_bias=self.delta)\\n        return scores\\n\\n\\nclass WatermarkDetector(WatermarkBase):\\n    def __init__(\\n        self,\\n        *args,\\n        device: torch.device = None,\\n        tokenizer: Tokenizer = None,\\n        z_threshold: float = 4.0,\\n        normalizers: list[str] = [\"unicode\"],  # or also: [\"unicode\", \"homoglyphs\", \"truecase\"]\\n        ignore_repeated_bigrams: bool = True,\\n        **kwargs,\\n    ):\\n        super().__init__(*args, **kwargs)\\n        # also configure the metrics returned/preprocessing options\\n        assert device, \"Must pass device\"\\n        assert tokenizer, \"Need an instance of the generating tokenizer to perform detection\"\\n\\n        self.tokenizer = tokenizer\\n        self.device = device\\n        self.z_threshold = z_threshold\\n        self.rng = torch.Generator(device=self.device)\\n\\n        if self.seeding_scheme == \"simple_1\":\\n            self.min_prefix_len = 1\\n        else:\\n            raise NotImplementedError(f\"Unexpected seeding_scheme: {self.seeding_scheme}\")\\n\\n        self.normalizers = []\\n        for normalization_strategy in normalizers:\\n            self.normalizers.append(normalization_strategy_lookup(normalization_strategy))\\n\\n        self.ignore_repeated_bigrams = ignore_repeated_bigrams\\n        if self.ignore_repeated_bigrams:\\n            assert self.seeding_scheme == \"simple_1\", \"No repeated bigram credit variant assumes the single token seeding scheme.\"\\n\\n    def _compute_z_score(self, observed_count, T):\\n        # count refers to number of green tokens, T is total number of tokens\\n        expected_count = self.gamma\\n        numer = observed_count - expected_count * T\\n        denom = sqrt(T * expected_count * (1 - expected_count))\\n        z = numer / denom\\n        return z\\n\\n    def _compute_p_value(self, z):\\n        p_value = scipy.stats.norm.sf(z)\\n        return p_value\\n\\n    def _score_sequence(\\n        self,\\n        input_ids: Tensor,\\n        return_num_tokens_scored: bool = True,\\n        return_num_green_tokens: bool = True,\\n        return_green_fraction: bool = True,\\n        return_green_token_mask: bool = False,\\n        return_z_score: bool = True,\\n        return_p_value: bool = True,\\n    ):\\n        if self.ignore_repeated_bigrams:\\n            # Method that only counts a green/red hit once per unique bigram.\\n            # New num total tokens scored (T) becomes the number unique bigrams.\\n            # We iterate over all unqiue token bigrams in the input, computing the greenlist\\n            # induced by the first token in each, and then checking whether the second\\n            # token falls in that greenlist.\\n            assert return_green_token_mask is False, \"Can\\'t return the green/red mask when ignoring repeats.\"\\n            bigram_table = {}\\n            token_bigram_generator = ngrams(input_ids.cpu().tolist(), 2)\\n            freq = collections.Counter(token_bigram_generator)\\n            num_tokens_scored = len(freq.keys())\\n            for idx, bigram in enumerate(freq.keys()):\\n                prefix = torch.tensor([bigram[0]], device=self.device)  # expects a 1-d prefix tensor on the randperm device\\n                greenlist_ids = self._get_greenlist_ids(prefix)\\n                bigram_table[bigram] = True if bigram[1] in greenlist_ids else False\\n            green_token_count = sum(bigram_table.values())\\n        else:\\n            num_tokens_scored = len(input_ids) - self.min_prefix_len\\n            if num_tokens_scored < 1:\\n                raise ValueError(\\n                    (\\n                        f\"Must have at least {1} token to score after \"\\n                        f\"the first min_prefix_len={self.min_prefix_len} tokens required by the seeding scheme.\"\\n                    )\\n                )\\n            # Standard method.\\n            # Since we generally need at least 1 token (for the simplest scheme)\\n            # we start the iteration over the token sequence with a minimum\\n            # num tokens as the first prefix for the seeding scheme,\\n            # and at each step, compute the greenlist induced by the\\n            # current prefix and check if the current token falls in the greenlist.\\n            green_token_count, green_token_mask = 0, []\\n            for idx in range(self.min_prefix_len, len(input_ids)):\\n                curr_token = input_ids[idx]\\n                greenlist_ids = self._get_greenlist_ids(input_ids[:idx])\\n                if curr_token in greenlist_ids:\\n                    green_token_count += 1\\n                    green_token_mask.append(True)\\n                else:\\n                    green_token_mask.append(False)\\n\\n        score_dict = dict()\\n        if return_num_tokens_scored:\\n            score_dict.update(dict(num_tokens_scored=num_tokens_scored))\\n        if return_num_green_tokens:\\n            score_dict.update(dict(num_green_tokens=green_token_count))\\n        if return_green_fraction:\\n            score_dict.update(dict(green_fraction=(green_token_count / num_tokens_scored)))\\n        if return_z_score:\\n            score_dict.update(dict(z_score=self._compute_z_score(green_token_count, num_tokens_scored)))\\n        if return_p_value:\\n            z_score = score_dict.get(\"z_score\")\\n            if z_score is None:\\n                z_score = self._compute_z_score(green_token_count, num_tokens_scored)\\n            score_dict.update(dict(p_value=self._compute_p_value(z_score)))\\n        if return_green_token_mask:\\n            score_dict.update(dict(green_token_mask=green_token_mask))\\n\\n        return score_dict\\n\\n    def detect(\\n        self,\\n        text: str = None,\\n        tokenized_text: list[int] = None,\\n        return_prediction: bool = True,\\n        return_scores: bool = True,\\n        z_threshold: float = None,\\n        **kwargs,\\n    ) -> dict:\\n\\n        assert (text is not None) ^ (tokenized_text is not None), \"Must pass either the raw or tokenized string\"\\n        if return_prediction:\\n            kwargs[\"return_p_value\"] = True  # to return the \"confidence\":=1-p of positive detections\\n\\n        # run optional normalizers on text\\n        for normalizer in self.normalizers:\\n            text = normalizer(text)\\n        if len(self.normalizers) > 0:\\n            print(f\"Text after normalization:\\n\\n{text}\\n\")\\n\\n        if tokenized_text is None:\\n            assert self.tokenizer is not None, (\\n                \"Watermark detection on raw string \",\\n                \"requires an instance of the tokenizer \",\\n                \"that was used at generation time.\",\\n            )\\n            tokenized_text = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].to(self.device)\\n            if tokenized_text[0] == self.tokenizer.bos_token_id:\\n                tokenized_text = tokenized_text[1:]\\n        else:\\n            # try to remove the bos_tok at beginning if it\\'s there\\n            if (self.tokenizer is not None) and (tokenized_text[0] == self.tokenizer.bos_token_id):\\n                tokenized_text = tokenized_text[1:]\\n\\n        # call score method\\n        output_dict = {}\\n        score_dict = self._score_sequence(tokenized_text, **kwargs)\\n        if return_scores:\\n            output_dict.update(score_dict)\\n        # if passed return_prediction then perform the hypothesis test and return the outcome\\n        if return_prediction:\\n            z_threshold = z_threshold if z_threshold else self.z_threshold\\n            assert z_threshold is not None, \"Need a threshold in order to decide outcome of detection test\"\\n            output_dict[\"prediction\"] = score_dict[\"z_score\"] > z_threshold\\n            if output_dict[\"prediction\"]:\\n                output_dict[\"confidence\"] = 1 - score_dict[\"p_value\"]\\n\\n        return output_dict'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n",
    "# available at https://arxiv.org/abs/2301.10226\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import annotations\n",
    "import collections\n",
    "from math import sqrt\n",
    "import scipy.stats\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import LogitsProcessor\n",
    "import nltk\n",
    "import ssl\n",
    "from nltk.util import ngrams\n",
    "\n",
    "###############################################\n",
    "# Revised Watermark Classes\n",
    "###############################################\n",
    "\n",
    "class WatermarkBase_kgw:\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab: list[int] = None,\n",
    "            gamma: float = 0.5,\n",
    "            delta: float = 2.0,\n",
    "            seeding_scheme: str = \"simple_1\",\n",
    "            hash_key: int = 15485863,\n",
    "            select_green_tokens: bool = True,\n",
    "            precomputed_pairing: list[tuple[int, int]] = None,\n",
    "            unique_tokens: list[int] = None,\n",
    "    ):\n",
    "        self.vocab = vocab  # list of token IDs (usually 0,...,n-1)\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.gamma = gamma  # fraction of tokens to designate as green (target size)\n",
    "        self.delta = delta  # bias to add to green tokens' logits\n",
    "        self.seeding_scheme = seeding_scheme\n",
    "        self.rng = None\n",
    "        self.hash_key = hash_key\n",
    "        self.select_green_tokens = select_green_tokens\n",
    "        self.pairing = precomputed_pairing  # perfect matching on tokens with synonyms (indices in vocab)\n",
    "        self.unique_tokens = unique_tokens  # list of token IDs that have no synonyms\n",
    "\n",
    "    def _seed_rng(self, input_ids: torch.LongTensor, seeding_scheme: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Seeds the RNG deterministically using the last token in input_ids.\n",
    "        For the \"simple_1\" scheme, seed = hash_key * (last token id).\n",
    "        \"\"\"\n",
    "        if seeding_scheme is None:\n",
    "            seeding_scheme = self.seeding_scheme\n",
    "        # Ensure the RNG is initialized.\n",
    "        if self.rng is None:\n",
    "            self.rng = torch.Generator(device=input_ids.device)\n",
    "        if seeding_scheme == \"simple_1\":\n",
    "            assert input_ids.shape[-1] >= 1, \"Input must have at least one token.\"\n",
    "            prev_token = input_ids[-1].item()\n",
    "            self.rng.manual_seed(self.hash_key * prev_token)\n",
    "            #print(self.hash_key * prev_token)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Seeding scheme {seeding_scheme} not implemented.\")\n",
    "        return\n",
    "\n",
    "    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> list[int]:\n",
    "        \"\"\"\n",
    "        Returns a list of token IDs that form the green list.\n",
    "        If a precomputed pairing exists, then:\n",
    "          - All tokens from the unique set (those with no synonyms) are automatically in the green list.\n",
    "          - For each pair in the perfect matching (on tokens with synonyms), a fair coin flip (using the seeded RNG)\n",
    "            selects one token from the pair.\n",
    "        Otherwise, falls back to a random permutation method.\n",
    "        Optionally, the list may be truncated to a target size (gamma * vocab_size).\n",
    "        \"\"\"\n",
    "        self._seed_rng(input_ids)\n",
    "        # Fallback: use random permutation.\n",
    "        greenlist_size = int(self.vocab_size * self.gamma)\n",
    "        vocab_permutation = torch.randperm(self.vocab_size, device=input_ids.device, generator=self.rng)\n",
    "        if self.select_green_tokens:\n",
    "            return vocab_permutation[:greenlist_size].tolist()\n",
    "        else:\n",
    "            return vocab_permutation[-greenlist_size:].tolist()\n",
    "        #if self.pairing is None or self.unique_tokens is None:\n",
    "            # Fallback: use random permutation.\n",
    "        #    greenlist_size = int(self.vocab_size * self.gamma)\n",
    "        #    vocab_permutation = torch.randperm(self.vocab_size, device=input_ids.device, generator=self.rng)\n",
    "        #    if self.select_green_tokens:\n",
    "        #        return vocab_permutation[:greenlist_size].tolist()\n",
    "        #    else:\n",
    "        #        return vocab_permutation[-greenlist_size:].tolist()\n",
    "        #else:\n",
    "            # Start with all unique tokens.\n",
    "        #    greenlist_ids = self.unique_tokens.copy()\n",
    "            # For tokens that have synonyms (precomputed pairing), randomly assign one from each pair.\n",
    "        #    for pair in self.pairing:\n",
    "        #        coin_flip = (torch.rand(1, generator=self.rng, device=input_ids.device) < 0.5).item()\n",
    "        #        chosen = pair[0] if coin_flip == 1 else pair[1]\n",
    "        #        greenlist_ids.append(chosen)\n",
    "            # Optionally, enforce a maximum size (gamma * vocab_size)\n",
    "        #    desired_size = int(self.vocab_size * self.gamma)\n",
    "        #    if len(greenlist_ids) > desired_size:\n",
    "        #        indices = torch.randperm(len(greenlist_ids), generator=self.rng, device=input_ids.device)[:desired_size].tolist()\n",
    "        #        greenlist_ids = [greenlist_ids[i] for i in indices]\n",
    "        #    return greenlist_ids\n",
    "\n",
    "\n",
    "class WatermarkLogitsProcessor_kgw(WatermarkBase_kgw, LogitsProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _calc_greenlist_mask(self, scores: torch.FloatTensor, greenlist_token_ids) -> torch.BoolTensor:\n",
    "        green_tokens_mask = torch.zeros_like(scores)\n",
    "        for b_idx in range(len(greenlist_token_ids)):\n",
    "            green_tokens_mask[b_idx][greenlist_token_ids[b_idx]] = 1\n",
    "        return green_tokens_mask.bool()\n",
    "\n",
    "    def _bias_greenlist_logits(self, scores: torch.Tensor, greenlist_mask: torch.Tensor,\n",
    "                               greenlist_bias: float) -> torch.Tensor:\n",
    "        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\n",
    "        return scores\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        #print(input_ids.device)\n",
    "        if self.rng is None:\n",
    "            self.rng = torch.Generator(device=input_ids.device)\n",
    "        batched_greenlist_ids = [None for _ in range(input_ids.shape[0])]\n",
    "        for b_idx in range(input_ids.shape[0]):\n",
    "            greenlist_ids = self._get_greenlist_ids(input_ids[b_idx])\n",
    "            batched_greenlist_ids[b_idx] = greenlist_ids\n",
    "        green_tokens_mask = self._calc_greenlist_mask(scores, batched_greenlist_ids)\n",
    "        scores = self._bias_greenlist_logits(scores, green_tokens_mask, self.delta)\n",
    "        #print(torch.max(scores))\n",
    "        return scores\n",
    "\n",
    "\n",
    "class WatermarkDetector_kgw(WatermarkBase_kgw):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *args,\n",
    "            device: torch.device = None,\n",
    "            tokenizer: Tokenizer = None,\n",
    "            z_threshold: float = 4.0,\n",
    "            normalizers: list[str] = [\"unicode\"],\n",
    "            ignore_repeated_bigrams: bool = True,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert device, \"Device must be provided.\"\n",
    "        assert tokenizer, \"A tokenizer is required for detection.\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.z_threshold = z_threshold\n",
    "        self.rng = torch.Generator(device=self.device)\n",
    "        if self.seeding_scheme == \"simple_1\":\n",
    "            self.min_prefix_len = 1\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Seeding scheme {self.seeding_scheme} not implemented.\")\n",
    "        self.normalizers = [normalization_strategy_lookup(norm) for norm in normalizers]\n",
    "        self.ignore_repeated_bigrams = ignore_repeated_bigrams\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            assert self.seeding_scheme == \"simple_1\", \"Repeated bigram variant requires simple_1 seeding.\"\n",
    "\n",
    "    def _compute_z_score(self, observed_count, T):\n",
    "        expected_count = self.gamma\n",
    "        numer = observed_count - expected_count * T\n",
    "        denom = sqrt(T * expected_count * (1 - expected_count))\n",
    "        return numer / denom\n",
    "\n",
    "    def _compute_p_value(self, z):\n",
    "        return scipy.stats.norm.sf(z)\n",
    "\n",
    "    def _score_sequence(\n",
    "            self,\n",
    "            input_ids: Tensor,\n",
    "            return_num_tokens_scored: bool = True,\n",
    "            return_num_green_tokens: bool = True,\n",
    "            return_green_fraction: bool = True,\n",
    "            return_green_token_mask: bool = False,\n",
    "            return_z_score: bool = True,\n",
    "            return_p_value: bool = True,\n",
    "    ):\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            bigram_table = {}\n",
    "            token_bigram_generator = ngrams(input_ids.cpu().tolist(), 2)\n",
    "            freq = collections.Counter(token_bigram_generator)\n",
    "            num_tokens_scored = len(freq.keys())\n",
    "            for bigram in freq.keys():\n",
    "                prefix = torch.tensor([bigram[0]], device=self.device)\n",
    "                greenlist_ids = self._get_greenlist_ids(prefix)\n",
    "                bigram_table[bigram] = True if bigram[1] in greenlist_ids else False\n",
    "            green_token_count = sum(bigram_table.values())\n",
    "        else:\n",
    "            #print(1)\n",
    "            num_tokens_scored = len(input_ids) - self.min_prefix_len\n",
    "            if num_tokens_scored < 1:\n",
    "                score_dict = None\n",
    "                return score_dict #raise ValueError(\"Not enough tokens to score.\")\n",
    "            green_token_count = 0\n",
    "            green_token_mask = []\n",
    "            for idx in range(self.min_prefix_len, len(input_ids)):\n",
    "                curr_token = input_ids[idx]\n",
    "                greenlist_ids = self._get_greenlist_ids(input_ids[:idx])\n",
    "                if curr_token in greenlist_ids:\n",
    "                    green_token_count += 1\n",
    "                    green_token_mask.append(True)\n",
    "                    #print(True)\n",
    "                else:\n",
    "                    green_token_mask.append(False)\n",
    "                    #print(False)\n",
    "        score_dict = {}\n",
    "        if return_num_tokens_scored:\n",
    "            score_dict[\"num_tokens_scored\"] = num_tokens_scored\n",
    "        if return_num_green_tokens:\n",
    "            score_dict[\"num_green_tokens\"] = green_token_count\n",
    "        if return_green_fraction:\n",
    "            score_dict[\"green_fraction\"] = green_token_count / num_tokens_scored\n",
    "        if return_z_score:\n",
    "            score_dict[\"z_score\"] = self._compute_z_score(green_token_count, num_tokens_scored)\n",
    "        if return_p_value:\n",
    "            z = score_dict.get(\"z_score\", self._compute_z_score(green_token_count, num_tokens_scored))\n",
    "            score_dict[\"p_value\"] = self._compute_p_value(z)\n",
    "        if return_green_token_mask:\n",
    "            score_dict[\"green_token_mask\"] = green_token_mask\n",
    "        return score_dict\n",
    "\n",
    "    def detect(\n",
    "            self,\n",
    "            text: str = None,\n",
    "            tokenized_text: list[int] = None,\n",
    "            return_prediction: bool = True,\n",
    "            return_scores: bool = True,\n",
    "            z_threshold: float = None,\n",
    "            **kwargs,\n",
    "    ) -> dict:\n",
    "        assert (text is not None) ^ (tokenized_text is not None), \"Provide either raw or tokenized text.\"\n",
    "        if return_prediction:\n",
    "            kwargs[\"return_p_value\"] = True\n",
    "        for normalizer in self.normalizers:\n",
    "            text = normalizer(text)\n",
    "        if tokenized_text is None:\n",
    "            tokenized_text = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].to(\n",
    "                self.device)\n",
    "            if tokenized_text[0] == self.tokenizer.bos_token_id:\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "        else:\n",
    "            if self.tokenizer is not None and tokenized_text[0] == self.tokenizer.bos_token_id:\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "        output_dict = {}\n",
    "        score_dict = self._score_sequence(tokenized_text, **kwargs)\n",
    "        if return_scores:\n",
    "            output_dict.update(score_dict)\n",
    "        if return_prediction:\n",
    "            z_threshold = z_threshold if z_threshold is not None else self.z_threshold\n",
    "            output_dict[\"prediction\"] = score_dict[\"z_score\"] > z_threshold\n",
    "            if output_dict[\"prediction\"]:\n",
    "                output_dict[\"confidence\"] = 1 - score_dict[\"p_value\"]\n",
    "        return output_dict\n",
    "\n",
    "'''\n",
    "###############################################\n",
    "# Outline of the New Partitioning Method\n",
    "###############################################\n",
    "# 1. Obtain the vocabulary from the model's tokenizer using get_vocabulary(tokenizer).\n",
    "# 2. Use filter_tokens_with_synonyms(vocab_list) to split the vocabulary indices into:\n",
    "#       - unique_indices: tokens with no synonyms (set A)\n",
    "#       - paired_indices: tokens with at least one synonym (set B)\n",
    "# 3. Construct the similarity matrix for tokens in set B using construct_similarity_matrix.\n",
    "# 4. Compute a perfect matching on set B using find_perfect_matching.\n",
    "# 5. In _get_greenlist_ids, use the precomputed pairing (mapped back to original token IDs)\n",
    "#    and return all tokens from set A plus one token per pair (chosen at random by a coin flip).\n",
    "###############################################\n",
    "# Full Code Example for the Revised Watermark Partition\n",
    "###############################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For testing purposes, use a small model like 'distilgpt2' which can run on CPU.\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "    vocab_list = get_vocabulary(tokenizer)\n",
    "    print(f\"Vocabulary size: {len(vocab_list)}\")\n",
    "\n",
    "    # Filter tokens: separate those with no synonyms (set A) and with synonyms (set B)\n",
    "    unique_indices, paired_indices = filter_tokens_with_synonyms(vocab_list)\n",
    "    print(f\"Number of unique tokens (no synonyms, set A): {len(unique_indices)}\")\n",
    "    print(f\"Number of tokens with synonyms (set B): {len(paired_indices)}\")\n",
    "\n",
    "    # Construct similarity matrix for tokens in set B.\n",
    "    similarity_matrix = construct_similarity_matrix(vocab_list, paired_indices)\n",
    "    # Find a perfect matching on the tokens in set B.\n",
    "    matching = find_perfect_matching(similarity_matrix)\n",
    "    # Map matching indices (relative to paired_indices) back to the original vocabulary indices.\n",
    "    mapped_pairing = [(paired_indices[i], paired_indices[j]) for (i, j) in matching]\n",
    "    print(\"Computed perfect matching (pairs) on tokens with synonyms (set B):\")\n",
    "    print(mapped_pairing)\n",
    "\n",
    "    # Initialize WatermarkLogitsProcessor with precomputed pairing and unique tokens.\n",
    "    wm_processor = WatermarkLogitsProcessor(\n",
    "        vocab=list(range(len(vocab_list))),\n",
    "        gamma=0.25,\n",
    "        delta=2.0,\n",
    "        seeding_scheme=\"simple_1\",\n",
    "        select_green_tokens=True,\n",
    "        precomputed_pairing=mapped_pairing,\n",
    "        unique_tokens=unique_indices\n",
    "    )\n",
    "\n",
    "    # Test _get_greenlist_ids with a sample prompt.\n",
    "    sample_prompt = \"This is a good day.\"\n",
    "    input_ids = torch.tensor(tokenizer.encode(sample_prompt))\n",
    "    greenlist_ids = wm_processor._get_greenlist_ids(input_ids)\n",
    "    print(\"Greenlist token IDs for the prompt:\")\n",
    "    print(greenlist_ids)\n",
    "    green_tokens = [vocab_list[tok] for tok in greenlist_ids]\n",
    "    print(\"Greenlist tokens (strings):\")\n",
    "    print(green_tokens)'''\n",
    "\n",
    "\n",
    "'''# coding=utf-8\n",
    "# Copyright 2023 Authors of \"A Watermark for Large Language Models\"\n",
    "# available at https://arxiv.org/abs/2301.10226\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import annotations\n",
    "import collections\n",
    "from math import sqrt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import LogitsProcessor\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from normalizers import normalization_strategy_lookup\n",
    "\n",
    "\n",
    "class WatermarkBase:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: list[int] = None,\n",
    "        gamma: float = 0.5,\n",
    "        delta: float = 2.0,\n",
    "        seeding_scheme: str = \"simple_1\",  # mostly unused/always default\n",
    "        hash_key: int = 15485863,  # just a large prime number to create a rng seed with sufficient bit width\n",
    "        select_green_tokens: bool = True,\n",
    "    ):\n",
    "\n",
    "        # watermarking parameters\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.seeding_scheme = seeding_scheme\n",
    "        self.rng = None\n",
    "        self.hash_key = hash_key\n",
    "        self.select_green_tokens = select_green_tokens\n",
    "\n",
    "    def _seed_rng(self, input_ids: torch.LongTensor, seeding_scheme: str = None) -> None:\n",
    "        # can optionally override the seeding scheme,\n",
    "        # but uses the instance attr by default\n",
    "        if seeding_scheme is None:\n",
    "            seeding_scheme = self.seeding_scheme\n",
    "\n",
    "        if seeding_scheme == \"simple_1\":\n",
    "            assert input_ids.shape[-1] >= 1, f\"seeding_scheme={seeding_scheme} requires at least a 1 token prefix sequence to seed rng\"\n",
    "            prev_token = input_ids[-1].item()\n",
    "            self.rng.manual_seed(self.hash_key * prev_token)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unexpected seeding_scheme: {seeding_scheme}\")\n",
    "        return\n",
    "\n",
    "    def _get_greenlist_ids(self, input_ids: torch.LongTensor) -> list[int]:\n",
    "        # seed the rng using the previous tokens/prefix\n",
    "        # according to the seeding_scheme\n",
    "        self._seed_rng(input_ids)\n",
    "\n",
    "        greenlist_size = int(self.vocab_size * self.gamma)\n",
    "        vocab_permutation = torch.randperm(self.vocab_size, device=input_ids.device, generator=self.rng)\n",
    "        if self.select_green_tokens:  # directly\n",
    "            greenlist_ids = vocab_permutation[:greenlist_size]  # new\n",
    "        else:  # select green via red\n",
    "            greenlist_ids = vocab_permutation[(self.vocab_size - greenlist_size) :]  # legacy behavior\n",
    "        return greenlist_ids\n",
    "\n",
    "\n",
    "class WatermarkLogitsProcessor(WatermarkBase, LogitsProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _calc_greenlist_mask(self, scores: torch.FloatTensor, greenlist_token_ids) -> torch.BoolTensor:\n",
    "        # TODO lets see if we can lose this loop\n",
    "        green_tokens_mask = torch.zeros_like(scores)\n",
    "        for b_idx in range(len(greenlist_token_ids)):\n",
    "            green_tokens_mask[b_idx][greenlist_token_ids[b_idx]] = 1\n",
    "        final_mask = green_tokens_mask.bool()\n",
    "        return final_mask\n",
    "\n",
    "    def _bias_greenlist_logits(self, scores: torch.Tensor, greenlist_mask: torch.Tensor, greenlist_bias: float) -> torch.Tensor:\n",
    "        scores[greenlist_mask] = scores[greenlist_mask] + greenlist_bias\n",
    "        return scores\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "\n",
    "        # this is lazy to allow us to colocate on the watermarked model's device\n",
    "        if self.rng is None:\n",
    "            self.rng = torch.Generator(device=input_ids.device)\n",
    "\n",
    "        # NOTE, it would be nice to get rid of this batch loop, but currently,\n",
    "        # the seed and partition operations are not tensor/vectorized, thus\n",
    "        # each sequence in the batch needs to be treated separately.\n",
    "        batched_greenlist_ids = [None for _ in range(input_ids.shape[0])]\n",
    "\n",
    "        for b_idx in range(input_ids.shape[0]):\n",
    "            greenlist_ids = self._get_greenlist_ids(input_ids[b_idx])\n",
    "            batched_greenlist_ids[b_idx] = greenlist_ids\n",
    "\n",
    "        green_tokens_mask = self._calc_greenlist_mask(scores=scores, greenlist_token_ids=batched_greenlist_ids)\n",
    "\n",
    "        scores = self._bias_greenlist_logits(scores=scores, greenlist_mask=green_tokens_mask, greenlist_bias=self.delta)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class WatermarkDetector(WatermarkBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        device: torch.device = None,\n",
    "        tokenizer: Tokenizer = None,\n",
    "        z_threshold: float = 4.0,\n",
    "        normalizers: list[str] = [\"unicode\"],  # or also: [\"unicode\", \"homoglyphs\", \"truecase\"]\n",
    "        ignore_repeated_bigrams: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # also configure the metrics returned/preprocessing options\n",
    "        assert device, \"Must pass device\"\n",
    "        assert tokenizer, \"Need an instance of the generating tokenizer to perform detection\"\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.z_threshold = z_threshold\n",
    "        self.rng = torch.Generator(device=self.device)\n",
    "\n",
    "        if self.seeding_scheme == \"simple_1\":\n",
    "            self.min_prefix_len = 1\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unexpected seeding_scheme: {self.seeding_scheme}\")\n",
    "\n",
    "        self.normalizers = []\n",
    "        for normalization_strategy in normalizers:\n",
    "            self.normalizers.append(normalization_strategy_lookup(normalization_strategy))\n",
    "\n",
    "        self.ignore_repeated_bigrams = ignore_repeated_bigrams\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            assert self.seeding_scheme == \"simple_1\", \"No repeated bigram credit variant assumes the single token seeding scheme.\"\n",
    "\n",
    "    def _compute_z_score(self, observed_count, T):\n",
    "        # count refers to number of green tokens, T is total number of tokens\n",
    "        expected_count = self.gamma\n",
    "        numer = observed_count - expected_count * T\n",
    "        denom = sqrt(T * expected_count * (1 - expected_count))\n",
    "        z = numer / denom\n",
    "        return z\n",
    "\n",
    "    def _compute_p_value(self, z):\n",
    "        p_value = scipy.stats.norm.sf(z)\n",
    "        return p_value\n",
    "\n",
    "    def _score_sequence(\n",
    "        self,\n",
    "        input_ids: Tensor,\n",
    "        return_num_tokens_scored: bool = True,\n",
    "        return_num_green_tokens: bool = True,\n",
    "        return_green_fraction: bool = True,\n",
    "        return_green_token_mask: bool = False,\n",
    "        return_z_score: bool = True,\n",
    "        return_p_value: bool = True,\n",
    "    ):\n",
    "        if self.ignore_repeated_bigrams:\n",
    "            # Method that only counts a green/red hit once per unique bigram.\n",
    "            # New num total tokens scored (T) becomes the number unique bigrams.\n",
    "            # We iterate over all unqiue token bigrams in the input, computing the greenlist\n",
    "            # induced by the first token in each, and then checking whether the second\n",
    "            # token falls in that greenlist.\n",
    "            assert return_green_token_mask is False, \"Can't return the green/red mask when ignoring repeats.\"\n",
    "            bigram_table = {}\n",
    "            token_bigram_generator = ngrams(input_ids.cpu().tolist(), 2)\n",
    "            freq = collections.Counter(token_bigram_generator)\n",
    "            num_tokens_scored = len(freq.keys())\n",
    "            for idx, bigram in enumerate(freq.keys()):\n",
    "                prefix = torch.tensor([bigram[0]], device=self.device)  # expects a 1-d prefix tensor on the randperm device\n",
    "                greenlist_ids = self._get_greenlist_ids(prefix)\n",
    "                bigram_table[bigram] = True if bigram[1] in greenlist_ids else False\n",
    "            green_token_count = sum(bigram_table.values())\n",
    "        else:\n",
    "            num_tokens_scored = len(input_ids) - self.min_prefix_len\n",
    "            if num_tokens_scored < 1:\n",
    "                raise ValueError(\n",
    "                    (\n",
    "                        f\"Must have at least {1} token to score after \"\n",
    "                        f\"the first min_prefix_len={self.min_prefix_len} tokens required by the seeding scheme.\"\n",
    "                    )\n",
    "                )\n",
    "            # Standard method.\n",
    "            # Since we generally need at least 1 token (for the simplest scheme)\n",
    "            # we start the iteration over the token sequence with a minimum\n",
    "            # num tokens as the first prefix for the seeding scheme,\n",
    "            # and at each step, compute the greenlist induced by the\n",
    "            # current prefix and check if the current token falls in the greenlist.\n",
    "            green_token_count, green_token_mask = 0, []\n",
    "            for idx in range(self.min_prefix_len, len(input_ids)):\n",
    "                curr_token = input_ids[idx]\n",
    "                greenlist_ids = self._get_greenlist_ids(input_ids[:idx])\n",
    "                if curr_token in greenlist_ids:\n",
    "                    green_token_count += 1\n",
    "                    green_token_mask.append(True)\n",
    "                else:\n",
    "                    green_token_mask.append(False)\n",
    "\n",
    "        score_dict = dict()\n",
    "        if return_num_tokens_scored:\n",
    "            score_dict.update(dict(num_tokens_scored=num_tokens_scored))\n",
    "        if return_num_green_tokens:\n",
    "            score_dict.update(dict(num_green_tokens=green_token_count))\n",
    "        if return_green_fraction:\n",
    "            score_dict.update(dict(green_fraction=(green_token_count / num_tokens_scored)))\n",
    "        if return_z_score:\n",
    "            score_dict.update(dict(z_score=self._compute_z_score(green_token_count, num_tokens_scored)))\n",
    "        if return_p_value:\n",
    "            z_score = score_dict.get(\"z_score\")\n",
    "            if z_score is None:\n",
    "                z_score = self._compute_z_score(green_token_count, num_tokens_scored)\n",
    "            score_dict.update(dict(p_value=self._compute_p_value(z_score)))\n",
    "        if return_green_token_mask:\n",
    "            score_dict.update(dict(green_token_mask=green_token_mask))\n",
    "\n",
    "        return score_dict\n",
    "\n",
    "    def detect(\n",
    "        self,\n",
    "        text: str = None,\n",
    "        tokenized_text: list[int] = None,\n",
    "        return_prediction: bool = True,\n",
    "        return_scores: bool = True,\n",
    "        z_threshold: float = None,\n",
    "        **kwargs,\n",
    "    ) -> dict:\n",
    "\n",
    "        assert (text is not None) ^ (tokenized_text is not None), \"Must pass either the raw or tokenized string\"\n",
    "        if return_prediction:\n",
    "            kwargs[\"return_p_value\"] = True  # to return the \"confidence\":=1-p of positive detections\n",
    "\n",
    "        # run optional normalizers on text\n",
    "        for normalizer in self.normalizers:\n",
    "            text = normalizer(text)\n",
    "        if len(self.normalizers) > 0:\n",
    "            print(f\"Text after normalization:\\n\\n{text}\\n\")\n",
    "\n",
    "        if tokenized_text is None:\n",
    "            assert self.tokenizer is not None, (\n",
    "                \"Watermark detection on raw string \",\n",
    "                \"requires an instance of the tokenizer \",\n",
    "                \"that was used at generation time.\",\n",
    "            )\n",
    "            tokenized_text = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"][0].to(self.device)\n",
    "            if tokenized_text[0] == self.tokenizer.bos_token_id:\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "        else:\n",
    "            # try to remove the bos_tok at beginning if it's there\n",
    "            if (self.tokenizer is not None) and (tokenized_text[0] == self.tokenizer.bos_token_id):\n",
    "                tokenized_text = tokenized_text[1:]\n",
    "\n",
    "        # call score method\n",
    "        output_dict = {}\n",
    "        score_dict = self._score_sequence(tokenized_text, **kwargs)\n",
    "        if return_scores:\n",
    "            output_dict.update(score_dict)\n",
    "        # if passed return_prediction then perform the hypothesis test and return the outcome\n",
    "        if return_prediction:\n",
    "            z_threshold = z_threshold if z_threshold else self.z_threshold\n",
    "            assert z_threshold is not None, \"Need a threshold in order to decide outcome of detection test\"\n",
    "            output_dict[\"prediction\"] = score_dict[\"z_score\"] > z_threshold\n",
    "            if output_dict[\"prediction\"]:\n",
    "                output_dict[\"confidence\"] = 1 - score_dict[\"p_value\"]\n",
    "\n",
    "        return output_dict'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2fc2e822bb5cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513c9c13-fee4-4584-937d-a37d7bb1deec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /venv/main/lib/python3.10/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3917a820-68b3-4d4d-858f-97eb534d7677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /venv/main/lib/python3.10/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /venv/main/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from datasets) (2.2.4)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 datasets-3.4.1 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.2.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de2ae8c-ce9a-4daf-8566-9ef3209704ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /venv/main/lib/python3.10/site-packages (from scikit-learn) (2.2.4)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2ccb738-045a-46fd-a754-26210e45b163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting triton==3.2.0\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch) (3.17.0)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377b5233-ba8b-4159-a7dc-cf3c06aac7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: regex>=2021.8.3 in /venv/main/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Collecting click\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /venv/main/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1908b-6846-4395-9132-2fae47fc875f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
